{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /usr/local/lib/python3.9/dist-packages (0.7.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.35.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Requirement already satisfied: torchvision==0.13.1 in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.12.1) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1) (2.28.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision==0.13.1) (1.23.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.1) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.13.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.1) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: timm==0.5.4 in /usr/local/lib/python3.9/dist-packages (0.5.4)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.5.4) (0.13.1+cu116)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.5.4) (1.12.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.5.4) (4.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.5.4) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.5.4) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.5.4) (2.28.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision->timm==0.5.4) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision->timm==0.5.4) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.5.4) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.5.4) (2.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: omegaconf==2.1.1 in /usr/local/lib/python3.9/dist-packages (2.1.1)\n",
      "Requirement already satisfied: tqdm==4.64.0 in /usr/local/lib/python3.9/dist-packages (4.64.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.9/dist-packages (from omegaconf==2.1.1) (4.8)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.9/dist-packages (from omegaconf==2.1.1) (5.4.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: lpips==0.1.4 in /usr/local/lib/python3.9/dist-packages (0.1.4)\n",
      "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from lpips==0.1.4) (1.12.1+cu116)\n",
      "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.9/dist-packages (from lpips==0.1.4) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.9/dist-packages (from lpips==0.1.4) (4.64.0)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from lpips==0.1.4) (0.13.1+cu116)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from lpips==0.1.4) (1.9.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.0->lpips==0.1.4) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips==0.1.4) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision>=0.2.1->lpips==0.1.4) (9.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision>=0.2.1->lpips==0.1.4) (1.26.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pytorch_lightning==1.7.7 in /usr/local/lib/python3.9/dist-packages (1.7.7)\n",
      "Requirement already satisfied: torchmetrics==0.11.4 in /usr/local/lib/python3.9/dist-packages (0.11.4)\n",
      "Requirement already satisfied: torchtext==0.13.1 in /usr/local/lib/python3.9/dist-packages (0.13.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (2.9.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (1.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (4.4.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (5.4.1)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (23.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning==1.7.7) (2023.12.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.13.1) (2.28.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (3.8.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (0.4.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (1.51.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (2.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (66.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (2.16.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (1.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (3.4.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.7) (3.19.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchtext==0.13.1) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.13.1) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext==0.13.1) (2.8)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (18.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.7) (1.8.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (1.14.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning==1.7.7) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: kornia in /usr/local/lib/python3.9/dist-packages (0.7.0)\n",
      "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.9/dist-packages (from kornia) (1.12.1+cu116)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from kornia) (23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.1->kornia) (4.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: rich==10.2.2 in /usr/local/lib/python3.9/dist-packages (10.2.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich==10.2.2) (2.14.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich==10.2.2) (0.9.1)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from rich==10.2.2) (0.4.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: natsort in /usr/local/lib/python3.9/dist-packages (8.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping keras as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-datasets as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-estimator as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-gcs-config as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-hub as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-io-gcs-filesystem as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-metadata as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping tensorflow-probability as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Pip Installs\n",
    "!pip install einops\n",
    "!pip install --upgrade transformers\n",
    "!pip install --extra-index-url https://download.pytorch.org/whl/cu116 torch==1.12.1 torchvision==0.13.1\n",
    "!pip install timm==0.5.4\n",
    "!pip install omegaconf==2.1.1 tqdm==4.64.0\n",
    "!pip install lpips==0.1.4\n",
    "!pip install pytorch_lightning==1.7.7 torchmetrics==0.11.4 torchtext==0.13.1\n",
    "!pip install kornia\n",
    "!pip install rich==10.2.2\n",
    "!pip install natsort\n",
    "!pip uninstall -y keras tensorflow tensorflow-datasets tensorflow-estimator tensorflow-gcs-config tensorflow-hub tensorflow-io-gcs-filesystem tensorflow-metadata tensorflow-probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle dataset related tasks scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2727,
     "status": "ok",
     "timestamp": 1701288238670,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "vldpfdOO9XFX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Handle dataset related tasks scripts:\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import interpolate\n",
    "from einops import rearrange\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.interpolate import interp1d\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "from natsort import natsorted\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "def pad_to_patch_size(x, patch_size):\n",
    "    assert x.ndim == 2\n",
    "    return np.pad(x, ((0,0),(0, patch_size-x.shape[1]%patch_size)), 'wrap')\n",
    "\n",
    "def pad_to_length(x, length):\n",
    "    assert x.ndim == 3\n",
    "    assert x.shape[-1] <= length\n",
    "    if x.shape[-1] == length:\n",
    "        return x\n",
    "\n",
    "    return np.pad(x, ((0,0),(0,0), (0, length - x.shape[-1])), 'wrap')\n",
    "\n",
    "def normalize(x, mean=None, std=None):\n",
    "    mean = np.mean(x) if mean is None else mean\n",
    "    std = np.std(x) if std is None else std\n",
    "    return (x - mean) / (std * 1.0)\n",
    "\n",
    "def img_norm(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = (img / 255.0) * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def channel_first(img):\n",
    "        if img.shape[-1] == 3:\n",
    "            return rearrange(img, 'h w c -> c h w')\n",
    "        return img\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def file_ext(name: Union[str, Path]) -> str:\n",
    "    return str(name).split('.')[-1]\n",
    "\n",
    "def is_npy_ext(fname: Union[str, Path]) -> bool:\n",
    "    ext = file_ext(fname).lower()\n",
    "    return f'{ext}' == 'npy'# type: ignore\n",
    "\n",
    "class eeg_pretrain_dataset(Dataset):\n",
    "    def __init__(self, path='eegData_npy/', roi='VC', patch_size=16, transform=identity, aug_times=2,\n",
    "                num_sub_limit=None, include_kam=False, include_hcp=True):\n",
    "        super(eeg_pretrain_dataset, self).__init__()\n",
    "        data = []\n",
    "        images = []\n",
    "        self.input_paths = [str(f) for f in sorted(Path(path).rglob('*')) if is_npy_ext(f) and os.path.isfile(f)]\n",
    "\n",
    "        assert len(self.input_paths) != 0, 'No data found'\n",
    "        self.data_len  = 1024\n",
    "        self.data_chan = 14\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_path = self.input_paths[index]\n",
    "\n",
    "        data = np.load(data_path)\n",
    "\n",
    "        if data.shape[-1] > self.data_len:\n",
    "            idx = np.random.randint(0, int(data.shape[-1] - self.data_len)+1)\n",
    "\n",
    "            data = data[:, idx: idx+self.data_len]\n",
    "        else:\n",
    "            x = np.linspace(0, 1, data.shape[-1])\n",
    "            x2 = np.linspace(0, 1, self.data_len)\n",
    "            f = interp1d(x, data)\n",
    "            data = f(x2)\n",
    "        ret = np.zeros((self.data_chan, self.data_len))\n",
    "        if (self.data_chan > data.shape[-2]):\n",
    "            for i in range((self.data_chan//data.shape[-2])):\n",
    "\n",
    "                ret[i * data.shape[-2]: (i+1) * data.shape[-2], :] = data\n",
    "            if self.data_chan % data.shape[-2] != 0:\n",
    "\n",
    "                ret[ -(self.data_chan%data.shape[-2]):, :] = data[: (self.data_chan%data.shape[-2]), :]\n",
    "        elif(self.data_chan < data.shape[-2]):\n",
    "            idx2 = np.random.randint(0, int(data.shape[-2] - self.data_chan)+1)\n",
    "            ret = data[idx2: idx2+self.data_chan, :]\n",
    "        # print(ret.shape)\n",
    "        elif(self.data_chan == data.shape[-2]):\n",
    "            ret = data\n",
    "        ret = ret/10 # reduce an order\n",
    "        # torch.tensor()\n",
    "        ret = torch.from_numpy(ret).float()\n",
    "        return {'eeg': ret } #,\n",
    "\n",
    "class base_dataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=identity):\n",
    "        super(base_dataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        if self.y is None:\n",
    "            return self.transform(self.x[index])\n",
    "        else:\n",
    "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
    "\n",
    "def remove_repeats(fmri, img_lb):\n",
    "    assert len(fmri) == len(img_lb), 'len error'\n",
    "    fmri_dict = {}\n",
    "    for f, lb in zip(fmri, img_lb):\n",
    "        if lb in fmri_dict.keys():\n",
    "            fmri_dict[lb].append(f)\n",
    "        else:\n",
    "            fmri_dict[lb] = [f]\n",
    "    lbs = []\n",
    "    fmris = []\n",
    "    for k, v in fmri_dict.items():\n",
    "        lbs.append(k)\n",
    "        fmris.append(np.mean(np.stack(v), axis=0))\n",
    "    return np.stack(fmris), lbs\n",
    "\n",
    "\n",
    "def list_get_all_index(list, value):\n",
    "    return [i for i, v in enumerate(list) if v == value]\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class EEGDataset_r(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, image_transform=identity):\n",
    "\n",
    "        self.imagesource = 'eegData_images'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 440\n",
    "        self.data_len = 1024\n",
    "        # # Compute size\n",
    "        self.size = 100\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return 100\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        eeg = torch.randn(14,1024)\n",
    "\n",
    "        # print(image.shape)\n",
    "        label = torch.tensor(0).long()\n",
    "        image = torch.randn(3,675,675)\n",
    "        image_raw = image\n",
    "\n",
    "        return {'eeg': eeg, 'label': label, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
    "\n",
    "\n",
    "class EEGDataset_s(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, image_transform, eeg_signals_path):\n",
    "        # Load EEG signals\n",
    "        loaded = torch.load(eeg_signals_path)\n",
    "        # if opt.subject!=0:\n",
    "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
    "        # else:\n",
    "        self.eeg = loaded['dataset']\n",
    "        self.labels = loaded[\"labels\"]\n",
    "        self.images = loaded[\"images\"]\n",
    "        self.imagesource = 'eegData_images'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 1024\n",
    "        # Compute size\n",
    "        self.size = len(self.data)\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        eeg = self.data[i][\"eeg\"].float().t()\n",
    "\n",
    "        # Get label\n",
    "        image_name = self.images[self.data[i][\"image\"]]\n",
    "        # image_path = os.path.join(self.imagenet, image_name.split('_')[0], image_name+'.JPEG')\n",
    "        return image_name\n",
    "\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, image_transform=identity):\n",
    "        eeg_dir = 'eeg14/eegData_npy'\n",
    "        self.eeg = [np.load(os.path.join(eeg_dir, f'{i+1}.npy')) for i in range(len(os.listdir(eeg_dir)))]\n",
    "\n",
    "        images_dir = 'eeg14/eegData_images'\n",
    "        self.images = [os.path.join(images_dir, f) for f in os.listdir(images_dir) if f.endswith('.png')]\n",
    "\n",
    "        #self.labels = loaded[\"labels\"]\n",
    "        self.imagesource = 'eeg14/eegData_images'\n",
    "        self.image_transform = image_transform\n",
    "        self.num_voxels = 1024\n",
    "        self.data_len = 1024\n",
    "        # Compute size\n",
    "        self.size = len(self.eeg)\n",
    "        self.processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        # Process EEG\n",
    "        eeg = torch.from_numpy(self.eeg[i]).float().t()\n",
    "\n",
    "        # Preprocess and transpose\n",
    "        eeg = eeg.numpy().transpose()  # Convert to NumPy array and transpose\n",
    "        x = np.linspace(0, 1, eeg.shape[-1])\n",
    "        x2 = np.linspace(0, 1, self.data_len)\n",
    "        f = interp1d(x, eeg)\n",
    "        eeg = f(x2)\n",
    "        eeg = torch.from_numpy(eeg).float()  # Convert back to a PyTorch tensor\n",
    "\n",
    "        # Process Image\n",
    "        # Assuming self.images[i] is a NumPy array representing an image\n",
    "        image_raw = Image.open(self.images[i]).convert('RGB')\n",
    "\n",
    "        # Normalize and transform the image if necessary\n",
    "        image = np.array(image_raw) / 255.0\n",
    "        image_raw = self.processor(images=image_raw, return_tensors=\"pt\")\n",
    "        image_raw['pixel_values'] = image_raw['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {'eeg': eeg, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
    "        # return eeg, label?\n",
    "\n",
    "class Splitter:\n",
    "\n",
    "    def __init__(self, dataset, split_name=\"train\"):\n",
    "        # Set EEG dataset\n",
    "        self.dataset = dataset\n",
    "\n",
    "        # Compute the indices for the split based on the percentage\n",
    "        total_size = len(self.dataset.eeg)  # Changed to 'eeg'\n",
    "        split_index = int(total_size * 0.8)  # 80% for training\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            self.split_idx = list(range(split_index))  # First 80%\n",
    "        elif split_name == \"test\":\n",
    "            self.split_idx = list(range(split_index, total_size))  # Remaining 20%\n",
    "        else:\n",
    "            raise ValueError(\"Invalid split_name. Expected 'train' or 'test'.\")\n",
    "\n",
    "        # Compute size\n",
    "        self.size = len(self.split_idx)\n",
    "        self.num_voxels = 1024\n",
    "        self.data_len = 1024\n",
    "\n",
    "    # Get size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    # Get item\n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[self.split_idx[i]]\n",
    "\n",
    "\n",
    "def create_EEG_dataset(image_transform=identity, subject = 0):\n",
    "    if isinstance(image_transform, list):\n",
    "        dataset_train = EEGDataset(image_transform[0] )\n",
    "        dataset_test = EEGDataset(image_transform[1])\n",
    "    else:\n",
    "        dataset_train = EEGDataset(image_transform)\n",
    "        dataset_test = EEGDataset(image_transform)\n",
    "    split_train = Splitter(dataset_train, split_name = 'train')\n",
    "    split_test = Splitter(dataset_test, split_name = 'test')\n",
    "    return (split_train, split_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_EEG_dataset_r(\n",
    "            image_transform=identity):\n",
    "    if isinstance(image_transform, list):\n",
    "        dataset_train = EEGDataset_r(image_transform[0])\n",
    "        dataset_test = EEGDataset_r(image_transform[1])\n",
    "    else:\n",
    "        dataset_train = EEGDataset_r(image_transform)\n",
    "        dataset_test = EEGDataset_r(image_transform)\n",
    "    return (dataset_train,dataset_test)\n",
    "\n",
    "class random_crop:\n",
    "    def __init__(self, size, p):\n",
    "        self.size = size\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.RandomCrop(size=(self.size, self.size))(img)\n",
    "        return img\n",
    "def normalize2(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "def channel_last(img):\n",
    "        if img.shape[-1] == 3:\n",
    "            return img\n",
    "        return rearrange(img, 'c h w -> h w c')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288238670,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "XU52BNoLFu58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Diffusion util\n",
    "# adopted from\n",
    "# https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n",
    "# and\n",
    "# https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\n",
    "# and\n",
    "# https://github.com/openai/guided-diffusion/blob/0ba878e517b276c45d1195eb29f6f5f72659a05b/guided_diffusion/nn.py\n",
    "#\n",
    "# thanks!\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "    if schedule == \"linear\":\n",
    "        betas = (\n",
    "                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
    "        )\n",
    "\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = (\n",
    "                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n",
    "        )\n",
    "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
    "        alphas = torch.cos(alphas).pow(2)\n",
    "        alphas = alphas / alphas[0]\n",
    "        betas = 1 - alphas[1:] / alphas[:-1]\n",
    "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
    "\n",
    "    elif schedule == \"sqrt_linear\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n",
    "    elif schedule == \"sqrt\":\n",
    "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n",
    "    else:\n",
    "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
    "    return betas.numpy()\n",
    "\n",
    "\n",
    "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n",
    "    if ddim_discr_method == 'uniform':\n",
    "        c = num_ddpm_timesteps // num_ddim_timesteps\n",
    "        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
    "    elif ddim_discr_method == 'quad':\n",
    "        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n",
    "    else:\n",
    "        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
    "\n",
    "    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n",
    "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
    "    steps_out = ddim_timesteps + 1\n",
    "    if verbose:\n",
    "        print(f'Selected timesteps for ddim sampler: {steps_out}')\n",
    "    return steps_out\n",
    "\n",
    "\n",
    "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n",
    "    # select alphas for computing the variance schedule\n",
    "    alphas = alphacums[ddim_timesteps]\n",
    "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
    "\n",
    "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
    "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
    "    if verbose:\n",
    "        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n",
    "        print(f'For the chosen value of eta, which is {eta}, '\n",
    "              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n",
    "    return sigmas, alphas, alphas_prev\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "\n",
    "def extract_into_tensor(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "\n",
    "def checkpoint(func, inputs, params, flag):\n",
    "    \"\"\"\n",
    "    Evaluate a function without caching intermediate activations, allowing for\n",
    "    reduced memory at the expense of extra compute in the backward pass.\n",
    "    :param func: the function to evaluate.\n",
    "    :param inputs: the argument sequence to pass to `func`.\n",
    "    :param params: a sequence of parameters `func` depends on but does not\n",
    "                   explicitly take as arguments.\n",
    "    :param flag: if False, disable gradient checkpointing.\n",
    "    \"\"\"\n",
    "    if flag:\n",
    "        args = tuple(inputs) + tuple(params)\n",
    "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
    "    else:\n",
    "        return func(*inputs)\n",
    "\n",
    "\n",
    "class CheckpointFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, run_function, length, *args):\n",
    "        ctx.run_function = run_function\n",
    "        ctx.input_tensors = list(args[:length])\n",
    "        ctx.input_params = list(args[length:])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
    "        return output_tensors\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, *output_grads):\n",
    "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
    "        with torch.enable_grad():\n",
    "            # Fixes a bug where the first op in run_function modifies the\n",
    "            # Tensor storage in place, which is not allowed for detach()'d\n",
    "            # Tensors.\n",
    "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
    "            output_tensors = ctx.run_function(*shallow_copies)\n",
    "        input_grads = torch.autograd.grad(\n",
    "            output_tensors,\n",
    "            ctx.input_tensors + ctx.input_params,\n",
    "            output_grads,\n",
    "            allow_unused=True,\n",
    "        )\n",
    "        del ctx.input_tensors\n",
    "        del ctx.input_params\n",
    "        del output_tensors\n",
    "        return (None, None) + input_grads\n",
    "\n",
    "\n",
    "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    else:\n",
    "        embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def scale_module(module, scale):\n",
    "    \"\"\"\n",
    "    Scale the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().mul_(scale)\n",
    "    return module\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def normalization(channels):\n",
    "    \"\"\"\n",
    "    Make a standard normalization layer.\n",
    "    :param channels: number of input channels.\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    return GroupNorm32(32, channels)\n",
    "\n",
    "\n",
    "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
    "class SiLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "def conv_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D convolution module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.Conv1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.Conv2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.Conv3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "def linear(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a linear module.\n",
    "    \"\"\"\n",
    "    return nn.Linear(*args, **kwargs)\n",
    "\n",
    "\n",
    "def avg_pool_nd(dims, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a 1D, 2D, or 3D average pooling module.\n",
    "    \"\"\"\n",
    "    if dims == 1:\n",
    "        return nn.AvgPool1d(*args, **kwargs)\n",
    "    elif dims == 2:\n",
    "        return nn.AvgPool2d(*args, **kwargs)\n",
    "    elif dims == 3:\n",
    "        return nn.AvgPool3d(*args, **kwargs)\n",
    "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
    "\n",
    "\n",
    "class HybridConditioner(nn.Module):\n",
    "\n",
    "    def __init__(self, c_concat_config, c_crossattn_config):\n",
    "        super().__init__()\n",
    "        self.concat_conditioner = instantiate_from_config(c_concat_config)\n",
    "        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n",
    "\n",
    "    def forward(self, c_concat, c_crossattn):\n",
    "        c_concat = self.concat_conditioner(c_concat)\n",
    "        c_crossattn = self.crossattn_conditioner(c_crossattn)\n",
    "        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n",
    "\n",
    "\n",
    "def noise_like(shape, device, repeat=False):\n",
    "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
    "    noise = lambda: torch.randn(shape, device=device)\n",
    "    return repeat_noise() if repeat else noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLM Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288238670,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "JB7y4_5dFu5-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title PLM Sampler\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PLMSSampler(object):\n",
    "    def __init__(self, model, schedule=\"linear\", **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.ddpm_num_timesteps = model.num_timesteps\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def register_buffer(self, name, attr):\n",
    "        if type(attr) == torch.Tensor:\n",
    "            if attr.device != torch.device(\"cuda\"):\n",
    "                attr = attr.to(torch.device(\"cuda\"))\n",
    "        setattr(self, name, attr)\n",
    "\n",
    "    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n",
    "        if ddim_eta != 0:\n",
    "            raise ValueError('ddim_eta must be 0 for PLMS')\n",
    "        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n",
    "                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n",
    "        alphas_cumprod = self.model.alphas_cumprod\n",
    "        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n",
    "        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(self.model.betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n",
    "\n",
    "        # ddim sampling parameters\n",
    "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
    "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
    "                                                                                   eta=ddim_eta,verbose=verbose)\n",
    "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
    "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
    "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
    "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
    "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n",
    "            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n",
    "                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
    "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self,\n",
    "               S,\n",
    "               batch_size,\n",
    "               shape,\n",
    "               conditioning=None,\n",
    "               callback=None,\n",
    "               normals_sequence=None,\n",
    "               img_callback=None,\n",
    "               quantize_x0=False,\n",
    "               eta=0.,\n",
    "               mask=None,\n",
    "               x0=None,\n",
    "               temperature=1.,\n",
    "               noise_dropout=0.,\n",
    "               score_corrector=None,\n",
    "               corrector_kwargs=None,\n",
    "               verbose=True,\n",
    "               x_T=None,\n",
    "               log_every_t=100,\n",
    "               unconditional_guidance_scale=1.,\n",
    "               unconditional_conditioning=None,\n",
    "               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n",
    "               **kwargs\n",
    "               ):\n",
    "        if conditioning is not None:\n",
    "            if isinstance(conditioning, dict):\n",
    "                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n",
    "                if cbs != batch_size:\n",
    "                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n",
    "            else:\n",
    "                if conditioning.shape[0] != batch_size:\n",
    "                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n",
    "\n",
    "        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n",
    "        # sampling\n",
    "        C, H, W = shape\n",
    "        size = (batch_size, C, H, W)\n",
    "        print(f'Data shape for PLMS sampling is {size}')\n",
    "\n",
    "        samples, intermediates = self.plms_sampling(conditioning, size,\n",
    "                                                    callback=callback,\n",
    "                                                    img_callback=img_callback,\n",
    "                                                    quantize_denoised=quantize_x0,\n",
    "                                                    mask=mask, x0=x0,\n",
    "                                                    ddim_use_original_steps=False,\n",
    "                                                    noise_dropout=noise_dropout,\n",
    "                                                    temperature=temperature,\n",
    "                                                    score_corrector=score_corrector,\n",
    "                                                    corrector_kwargs=corrector_kwargs,\n",
    "                                                    x_T=x_T,\n",
    "                                                    log_every_t=log_every_t,\n",
    "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                                    unconditional_conditioning=unconditional_conditioning,\n",
    "                                                    **kwargs\n",
    "                                                    )\n",
    "        return samples, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def plms_sampling(self, cond, shape,\n",
    "                      x_T=None, ddim_use_original_steps=False,\n",
    "                      callback=None, timesteps=None, quantize_denoised=False,\n",
    "                      mask=None, x0=None, img_callback=None, log_every_t=100,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None, generator=None):\n",
    "        device = self.model.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device, generator=generator)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        if timesteps is None:\n",
    "            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n",
    "        elif timesteps is not None and not ddim_use_original_steps:\n",
    "            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n",
    "            timesteps = self.ddim_timesteps[:subset_end]\n",
    "\n",
    "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
    "        time_range = list(reversed(range(0,timesteps))) if ddim_use_original_steps else np.flip(timesteps)\n",
    "        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n",
    "        print(f\"Running PLMS Sampling with {total_steps} timesteps\")\n",
    "\n",
    "        iterator = tqdm(time_range, desc='PLMS Sampler', total=total_steps)\n",
    "        old_eps = []\n",
    "\n",
    "        for i, step in enumerate(iterator):\n",
    "            index = total_steps - i - 1\n",
    "            ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
    "            ts_next = torch.full((b,), time_range[min(i + 1, len(time_range) - 1)], device=device, dtype=torch.long)\n",
    "\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            outs = self.p_sample_plms(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
    "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
    "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
    "                                      corrector_kwargs=corrector_kwargs,\n",
    "                                      unconditional_guidance_scale=unconditional_guidance_scale,\n",
    "                                      unconditional_conditioning=unconditional_conditioning,\n",
    "                                      old_eps=old_eps, t_next=ts_next)\n",
    "            img, pred_x0, e_t = outs\n",
    "            old_eps.append(e_t)\n",
    "            if len(old_eps) >= 4:\n",
    "                old_eps.pop(0)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(pred_x0, i)\n",
    "\n",
    "            if index % log_every_t == 0 or index == total_steps - 1:\n",
    "                intermediates['x_inter'].append(img)\n",
    "                intermediates['pred_x0'].append(pred_x0)\n",
    "\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_plms(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n",
    "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None, old_eps=None, t_next=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "\n",
    "        def get_model_output(x, t):\n",
    "            if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n",
    "                e_t = self.model.apply_model(x, t, c)\n",
    "            else:\n",
    "                x_in = torch.cat([x] * 2)\n",
    "                t_in = torch.cat([t] * 2)\n",
    "                c_in = torch.cat([unconditional_conditioning, c])\n",
    "                e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n",
    "                e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
    "\n",
    "            if score_corrector is not None:\n",
    "                assert self.model.parameterization == \"eps\"\n",
    "                e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n",
    "\n",
    "            return e_t\n",
    "\n",
    "        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n",
    "        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n",
    "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n",
    "        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n",
    "\n",
    "        def get_x_prev_and_pred_x0(e_t, index):\n",
    "            # select parameters corresponding to the currently considered timestep\n",
    "            a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
    "            a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
    "            sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
    "            sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
    "\n",
    "            # current prediction for x_0\n",
    "            pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
    "            if quantize_denoised:\n",
    "                pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n",
    "            # direction pointing to x_t\n",
    "            dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
    "            noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
    "            if noise_dropout > 0.:\n",
    "                noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "            x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
    "            return x_prev, pred_x0\n",
    "\n",
    "        e_t = get_model_output(x, t)\n",
    "        if len(old_eps) == 0:\n",
    "            # Pseudo Improved Euler (2nd order)\n",
    "            x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t, index)\n",
    "            e_t_next = get_model_output(x_prev, t_next)\n",
    "            e_t_prime = (e_t + e_t_next) / 2\n",
    "        elif len(old_eps) == 1:\n",
    "            # 2nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (3 * e_t - old_eps[-1]) / 2\n",
    "        elif len(old_eps) == 2:\n",
    "            # 3nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (23 * e_t - 16 * old_eps[-1] + 5 * old_eps[-2]) / 12\n",
    "        elif len(old_eps) >= 3:\n",
    "            # 4nd order Pseudo Linear Multistep (Adams-Bashforth)\n",
    "            e_t_prime = (55 * e_t - 59 * old_eps[-1] + 37 * old_eps[-2] - 9 * old_eps[-3]) / 24\n",
    "\n",
    "        x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t_prime, index)\n",
    "\n",
    "        return x_prev, pred_x0, e_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE for EEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701288239131,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "8NAwpMJuFu5_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title MAE for EEG\n",
    "\n",
    "# utils\n",
    "import math\n",
    "import os\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, length, cls_token=False):\n",
    "    \"\"\"\n",
    "    grid_size: int of the grid height and width\n",
    "    return:\n",
    "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
    "    \"\"\"\n",
    "    grid_l = np.arange(length, dtype=np.float32)\n",
    "\n",
    "    grid_l = grid_l.reshape([1, length])\n",
    "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid_l)\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "    return pos_embed\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = pos.reshape(-1)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# Interpolate position embeddings for high-resolution\n",
    "# References:\n",
    "# DeiT: https://github.com/facebookresearch/deit\n",
    "# --------------------------------------------------------\n",
    "def interpolate_pos_embed(model, checkpoint_model):\n",
    "    if 'pos_embed' in checkpoint_model:\n",
    "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
    "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "        num_patches = model.patch_embed.num_patches\n",
    "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches # cls token\n",
    "        # height (== width) for the checkpoint position embedding\n",
    "        orig_size = int(pos_embed_checkpoint.shape[-2] - num_extra_tokens)\n",
    "        # height (== width) for the new position embedding\n",
    "        new_size = int(num_patches)\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        if orig_size != new_size:\n",
    "            print(\"Position interpolate from %d to %d\" % (orig_size, new_size))\n",
    "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "            # only the position tokens are interpolated\n",
    "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "            pos_tokens = pos_tokens.reshape(-1, orig_size, embedding_size).permute(0, 2, 1)\n",
    "            pos_tokens = torch.nn.functional.interpolate(\n",
    "                pos_tokens, size=(new_size))\n",
    "            pos_tokens = pos_tokens.permute(0, 2, 1)\n",
    "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "            checkpoint_model['pos_embed'] = new_pos_embed\n",
    "\n",
    "\n",
    "\n",
    "def original_schedule(epoch, config):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < config.warmup_epochs:\n",
    "        lr = config.lr * epoch / config.warmup_epochs\n",
    "    else:\n",
    "        lr = config.min_lr + (config.lr - config.min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - config.warmup_epochs) / (config.num_epoch - config.warmup_epochs)))\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, config):\n",
    "    # Define new schedule parameters\n",
    "    cycle_length = 50\n",
    "    max_lr = config.lr\n",
    "    min_lr = config.lr * 0.1\n",
    "\n",
    "    # Blending period\n",
    "    blending_epochs = 20\n",
    "    start_blending_epoch = 1000\n",
    "    end_blending_epoch = start_blending_epoch + blending_epochs\n",
    "\n",
    "    if epoch < start_blending_epoch:\n",
    "        lr = original_schedule(epoch, config)\n",
    "    elif start_blending_epoch <= epoch < end_blending_epoch:\n",
    "        blend_weight = (epoch - start_blending_epoch) / blending_epochs\n",
    "        lr_old = original_schedule(epoch, config)\n",
    "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
    "        lr_new = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * cycle_progress))\n",
    "        lr = (1 - blend_weight) * lr_old + blend_weight * lr_new\n",
    "    else:\n",
    "        # New schedule\n",
    "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
    "        lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * cycle_progress))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def save_model(config, epoch, model, optimizer, loss_scaler, checkpoint_paths):\n",
    "    os.makedirs(checkpoint_paths, exist_ok=True)\n",
    "    to_save = {\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'scaler': loss_scaler.state_dict(),\n",
    "        'config': config,\n",
    "    }\n",
    "    torch.save(to_save, os.path.join(checkpoint_paths, 'checkpoint.pth'))\n",
    "\n",
    "\n",
    "def load_model(config, model, checkpoint_path ):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print(f'Model loaded with {checkpoint_path}')\n",
    "\n",
    "def patchify(imgs, patch_size):\n",
    "    \"\"\"\n",
    "    imgs: (N, 1, num_voxels)\n",
    "    x: (N, L, patch_size)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    assert imgs.ndim == 3 and imgs.shape[2] % p == 0\n",
    "\n",
    "    h = imgs.shape[2] // p\n",
    "    x = imgs.reshape(shape=(imgs.shape[0], h, p))\n",
    "    return x\n",
    "\n",
    "def unpatchify(x, patch_size):\n",
    "    \"\"\"\n",
    "    x: (N, L, patch_size)\n",
    "    imgs: (N, 1, num_voxels)\n",
    "    \"\"\"\n",
    "    p = patch_size\n",
    "    h = x.shape[1]\n",
    "\n",
    "    imgs = x.reshape(shape=(x.shape[0], 1, h * p))\n",
    "    return imgs\n",
    "\n",
    "import sys\n",
    "#sys.path.append('../dreamdiffusion/code/')\n",
    "# print(sys.path)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from timm.models.vision_transformer import Block\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class CustomBlock(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, dropout_rate=0.2, attn_dropout_rate=0.2, norm_layer=nn.LayerNorm):\n",
    "#         super().__init__()\n",
    "#         # Multi-head self-attention\n",
    "#         self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_dropout_rate)\n",
    "\n",
    "#         # Layer Norm\n",
    "#         self.norm1 = norm_layer(dim)\n",
    "#         self.norm2 = norm_layer(dim)\n",
    "\n",
    "#         # Feed-forward network\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(dropout_rate),\n",
    "#             nn.Linear(int(dim * mlp_ratio), dim),\n",
    "#             nn.Dropout(dropout_rate)\n",
    "#         )\n",
    "#         self.norm1 = nn.LayerNorm(dim)\n",
    "#         self.norm2 = nn.LayerNorm(dim)\n",
    "#         self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.dropout(self.attn(self.norm1(x), x, x)[0])\n",
    "#         x = x + self.dropout(self.mlp(self.norm2(x)))\n",
    "#         return x\n",
    "\n",
    "class PatchEmbed1D(nn.Module):\n",
    "    \"\"\" 1 Dimensional version of data (fmri voxels) to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, time_len=224, patch_size=1, in_chans=14, embed_dim=256, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        num_patches = time_len // patch_size\n",
    "        self.patch_shape = patch_size\n",
    "        self.time_len = time_len\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        B, C, V = x.shape # batch, channel, voxels\n",
    "        # assert V == self.num_voxels, \\\n",
    "        #     f\"Input fmri length ({V}) doesn't match model ({self.num_voxels}).\"\n",
    "        x = self.proj(x).transpose(1, 2).contiguous() # put embed_dim at the last dimension\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class MAEforEEG(nn.Module):\n",
    "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(self, time_len=1024, patch_size=4, embed_dim=1024, in_chans=14,\n",
    "                 depth=24, num_heads=16, decoder_embed_dim=512,\n",
    "                 decoder_depth=4, decoder_num_heads=16,\n",
    "                 mlp_ratio=2., norm_layer=nn.LayerNorm, focus_range=None, focus_rate=None, img_recon_weight=1.0,\n",
    "                 use_nature_img_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE encoder specifics\n",
    "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
    "\n",
    "        num_patches = int(time_len / patch_size)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.115)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # MAE decoder specifics\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.15)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, in_chans * patch_size, bias=True) # encoder to decoder\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # nature image decoder specifics\n",
    "        if use_nature_img_loss:\n",
    "            self.nature_img_decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "\n",
    "            self.nature_img_mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "\n",
    "            self.nature_img_decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "            self.nature_img_decoder_blocks = nn.ModuleList([\n",
    "                Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.15)\n",
    "                for i in range(2)])\n",
    "\n",
    "            self.nature_img_decoder_norm = norm_layer(decoder_embed_dim)\n",
    "            self.nature_img_decoder_pred = nn.Sequential(\n",
    "                nn.Conv1d(num_patches, 512, kernel_size=1, stride=1, bias=True),\n",
    "                nn.Linear(decoder_embed_dim, 28*28, bias=True)\n",
    "            )\n",
    "            # --------------------------------------------------------------------------\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.focus_range = focus_range\n",
    "        self.focus_rate = focus_rate\n",
    "        self.img_recon_weight = img_recon_weight\n",
    "        self.use_nature_img_loss = use_nature_img_loss\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        if self.use_nature_img_loss:\n",
    "            nature_img_decoder_pos_embed = get_1d_sincos_pos_embed(self.nature_img_decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "            self.nature_img_decoder_pos_embed.data.copy_(torch.from_numpy(nature_img_decoder_pos_embed).float().unsqueeze(0))\n",
    "            torch.nn.init.normal_(self.nature_img_mask_token, std=.02)\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        imgs: [N, chan, T]\n",
    "        x: (N, L, patch_size)\n",
    "        x: [N, chan * 4, T/4]\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.ndim == 3 and imgs.shape[1] % p == 0\n",
    "\n",
    "        # h = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], imgs.shape[1] // p, -1))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size)\n",
    "        imgs: (N, 1, num_voxels)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = x.shape[1]\n",
    "\n",
    "        imgs = x.reshape(shape=(x.shape[0], -1, x.shape[2] // p))\n",
    "        return imgs.transpose(1,2)\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "\n",
    "        if self.focus_range is not None:\n",
    "            len_mask = L - len_keep\n",
    "            weights = [1-self.focus_rate] * L\n",
    "            weights[self.focus_range[0] // self.patch_size : self.focus_range[1] // self.patch_size\n",
    "                        ] = [self.focus_rate] * (self.focus_range[1] // self.patch_size - self.focus_range[0] // self.patch_size)\n",
    "            weights = torch.tensor(weights).repeat(N, 1).to(x.device)\n",
    "            ids_mask = torch.multinomial(weights, len_mask, replacement=False)\n",
    "\n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        if self.focus_range is not None:\n",
    "            for i in range(N):\n",
    "                noise[i, ids_mask[i,:]] = 1.1  # set mask portion to 1.1\n",
    "\n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        # print('encoder embed')\n",
    "        # print(x.shape)\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore = None):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "        # print('decoder embed')\n",
    "        # print(x.shape)\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        # x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "        # x = x_\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "        # x = x + self.decoder_pos_embed[:, 1:, :]\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "        # print(x.shape)\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_nature_img_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.nature_img_decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.nature_img_mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.nature_img_decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.nature_img_decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.nature_img_decoder_norm(x)\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "        # predictor projection\n",
    "        # x = x.mean(dim=1, keepdim=True)\n",
    "        x = self.nature_img_decoder_pred(x)\n",
    "        x = x.view(x.shape[0], 512, 28, 28)\n",
    "\n",
    "        return x # n, 512, 28, 28\n",
    "\n",
    "    def forward_nature_img_loss(self, inputs, reconstructions):\n",
    "        loss = ((torch.tanh(inputs) - torch.tanh(reconstructions))**2).mean()\n",
    "        if torch.isnan(reconstructions).sum():\n",
    "            print('nan in reconstructions')\n",
    "        if torch.isnan(inputs).sum():\n",
    "            print('nan in inputs')\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 1, num_voxels]\n",
    "        imgs: [N, chan, T]\n",
    "        pred: [N, L, p]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        imgs = imgs.transpose(1,2)\n",
    "        target = self.patchify(imgs)\n",
    "        # target = imgs.transpose(1,2)\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "        # loss = loss.mean()\n",
    "        loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, img_features=None, valid_idx=None, mask_ratio=0.75):\n",
    "        # latent = self.forward_encoder(imgs, mask_ratio)\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "            # print(x)\n",
    "        # print(latent.shape)\n",
    "        # # print(mask)\n",
    "        # print(mask.shape)\n",
    "        # # print(ids_restore)\n",
    "        # print(ids_restore.shape)\n",
    "\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p]\n",
    "        # pred = self.forward_decoder(latent)  # [N, L, p]\n",
    "        # pred = pred\n",
    "        # print(pred.shape)\n",
    "        # mask=None\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        # print(self.unpatchify(pred.transpose(1,2)).shape)\n",
    "\n",
    "        if self.use_nature_img_loss and img_features is not None:\n",
    "            # valid_idx = torch.nonzero(nature_image.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
    "            if len(valid_idx) != 0:\n",
    "                nature_image_recon = self.forward_nature_img_decoder(latent[valid_idx], ids_restore[valid_idx])\n",
    "                loss_nature_image_recon = self.forward_nature_img_loss(img_features, nature_image_recon)\n",
    "                if torch.isnan(loss_nature_image_recon).sum():\n",
    "                    print(loss_nature_image_recon)\n",
    "                    print(\"loss_nature_image_recon is nan\")\n",
    "\n",
    "                loss = loss + self.img_recon_weight*loss_nature_image_recon\n",
    "\n",
    "        return loss, pred, mask\n",
    "\n",
    "class eeg_encoder(nn.Module):\n",
    "    def __init__(self, time_len=1024, patch_size=4, embed_dim=2048, in_chans=14,\n",
    "                 depth=24, num_heads=16, mlp_ratio=1., norm_layer=nn.LayerNorm, global_pool=False):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
    "\n",
    "        num_patches = int(time_len / patch_size)\n",
    "\n",
    "        self.num_patches = num_patches\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.global_pool = global_pool\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # initialization\n",
    "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
    "        w = self.patch_embed.proj.weight.data\n",
    "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
    "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        # initialize nn.Linear and nn.LayerNorm\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # we use xavier_uniform following official JAX ViT:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv1d):\n",
    "            torch.nn.init.normal_(m.weight, std=.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        # embed patches\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        # print(x.shape)\n",
    "        # print(self.pos_embed[:, 1:, :].shape)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        # print(x.shape)\n",
    "        if self.global_pool:\n",
    "            x = x.mean(dim=1, keepdim=True)\n",
    "        # print(x.shape)\n",
    "        x = self.norm(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        if imgs.ndim == 2:\n",
    "            imgs = torch.unsqueeze(imgs, dim=0)  # N, n_seq, embed_dim\n",
    "        latent = self.forward_encoder(imgs) # N, n_seq, embed_dim\n",
    "        return latent # N, n_seq, embed_dim\n",
    "\n",
    "    def load_checkpoint(self, state_dict):\n",
    "        if self.global_pool:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k and 'norm' not in k)}\n",
    "        else:\n",
    "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k)}\n",
    "        interpolate_pos_embed(self, state_dict)\n",
    "\n",
    "        m, u = self.load_state_dict(state_dict, strict=False)\n",
    "        print('missing keys:', u)\n",
    "        print('unexpected keys:', m)\n",
    "        return\n",
    "\n",
    "class classify_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.Conv1d(14, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(1024, 40)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class mapping(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.maxpool = nn.Conv1d(256, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Linear(1024, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for ELDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701288239957,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "iZJtHTbcFu6B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Utils for ELDM\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config in ['__is_first_stage__', \"__is_unconditional__\"]:\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "def get_obj_from_str(string):\n",
    "    try:\n",
    "        # Directly get the global object from the global scope\n",
    "        obj = globals()[string]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Object '{string}' not found in the global scope.\")\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ********  ELDM  ********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1701288681954,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "_m-2RnUlFu6B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ********  ELDM  ********\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def create_model_from_config(config, num_voxels, global_pool):\n",
    "    model = eeg_encoder(time_len=num_voxels, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
    "                depth=config.depth, num_heads=config.num_heads, mlp_ratio=config.mlp_ratio, global_pool=global_pool)\n",
    "    return model\n",
    "\n",
    "class cond_stage_model(nn.Module):\n",
    "    def __init__(self, metafile, num_voxels=440, cond_dim=1280, global_pool=True, clip_tune = True, cls_tune = False):\n",
    "        super().__init__()\n",
    "        # prepare pretrained fmri mae\n",
    "        if metafile is not None:\n",
    "            model = create_model_from_config(metafile['config'], num_voxels, global_pool)\n",
    "\n",
    "            model.load_checkpoint(metafile['model'])\n",
    "        else:\n",
    "            model = eeg_encoder(time_len=num_voxels, global_pool=global_pool)\n",
    "        self.mae = model\n",
    "        if clip_tune:\n",
    "            self.mapping = mapping()\n",
    "        if cls_tune:\n",
    "            self.cls_net = classify_network()\n",
    "\n",
    "        self.fmri_seq_len = model.num_patches\n",
    "        self.fmri_latent_dim = model.embed_dim\n",
    "        if global_pool == False:\n",
    "            self.channel_mapper = nn.Sequential(\n",
    "                nn.Conv1d(self.fmri_seq_len, self.fmri_seq_len // 2, 1, bias=True),\n",
    "                nn.Conv1d(self.fmri_seq_len // 2, 77, 1, bias=True)\n",
    "            )\n",
    "        self.dim_mapper = nn.Linear(self.fmri_latent_dim, cond_dim, bias=True)\n",
    "        self.global_pool = global_pool\n",
    "\n",
    "        # self.image_embedder = FrozenImageEmbedder()\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     # n, c, w = x.shape\n",
    "    #     latent_crossattn = self.mae(x)\n",
    "    #     if self.global_pool == False:\n",
    "    #         latent_crossattn = self.channel_mapper(latent_crossattn)\n",
    "    #     latent_crossattn = self.dim_mapper(latent_crossattn)\n",
    "    #     out = latent_crossattn\n",
    "    #     return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        # n, c, w = x.shape\n",
    "        latent_crossattn = self.mae(x)\n",
    "        latent_return = latent_crossattn\n",
    "        if self.global_pool == False:\n",
    "            latent_crossattn = self.channel_mapper(latent_crossattn)\n",
    "        latent_crossattn = self.dim_mapper(latent_crossattn)\n",
    "        out = latent_crossattn\n",
    "        return out, latent_return\n",
    "\n",
    "    # def recon(self, x):\n",
    "    #     recon = self.decoder(x)\n",
    "    #     return recon\n",
    "\n",
    "    def get_cls(self, x):\n",
    "        return self.cls_net(x)\n",
    "\n",
    "    def get_clip_loss(self, x, image_embeds, weight_decay=0.05):\n",
    "        target_emb = self.mapping(x)\n",
    "        loss = 1 - torch.cosine_similarity(target_emb, image_embeds, dim=-1).mean()\n",
    "\n",
    "        # L2 Regularization (squared L2 norm)\n",
    "        l2_reg = sum(torch.sum(param ** 2) for param in self.mapping.parameters())\n",
    "\n",
    "        # No need to take the square root for L2 regularization\n",
    "        # Apply weight decay to the regularization term\n",
    "        loss += weight_decay * l2_reg\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class eLDM:\n",
    "\n",
    "    def __init__(self, metafile, num_voxels, device=torch.device('cpu'),\n",
    "                 pretrain_root='../pretrains/',\n",
    "                 logger=None, ddim_steps=125, global_pool=True, use_time_cond=False, clip_tune = True, cls_tune = False, temperature=1.0):\n",
    "        # self.ckp_path = os.path.join(pretrain_root, 'model.ckpt')\n",
    "        self.ckp_path = 'mj/mdjrny-v4.ckpt'\n",
    "        self.config_path = os.path.join('config15.yaml')\n",
    "        config = OmegaConf.load(self.config_path)\n",
    "        config.model.params.unet_config.params.use_time_cond = use_time_cond\n",
    "        config.model.params.unet_config.params.global_pool = global_pool\n",
    "\n",
    "        self.cond_dim = config.model.params.unet_config.params.context_dim\n",
    "\n",
    "        print(config.model.target)\n",
    "        model = instantiate_from_config(config.model)\n",
    "        pl_sd = torch.load(self.ckp_path, map_location=\"cpu\")['state_dict']\n",
    "\n",
    "        m, u = model.load_state_dict(pl_sd, strict=False)\n",
    "        model.cond_stage_trainable = True\n",
    "        model.cond_stage_model = cond_stage_model(metafile, num_voxels, self.cond_dim, global_pool=global_pool, clip_tune = clip_tune,cls_tune = cls_tune)\n",
    "\n",
    "        model.ddim_steps = ddim_steps\n",
    "        model.re_init_ema()\n",
    "        if logger is not None:\n",
    "            logger.watch(model, log=\"all\", log_graph=False)\n",
    "\n",
    "        model.p_channels = config.model.params.channels\n",
    "        model.p_image_size = config.model.params.image_size\n",
    "        model.ch_mult = config.model.params.first_stage_config.params.ddconfig.ch_mult\n",
    "\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "\n",
    "        self.model.clip_tune = clip_tune\n",
    "        self.model.cls_tune = cls_tune\n",
    "\n",
    "        self.ldm_config = config\n",
    "        self.pretrain_root = pretrain_root\n",
    "        self.fmri_latent_dim = model.cond_stage_model.fmri_latent_dim\n",
    "        self.metafile = metafile\n",
    "        self.temperature=temperature\n",
    "\n",
    "    def finetune(self, trainers, dataset, test_dataset, bs1, lr1,\n",
    "                output_path, config=None):\n",
    "        config.trainer = None\n",
    "        config.logger = None\n",
    "        self.model.main_config = config\n",
    "        self.model.output_path = output_path\n",
    "        # self.model.train_dataset = dataset\n",
    "        self.model.run_full_validation_threshold = 0.15\n",
    "        # stage one: train the cond encoder with the pretrained one\n",
    "\n",
    "        # # stage one: only optimize conditional encoders\n",
    "        print('\\n##### Stage One: only optimize conditional encoders #####')\n",
    "        print(f'batch_size is: {bs1}')\n",
    "        dataloader = DataLoader(dataset, batch_size=bs1, num_workers=8, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=bs1, num_workers=8, shuffle=False)\n",
    "        self.model.unfreeze_whole_model()\n",
    "        self.model.freeze_first_stage()\n",
    "        # self.model.freeze_whole_model()\n",
    "        # self.model.unfreeze_cond_stage()\n",
    "\n",
    "        self.model.learning_rate = lr1\n",
    "        self.model.train_cond_stage_only = True\n",
    "        self.model.eval_avg = config.eval_avg\n",
    "        trainers.fit(self.model, dataloader, val_dataloaders=test_loader)\n",
    "\n",
    "        self.model.unfreeze_whole_model()\n",
    "\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 'model_state_dict': self.model.state_dict(),\n",
    "#                 'config': config,\n",
    "#                 'state': torch.random.get_rng_state()\n",
    "\n",
    "#             },\n",
    "#             os.path.join(output_path, 'checkpoint.pth')\n",
    "#         )\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, fmri_embedding, num_samples, ddim_steps, HW=None, limit=None, state=None, output_path = None, shouldSave = True):\n",
    "        # fmri_embedding: n, seq_len, embed_dim\n",
    "        all_samples = []\n",
    "        if HW is None:\n",
    "            shape = (self.ldm_config.model.params.channels,\n",
    "                self.ldm_config.model.params.image_size, self.ldm_config.model.params.image_size)\n",
    "        else:\n",
    "            num_resolutions = len(self.ldm_config.model.params.first_stage_config.params.ddconfig.ch_mult)\n",
    "            shape = (self.ldm_config.model.params.channels,\n",
    "                HW[0] // 2**(num_resolutions-1), HW[1] // 2**(num_resolutions-1))\n",
    "\n",
    "        model = self.model.to(self.device)\n",
    "        sampler = PLMSSampler(model, temperature=self.temperature)\n",
    "        # sampler = DDIMSampler(model)\n",
    "        if state is not None:\n",
    "            torch.cuda.set_rng_state(state)\n",
    "\n",
    "        with model.ema_scope():\n",
    "            model.eval()\n",
    "            for count, item in enumerate(fmri_embedding):\n",
    "                if limit is not None:\n",
    "                    if count >= limit:\n",
    "                        break\n",
    "                print(item)\n",
    "                latent = item['eeg']\n",
    "                gt_image = rearrange(item['image'], 'h w c -> 1 c h w') # h w c\n",
    "                print(f\"rendering {num_samples} examples in {ddim_steps} steps.\")\n",
    "                # assert latent.shape[-1] == self.fmri_latent_dim, 'dim error'\n",
    "\n",
    "                c, re_latent = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                # c = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=num_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "                gt_image = torch.clamp((gt_image+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "                all_samples.append(torch.cat([gt_image, x_samples_ddim.detach().cpu()], dim=0)) # put groundtruth at first\n",
    "                if output_path is not None and shouldSave == True:\n",
    "                    samples_t = (255. * torch.cat([gt_image, x_samples_ddim.detach().cpu()], dim=0).numpy()).astype(np.uint8)\n",
    "                    for copy_idx, img_t in enumerate(samples_t):\n",
    "                        img_t = rearrange(img_t, 'c h w -> h w c')\n",
    "                        Image.fromarray(img_t).save(os.path.join(output_path,\n",
    "                            f'./test{count}-{copy_idx}.png'))\n",
    "\n",
    "        # display as grid\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=num_samples+1)\n",
    "\n",
    "        # to image\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        model = model.to('cpu')\n",
    "\n",
    "        return grid, (255. * torch.stack(all_samples, 0).cpu().numpy()).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1701288243906,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "vZ1j9ZJul3Wm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title DDPM Models\n",
    "#@title DDPM models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LitEma(nn.Module):\n",
    "    def __init__(self, model, decay=0.9999, use_num_upates=True):\n",
    "        super().__init__()\n",
    "        if decay < 0.0 or decay > 1.0:\n",
    "            raise ValueError('Decay must be between 0 and 1')\n",
    "\n",
    "        self.m_name2s_name = {}\n",
    "        self.register_buffer('decay', torch.tensor(decay, dtype=torch.float32))\n",
    "        self.register_buffer('num_updates', torch.tensor(0,dtype=torch.int) if use_num_upates\n",
    "                             else torch.tensor(-1,dtype=torch.int))\n",
    "\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                #remove as '.'-character is not allowed in buffers\n",
    "                s_name = name.replace('.','')\n",
    "                self.m_name2s_name.update({name:s_name})\n",
    "                self.register_buffer(s_name,p.clone().detach().data)\n",
    "\n",
    "        self.collected_params = []\n",
    "\n",
    "    def forward(self,model):\n",
    "        decay = self.decay\n",
    "\n",
    "        if self.num_updates >= 0:\n",
    "            self.num_updates += 1\n",
    "            decay = min(self.decay,(1 + self.num_updates) / (10 + self.num_updates))\n",
    "\n",
    "        one_minus_decay = 1.0 - decay\n",
    "\n",
    "        with torch.no_grad():\n",
    "            m_param = dict(model.named_parameters())\n",
    "            shadow_params = dict(self.named_buffers())\n",
    "\n",
    "            for key in m_param:\n",
    "                if m_param[key].requires_grad:\n",
    "                    sname = self.m_name2s_name[key]\n",
    "                    shadow_params[sname] = shadow_params[sname].type_as(m_param[key])\n",
    "                    shadow_params[sname].sub_(one_minus_decay * (shadow_params[sname] - m_param[key]))\n",
    "                else:\n",
    "                    assert not key in self.m_name2s_name\n",
    "\n",
    "    def copy_to(self, model):\n",
    "        m_param = dict(model.named_parameters())\n",
    "        shadow_params = dict(self.named_buffers())\n",
    "        for key in m_param:\n",
    "            if m_param[key].requires_grad:\n",
    "                m_param[key].data.copy_(shadow_params[self.m_name2s_name[key]].data)\n",
    "            else:\n",
    "                assert not key in self.m_name2s_name\n",
    "\n",
    "    def store(self, parameters):\n",
    "        \"\"\"\n",
    "        Save the current parameters for restoring later.\n",
    "        Args:\n",
    "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
    "            temporarily stored.\n",
    "        \"\"\"\n",
    "        self.collected_params = [param.clone() for param in parameters]\n",
    "\n",
    "    def restore(self, parameters):\n",
    "        \"\"\"\n",
    "        Restore the parameters stored with the `store` method.\n",
    "        Useful to validate the model with EMA parameters without affecting the\n",
    "        original optimization process. Store the parameters before the\n",
    "        `copy_to` method. After validation (or model saving), use this to\n",
    "        restore the former parameters.\n",
    "        Args:\n",
    "          parameters: Iterable of `torch.nn.Parameter`; the parameters to be\n",
    "            updated with the stored parameters.\n",
    "        \"\"\"\n",
    "        for c_param, param in zip(self.collected_params, parameters):\n",
    "            param.data.copy_(c_param.data)\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from einops import rearrange, repeat\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "__conditioning_keys__ = {'concat': 'c_concat',\n",
    "                         'crossattn': 'c_crossattn',\n",
    "                         'adm': 'y'}\n",
    "\n",
    "def disabled_train(self, mode=True):\n",
    "    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n",
    "    does not change anymore.\"\"\"\n",
    "    return self\n",
    "\n",
    "\n",
    "def uniform_on_device(r1, r2, shape, device):\n",
    "    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n",
    "\n",
    "\n",
    "class DDPM(pl.LightningModule):\n",
    "    # classic DDPM with Gaussian diffusion, in image space\n",
    "    def __init__(self,\n",
    "                 unet_config,\n",
    "                 timesteps=1000,\n",
    "                 beta_schedule=\"linear\",\n",
    "                 loss_type=\"l2\",\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 load_only_unet=False,\n",
    "                 monitor=\"val/loss\",\n",
    "                 use_ema=True,\n",
    "                 first_stage_key=\"image\",\n",
    "                 image_size=256,\n",
    "                 channels=3,\n",
    "                 log_every_t=100,\n",
    "                 clip_denoised=True,\n",
    "                 linear_start=1e-4,\n",
    "                 linear_end=2e-2,\n",
    "                 cosine_s=8e-3,\n",
    "                 given_betas=None,\n",
    "                 original_elbo_weight=0.,\n",
    "                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n",
    "                 l_simple_weight=1.,\n",
    "                 conditioning_key=None,\n",
    "                 parameterization=\"eps\",  # all assuming fixed variance schedules\n",
    "                 scheduler_config=None,\n",
    "                 use_positional_encodings=False,\n",
    "                 learn_logvar=False,\n",
    "                 logvar_init=0.,\n",
    "                 ddim_steps=300,\n",
    "                 temperature=1.0,\n",
    "                 num_epoch=300\n",
    "                 ):\n",
    "        print(f'Loss Type is: {loss_type}')\n",
    "        super().__init__()\n",
    "        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n",
    "        self.parameterization = parameterization\n",
    "        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n",
    "        self.cond_stage_model = None\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.log_every_t = log_every_t\n",
    "        self.first_stage_key = first_stage_key\n",
    "        self.image_size = image_size  # try conv?\n",
    "        self.channels = channels\n",
    "        self.use_positional_encodings = use_positional_encodings\n",
    "        self.model = DiffusionWrapper(unet_config, conditioning_key)\n",
    "        self.ranFullValidation = False\n",
    "        self.num_epoch = num_epoch\n",
    "        count_params(self.model, verbose=True)\n",
    "        self.use_ema = use_ema\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self.model)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "        self.use_scheduler = scheduler_config is not None\n",
    "        if self.use_scheduler:\n",
    "            self.scheduler_config = scheduler_config\n",
    "\n",
    "        self.v_posterior = v_posterior\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.l_simple_weight = l_simple_weight\n",
    "\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys, only_model=load_only_unet)\n",
    "\n",
    "        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n",
    "                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n",
    "\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        self.learn_logvar = learn_logvar\n",
    "        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n",
    "        if self.learn_logvar:\n",
    "            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n",
    "\n",
    "        self.validation_count = 0\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.return_cond = False\n",
    "        self.output_path = None\n",
    "        self.main_config = None\n",
    "        self.best_val = 0.0\n",
    "        self.run_full_validation_threshold = 0.0\n",
    "        self.eval_avg = True\n",
    "        self.temperature = temperature\n",
    "        self.loss_dict = None\n",
    "        self.outputImageForEpoch = {}\n",
    "\n",
    "    def re_init_ema(self):\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self.model)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        if exists(given_betas):\n",
    "            betas = given_betas\n",
    "        else:\n",
    "            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n",
    "                                       cosine_s=cosine_s)\n",
    "        alphas = 1. - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        self.linear_start = linear_start\n",
    "        self.linear_end = linear_end\n",
    "        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n",
    "\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer('betas', to_torch(betas))\n",
    "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
    "        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n",
    "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n",
    "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n",
    "                    1. - alphas_cumprod) + self.v_posterior * betas\n",
    "        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n",
    "        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n",
    "        self.register_buffer('posterior_mean_coef1', to_torch(\n",
    "            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n",
    "        self.register_buffer('posterior_mean_coef2', to_torch(\n",
    "            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            lvlb_weights = self.betas ** 2 / (\n",
    "                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n",
    "        elif self.parameterization == \"x0\":\n",
    "            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n",
    "        else:\n",
    "            raise NotImplementedError(\"mu not supported\")\n",
    "        # TODO how to choose this term\n",
    "        lvlb_weights[0] = lvlb_weights[1]\n",
    "        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n",
    "        assert not torch.isnan(self.lvlb_weights).all()\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.model.parameters())\n",
    "            self.model_ema.copy_to(self.model)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.model.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list(), only_model=False):\n",
    "        sd = torch.load(path, map_location=\"cpu\")\n",
    "        if \"state_dict\" in list(sd.keys()):\n",
    "            sd = sd[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n",
    "            sd, strict=False)\n",
    "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "        if len(missing) > 0:\n",
    "            print(f\"Missing Keys: {missing}\")\n",
    "        if len(unexpected) > 0:\n",
    "            print(f\"Unexpected Keys: {unexpected}\")\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n",
    "        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        return (\n",
    "                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n",
    "                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        posterior_mean = (\n",
    "                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n",
    "                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, x, t, clip_denoised: bool):\n",
    "        model_out = self.model(x, t)\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n",
    "        noise = noise_like(x.shape, device, repeat_noise)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, shape, return_intermediates=False):\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        img = torch.randn(shape, device=device)\n",
    "        intermediates = [img]\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n",
    "            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n",
    "                                clip_denoised=self.clip_denoised)\n",
    "            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, return_intermediates=False):\n",
    "        image_size = self.image_size\n",
    "        channels = self.channels\n",
    "        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n",
    "                                  return_intermediates=return_intermediates)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
    "\n",
    "    def get_loss(self, pred, target, mean=True):\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = (target - pred).abs()\n",
    "            if mean:\n",
    "                loss = loss.mean()\n",
    "        elif self.loss_type == 'l2':\n",
    "            if mean:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred)\n",
    "            else:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        model_out = self.model(x_noisy, t)\n",
    "\n",
    "        loss_dict = {}\n",
    "        if self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        elif self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Paramterization {self.parameterization} not yet supported\")\n",
    "\n",
    "        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n",
    "\n",
    "        log_prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n",
    "        loss_simple = loss.mean() * self.l_simple_weight\n",
    "\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n",
    "        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n",
    "\n",
    "        loss = loss_simple + self.original_elbo_weight * loss_vlb\n",
    "\n",
    "        loss_dict.update({f'{log_prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
    "        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        return self.p_losses(x, t, *args, **kwargs)\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = rearrange(x, 'b h w c -> b c h w')\n",
    "        x = x.to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def shared_step(self, batch):\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        loss, loss_dict = self(x)\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        self.cond_stage_model.train()  ###\n",
    "\n",
    "        loss, loss_dict = self.shared_step(batch)\n",
    "\n",
    "        self.log_dict(loss_dict, prog_bar=True,\n",
    "                    logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            lr = self.optimizers().param_groups[0]['lr']\n",
    "            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, data, num_samples, ddim_steps=150, HW=None, limit=None, state=None):\n",
    "        # fmri_embedding: n, seq_len, embed_dim\n",
    "        all_samples = []\n",
    "        if HW is None:\n",
    "            shape = (self.p_channels,\n",
    "                self.p_image_size, self.p_image_size)\n",
    "        else:\n",
    "            num_resolutions = len(self.ch_mult)\n",
    "            shape = (self.p_channels,\n",
    "                HW[0] // 2**(num_resolutions-1), HW[1] // 2**(num_resolutions-1))\n",
    "\n",
    "        model = self\n",
    "        sampler = PLMSSampler(model, self.temperature)\n",
    "        # sampler = DDIMSampler(model)\n",
    "        model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            state = torch.cuda.get_rng_state() if state is None else state\n",
    "            torch.cuda.set_rng_state(state)\n",
    "        else:\n",
    "            state = torch.get_rng_state() if state is None else state\n",
    "            torch.set_rng_state(state)\n",
    "\n",
    "        # rng = torch.Generator(device=self.device).manual_seed(2022).set_state(state)\n",
    "\n",
    "        # state = torch.cuda.get_rng_state()\n",
    "        with model.ema_scope():\n",
    "            for count, item in enumerate(zip(data['eeg'], data['image'])):\n",
    "                if limit is not None:\n",
    "                    if count >= limit:\n",
    "                        break\n",
    "                latent = item[0] # fmri embedding\n",
    "                gt_image = rearrange(item[1], 'h w c -> 1 c h w') # h w c\n",
    "                print(f\"rendering {num_samples} examples in {ddim_steps} steps.\")\n",
    "                # c = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                c, re_latent = model.get_learned_conditioning(repeat(latent, 'h w -> c h w', c=num_samples).to(self.device))\n",
    "                samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=num_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False,\n",
    "                                                generator=None,\n",
    "                                                temperature=self.temperature)\n",
    "\n",
    "                x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,min=0.0, max=1.0)\n",
    "                gt_image = torch.clamp((gt_image+1.0)/2.0,min=0.0, max=1.0)\n",
    "\n",
    "                all_samples.append(torch.cat([gt_image.detach().cpu(), x_samples_ddim.detach().cpu()], dim=0)) # put groundtruth at first\n",
    "\n",
    "        # display as grid\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=num_samples+1)\n",
    "\n",
    "        # to image\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        return grid, (255. * torch.stack(all_samples, 0).cpu().numpy()).astype(np.uint8), state\n",
    "\n",
    "    def save_images(self, all_samples, suffix='0'):\n",
    "        # print('output_path')\n",
    "        # print(self.output_path)\n",
    "        if self.output_path is not None:\n",
    "            os.makedirs(os.path.join(self.output_path, 'val', f'{self.validation_count}_{suffix}'), exist_ok=True)\n",
    "            for sp_idx, imgs in enumerate(all_samples):\n",
    "                # for copy_idx, img in enumerate(imgs[1:]):\n",
    "                for copy_idx, img in enumerate(imgs):\n",
    "                    img = rearrange(img, 'c h w -> h w c')\n",
    "                    Image.fromarray(img).save(os.path.join(self.output_path, 'val',\n",
    "                                    f'{self.validation_count}_{suffix}', f'test{sp_idx}-{copy_idx}.png'))\n",
    "                    \n",
    "    def log_losses_progression(self, loss_dict, output_path):\n",
    "        try:\n",
    "            # Ensure the output directory exists\n",
    "            if not os.path.exists(output_path):\n",
    "                os.makedirs(output_path)\n",
    "\n",
    "            # Path for the performance.txt file\n",
    "            file_path = os.path.join(output_path, 'losses.txt')\n",
    "\n",
    "            # Writing the metrics dictionary to the file\n",
    "            with open(file_path, 'w') as file:\n",
    "                for key, value in loss_dict.items():\n",
    "                    file.write(f'{key}: {value}\\n')\n",
    "        except Exception as e:\n",
    "            # Log the exception\n",
    "            print(f\"Error occurred in log_losses_progression: {e}\")\n",
    "        \n",
    "\n",
    "    def full_validation(self, batch, state=None):\n",
    "        print('###### run full validation! ######\\n')\n",
    "        grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=3, limit=None, state=state)\n",
    "        #metric, metric_list = self.get_eval_metric(all_samples)\n",
    "        self.save_images(all_samples)\n",
    "        #metric_dict = {f'val/{k}_full':v for k, v in zip(metric_list, metric)}\n",
    "        # self.logger.log_metrics(metric_dict)\n",
    "        grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "        if self.loss_dict is not None:\n",
    "            self.log_losses_progression(self.loss_progression, os.path.join(self.output_path))\n",
    "        # self.logger.log_image(key=f'samples_test_full', images=[grid_imgs])\n",
    "        # if metric[-1] > self.best_val:\n",
    "        #     self.best_val = metric[-1]\n",
    "        torch.save(\n",
    "            {\n",
    "                'model_state_dict': self.state_dict(),\n",
    "                'config': self.main_config,\n",
    "                'state': state\n",
    "\n",
    "            },\n",
    "            os.path.join(self.output_path, 'checkpoint_best.pth')\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # if batch_idx != 0:\n",
    "        #     return\n",
    "        print(self.trainer.current_epoch)\n",
    "        if self.trainer.current_epoch == self.num_epoch - 1 and not self.ranFullValidation:\n",
    "            self.full_validation(batch)\n",
    "            self.ranFullValidation = True\n",
    "        if not self.outputImageForEpoch.get(self.trainer.current_epoch, False):\n",
    "            self.outputImageForEpoch[self.trainer.current_epoch] = True\n",
    "            grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=1, limit=1)\n",
    "            grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "            grid_imgs.save(os.path.join(self.output_path, f'samples_test-{self.trainer.current_epoch}.png'))\n",
    "            \n",
    "        # no step validation for now because there isn't any stopping mechanism or hyperparameters tuning implemented\n",
    "        # else:\n",
    "        #     # pass\n",
    "        #     grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=1, limit=5)\n",
    "        #     metric, metric_list = self.get_eval_metric(all_samples, avg=self.eval_avg)\n",
    "        #     grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "        #     # self.logger.log_image(key=f'samples_test', images=[grid_imgs])\n",
    "        #     metric_dict = {f'val/{k}':v for k, v in zip(metric_list, metric)}\n",
    "        #     # self.logger.log_metrics(metric_dict)\n",
    "        #     # if metric[-1] > self.run_full_validation_threshold:\n",
    "        #     #     self.full_validation(batch, state=state)\n",
    "        # self.validation_count += 1\n",
    "\n",
    "    def get_eval_metric(self, samples, avg=True):\n",
    "        metric_list = ['mse', 'pcc', 'ssim', 'psm']\n",
    "        res_list = []\n",
    "\n",
    "        gt_images = [img[0] for img in samples]\n",
    "        gt_images = rearrange(np.stack(gt_images), 'n c h w -> n h w c')\n",
    "        samples_to_run = np.arange(1, len(samples[0])) if avg else [1]\n",
    "        for m in metric_list:\n",
    "            res_part = []\n",
    "            for s in samples_to_run:\n",
    "                pred_images = [img[s] for img in samples]\n",
    "                pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "                res = get_similarity_metric(pred_images, gt_images, method='pair-wise', metric_name=m)\n",
    "                res_part.append(np.mean(res))\n",
    "            res_list.append(np.mean(res_part))\n",
    "        res_part = []\n",
    "        # Skip multi class for now\n",
    "        # for s in samples_to_run:\n",
    "        #     pred_images = [img[s] for img in samples]\n",
    "        #     pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "        #     res = get_similarity_metric(pred_images, gt_images, 'class', None,\n",
    "        #                     n_way=50, num_trials=50, top_k=1, device='cuda')\n",
    "        #     res_part.append(np.mean(res))\n",
    "        # res_list.append(np.mean(res_part))\n",
    "        # res_list.append(np.max(res_part))\n",
    "        # metric_list.append('top-1-class')\n",
    "        # metric_list.append('top-1-class (max)')\n",
    "\n",
    "        return res_list, metric_list\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self.model)\n",
    "\n",
    "    def _get_rows_from_list(self, samples):\n",
    "        n_imgs_per_row = len(samples)\n",
    "        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.first_stage_key)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        x = x.to(self.device)[:N]\n",
    "        log[\"inputs\"] = x\n",
    "\n",
    "        # get diffusion row\n",
    "        diffusion_row = list()\n",
    "        x_start = x[:n_row]\n",
    "\n",
    "        for t in range(self.num_timesteps):\n",
    "            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                t = t.to(self.device).long()\n",
    "                noise = torch.randn_like(x_start)\n",
    "                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "                diffusion_row.append(x_noisy)\n",
    "\n",
    "        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with self.ema_scope(\"Plotting\"):\n",
    "                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n",
    "\n",
    "            log[\"samples\"] = samples\n",
    "            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        params = list(self.model.parameters())\n",
    "        if self.learn_logvar:\n",
    "            params = params + [self.logvar]\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt\n",
    "\n",
    "\n",
    "class LatentDiffusion(DDPM):\n",
    "    \"\"\"main class\"\"\"\n",
    "    def __init__(self,\n",
    "                first_stage_config,\n",
    "                cond_stage_config,\n",
    "                num_timesteps_cond=None,\n",
    "                cond_stage_key=\"image\",\n",
    "                cond_stage_trainable=True,\n",
    "                concat_mode=True,\n",
    "                cond_stage_forward=None,\n",
    "                conditioning_key=None,\n",
    "                scale_factor=1.0,\n",
    "                scale_by_std=False,\n",
    "                temperature=1.0,\n",
    "                *args, **kwargs):\n",
    "        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n",
    "        self.scale_by_std = scale_by_std\n",
    "        assert self.num_timesteps_cond <= kwargs['timesteps']\n",
    "        # for backwards compatibility after implementation of DiffusionWrapper\n",
    "        if conditioning_key is None:\n",
    "            conditioning_key = 'concat' if concat_mode else 'crossattn'\n",
    "        if cond_stage_config == '__is_unconditional__':\n",
    "            conditioning_key = None\n",
    "        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n",
    "        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n",
    "        super().__init__(conditioning_key=conditioning_key, *args, **kwargs, temperature=temperature)\n",
    "        self.concat_mode = concat_mode\n",
    "        self.cond_stage_trainable = cond_stage_trainable\n",
    "        self.cond_stage_key = cond_stage_key\n",
    "        try:\n",
    "            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n",
    "        except:\n",
    "            self.num_downs = 0\n",
    "        if not scale_by_std:\n",
    "            self.scale_factor = scale_factor\n",
    "        else:\n",
    "            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        self.instantiate_cond_stage(cond_stage_config)\n",
    "\n",
    "        self.cond_stage_forward = cond_stage_forward\n",
    "        self.clip_denoised = False\n",
    "        self.bbox_tokenizer = None\n",
    "\n",
    "        self.restarted_from_ckpt = False\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys)\n",
    "            self.restarted_from_ckpt = True\n",
    "        self.train_cond_stage_only = False\n",
    "        self.clip_tune = True\n",
    "        if self.clip_tune:\n",
    "            self.image_embedder = FrozenImageEmbedder()\n",
    "        self.cls_tune = False\n",
    "        self.temperature = temperature\n",
    "        self.loss_progression = {}\n",
    "        self.loss_progression_epochs = {}\n",
    "        self.last_epoch = None\n",
    "\n",
    "    def make_cond_schedule(self, ):\n",
    "        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n",
    "        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n",
    "        self.cond_ids[:self.num_timesteps_cond] = ids\n",
    "\n",
    "    @rank_zero_only\n",
    "    @torch.no_grad()\n",
    "    def on_train_batch_start(self, batch, batch_idx):\n",
    "        # only for very first batch\n",
    "        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n",
    "            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n",
    "            # set rescale weight to 1./std of encodings\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "            x = super().get_input(batch, self.first_stage_key)\n",
    "            x = x.to(self.device)\n",
    "            encoder_posterior = self.encode_first_stage(x)\n",
    "            z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "            del self.scale_factor\n",
    "            self.register_buffer('scale_factor', 1. / z.flatten().std())\n",
    "            print(f\"setting self.scale_factor to {self.scale_factor}\")\n",
    "            print(\"### USING STD-RESCALING ###\")\n",
    "\n",
    "    def register_schedule(self,\n",
    "                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n",
    "                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
    "        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n",
    "\n",
    "        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n",
    "        if self.shorten_cond_schedule:\n",
    "            self.make_cond_schedule()\n",
    "\n",
    "    def instantiate_first_stage(self, config):\n",
    "        model = instantiate_from_config(config)\n",
    "        self.first_stage_model = model.eval()\n",
    "\n",
    "    def freeze_diffusion_model(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_diffusion_model(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_cond_stage(self):\n",
    "        for param in self.cond_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_cond_stage(self):\n",
    "        for param in self.cond_stage_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def freeze_first_stage(self):\n",
    "        self.first_stage_model.trainable = False\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_first_stage(self):\n",
    "        self.first_stage_model.trainable = True\n",
    "        for param in self.first_stage_model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_whole_model(self):\n",
    "        self.first_stage_model.trainable = False\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_whole_model(self):\n",
    "        self.first_stage_model.trainable = True\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def instantiate_cond_stage(self, config):\n",
    "        if not self.cond_stage_trainable:\n",
    "            if config == \"__is_first_stage__\":\n",
    "                print(\"Using first stage also as cond stage.\")\n",
    "                self.cond_stage_model = self.first_stage_model\n",
    "            elif config == \"__is_unconditional__\":\n",
    "                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n",
    "                self.cond_stage_model = None\n",
    "                # self.be_unconditional = True\n",
    "            else:\n",
    "                model = instantiate_from_config(config)\n",
    "                self.cond_stage_model = model.eval()\n",
    "                # self.cond_stage_model.train = disabled_train\n",
    "                for param in self.cond_stage_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            assert config != '__is_first_stage__'\n",
    "            assert config != '__is_unconditional__'\n",
    "            model = instantiate_from_config(config)\n",
    "            self.cond_stage_model = model\n",
    "\n",
    "    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n",
    "        denoise_row = []\n",
    "        for zd in tqdm(samples, desc=desc):\n",
    "            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n",
    "                                                            force_not_quantize=force_no_decoder_quantization))\n",
    "        n_imgs_per_row = len(denoise_row)\n",
    "        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n",
    "        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n",
    "        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n",
    "        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n",
    "        return denoise_grid\n",
    "\n",
    "    def get_first_stage_encoding(self, encoder_posterior):\n",
    "        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n",
    "            z = encoder_posterior.sample()\n",
    "        elif isinstance(encoder_posterior, torch.Tensor):\n",
    "            z = encoder_posterior\n",
    "        else:\n",
    "            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n",
    "        return self.scale_factor * z\n",
    "\n",
    "    def get_learned_conditioning(self, c):\n",
    "        # self.cond_stage_model.eval()\n",
    "        if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
    "            c, re_latent = self.cond_stage_model.encode(c)\n",
    "            # c = self.cond_stage_model.encode(c)\n",
    "        else:\n",
    "            c, re_latent = self.cond_stage_model(c)\n",
    "            # c = self.cond_stage_model(c)\n",
    "        # return c\n",
    "        return c, re_latent\n",
    "\n",
    "    def meshgrid(self, h, w):\n",
    "        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n",
    "        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n",
    "\n",
    "        arr = torch.cat([y, x], dim=-1)\n",
    "        return arr\n",
    "\n",
    "    def delta_border(self, h, w):\n",
    "        \"\"\"\n",
    "        :param h: height\n",
    "        :param w: width\n",
    "        :return: normalized distance to image border,\n",
    "         wtith min distance = 0 at border and max dist = 0.5 at image center\n",
    "        \"\"\"\n",
    "        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n",
    "        arr = self.meshgrid(h, w) / lower_right_corner\n",
    "        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n",
    "        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n",
    "        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n",
    "        return edge_dist\n",
    "\n",
    "    def get_weighting(self, h, w, Ly, Lx, device):\n",
    "        weighting = self.delta_border(h, w)\n",
    "        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n",
    "                               self.split_input_params[\"clip_max_weight\"], )\n",
    "        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n",
    "\n",
    "        if self.split_input_params[\"tie_braker\"]:\n",
    "            L_weighting = self.delta_border(Ly, Lx)\n",
    "            L_weighting = torch.clip(L_weighting,\n",
    "                                     self.split_input_params[\"clip_min_tie_weight\"],\n",
    "                                     self.split_input_params[\"clip_max_tie_weight\"])\n",
    "\n",
    "            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n",
    "            weighting = weighting * L_weighting\n",
    "        return weighting\n",
    "\n",
    "    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n",
    "        \"\"\"\n",
    "        :param x: img of size (bs, c, h, w)\n",
    "        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n",
    "        \"\"\"\n",
    "        bs, nc, h, w = x.shape\n",
    "\n",
    "        # number of crops in image\n",
    "        Ly = (h - kernel_size[0]) // stride[0] + 1\n",
    "        Lx = (w - kernel_size[1]) // stride[1] + 1\n",
    "\n",
    "        if uf == 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n",
    "\n",
    "        elif uf > 1 and df == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] * uf, stride[1] * uf))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n",
    "\n",
    "        elif df > 1 and uf == 1:\n",
    "            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n",
    "            unfold = torch.nn.Unfold(**fold_params)\n",
    "\n",
    "            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n",
    "                                dilation=1, padding=0,\n",
    "                                stride=(stride[0] // df, stride[1] // df))\n",
    "            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n",
    "\n",
    "            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n",
    "            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n",
    "            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return fold, unfold, normalization, weighting\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n",
    "                  cond_key=None, return_original_cond=False, bs=None):\n",
    "        x = super().get_input(batch, k)\n",
    "        if bs is not None:\n",
    "            x = x[:bs]\n",
    "        x = x.to(self.device)\n",
    "        encoder_posterior = self.encode_first_stage(x)\n",
    "        # print('encoder_posterior.shape')\n",
    "        # print(encoder_posterior.shape)\n",
    "        z = self.get_first_stage_encoding(encoder_posterior).detach()\n",
    "        # print('z.shape')\n",
    "        # print(z.shape)\n",
    "        # print(cond_key)\n",
    "        # print(self.cond_stage_key)\n",
    "        # print(cond_key)\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if cond_key is None:\n",
    "                cond_key = self.cond_stage_key\n",
    "            if cond_key != self.first_stage_key:\n",
    "                if cond_key in ['caption', 'coordinates_bbox','fmri', 'eeg']:\n",
    "                    xc = batch[cond_key]\n",
    "                elif cond_key == 'class_label':\n",
    "                    xc = batch\n",
    "                else:\n",
    "                    xc = super().get_input(batch, cond_key).to(self.device)\n",
    "            else:\n",
    "                xc = x\n",
    "            # print('get input')\n",
    "            # print(not self.cond_stage_trainable)\n",
    "            # print(force_c_encode)\n",
    "            if not self.cond_stage_trainable or force_c_encode :\n",
    "                # print('get learned condition')\n",
    "                if isinstance(xc, dict) or isinstance(xc, list):\n",
    "                    # import pudb; pudb.set_trace()\n",
    "                    c, re_latent = self.get_learned_conditioning(xc)\n",
    "                    # c = self.get_learned_conditioning(xc)\n",
    "                else:\n",
    "                    c, re_latent = self.get_learned_conditioning(xc.to(self.device))\n",
    "                    # c = self.get_learned_conditioning(xc.to(self.device))\n",
    "            else:\n",
    "                c = xc\n",
    "            if bs is not None:\n",
    "                c = c[:bs]\n",
    "\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "\n",
    "        else:\n",
    "            c = None\n",
    "            xc = None\n",
    "            if self.use_positional_encodings:\n",
    "                pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "                c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "        out = [z, c , batch['image_raw']]\n",
    "        if return_first_stage_outputs:\n",
    "            xrec = self.decode_first_stage(z)\n",
    "            out.extend([x, xrec])\n",
    "        if return_original_cond:\n",
    "            out.append(xc)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        z = 1. / self.scale_factor * z\n",
    "\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                uf = self.split_input_params[\"vqf\"]\n",
    "                bs, nc, h, w = z.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n",
    "\n",
    "                z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
    "                # 1. Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                # 2. apply model loop over last dim\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n",
    "                                                                 force_not_quantize=predict_cids or force_not_quantize)\n",
    "                                   for i in range(z.shape[-1])]\n",
    "                else:\n",
    "\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n",
    "                                   for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
    "                o = o * weighting\n",
    "                # Reverse 1. reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
    "                return decoded\n",
    "            else:\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "                else:\n",
    "                    return self.first_stage_model.decode(z)\n",
    "\n",
    "        else:\n",
    "            if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "            else:\n",
    "                return self.first_stage_model.decode(z)\n",
    "\n",
    "    # same as above but without decorator\n",
    "    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n",
    "        if predict_cids:\n",
    "            if z.dim() == 4:\n",
    "                z = torch.argmax(z.exp(), dim=1).long()\n",
    "            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n",
    "            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        z = 1. / self.scale_factor * z\n",
    "\n",
    "        if hasattr(self, \"split_input_params\"):\n",
    "            if self.split_input_params[\"patch_distributed_vq\"]:\n",
    "                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n",
    "                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n",
    "                uf = self.split_input_params[\"vqf\"]\n",
    "                bs, nc, h, w = z.shape\n",
    "                if ks[0] > h or ks[1] > w:\n",
    "                    ks = (min(ks[0], h), min(ks[1], w))\n",
    "                    print(\"reducing Kernel\")\n",
    "\n",
    "                if stride[0] > h or stride[1] > w:\n",
    "                    stride = (min(stride[0], h), min(stride[1], w))\n",
    "                    print(\"reducing stride\")\n",
    "\n",
    "                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n",
    "\n",
    "                z = unfold(z)  # (bn, nc * prod(**ks), L)\n",
    "                # 1. Reshape to img shape\n",
    "                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n",
    "\n",
    "                # 2. apply model loop over last dim\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n",
    "                                                                 force_not_quantize=predict_cids or force_not_quantize)\n",
    "                                   for i in range(z.shape[-1])]\n",
    "                else:\n",
    "\n",
    "                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n",
    "                                   for i in range(z.shape[-1])]\n",
    "\n",
    "                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n",
    "                o = o * weighting\n",
    "                # Reverse 1. reshape to img shape\n",
    "                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n",
    "                # stitch crops together\n",
    "                decoded = fold(o)\n",
    "                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n",
    "                return decoded\n",
    "            else:\n",
    "                if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "                else:\n",
    "                    return self.first_stage_model.decode(z)\n",
    "\n",
    "        else:\n",
    "            if isinstance(self.first_stage_model, VQModelInterface):\n",
    "                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n",
    "            else:\n",
    "                return self.first_stage_model.decode(z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_first_stage(self, x):\n",
    "        return self.first_stage_model.encode(x)\n",
    "\n",
    "    def shared_step(self, batch, **kwargs):\n",
    "        self.freeze_first_stage()\n",
    "        # print('share step\\'s get input')\n",
    "        x, c, image_raw = self.get_input(batch, self.first_stage_key)\n",
    "        # print('get input shape')\n",
    "        # print('x.shape')\n",
    "        # print(x.shape)\n",
    "        # print('c.shape')\n",
    "        # print(c.shape)\n",
    "        if self.return_cond:\n",
    "            loss, cc = self(x, c, image_raw)\n",
    "            return loss, cc\n",
    "        else:\n",
    "            loss = self(x, c, image_raw)\n",
    "            return loss\n",
    "\n",
    "    def forward(self, x, c, image_raw, *args, **kwargs):\n",
    "        # print(self.num_timesteps)\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "        # print('t.shape')\n",
    "        # print(t.shape)\n",
    "        if self.model.conditioning_key is not None:\n",
    "            assert c is not None\n",
    "            imgs = c\n",
    "            if self.cond_stage_trainable:\n",
    "                # c = self.get_learned_conditioning(c)\n",
    "                c, re_latent = self.get_learned_conditioning(c)\n",
    "                # print('c.shape')\n",
    "                # print(c.shape)\n",
    "\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "        loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "        # pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "        # rencon = self.cond_stage_model.recon(re_latent)\n",
    "        if self.clip_tune:\n",
    "            image_embeds = self.image_embedder(image_raw)\n",
    "            loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "        # loss_recon = self.recon_loss(imgs, rencon)\n",
    "        # loss_cls = self.cls_loss(label, pre_cls)\n",
    "            scaledLossClip = (loss_clip)\n",
    "            loss += scaledLossClip\n",
    "        # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "        # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "            loss_dict.update({f'{prefix}/loss_clip': scaledLossClip})\n",
    "        # if self.cls_tune:\n",
    "        #     pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "        #     loss_cls = self.cls_loss(label, pre_cls)\n",
    "            # image_embeds = self.image_embedder(image_raw)\n",
    "            # loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "        # loss_recon = self.recon_loss(imgs, rencon)\n",
    "        # loss_cls = self.cls_loss(label, pre_cls)\n",
    "            #loss += loss_cls\n",
    "        # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "        # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "        # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "            #loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "                # if self.return_cond:\n",
    "                    # return self.p_losses(x, c, t, *args, **kwargs), c\n",
    "        # return self.p_losses(x, c, t, *args, **kwargs)\n",
    "        self.loss_dict = loss_dict\n",
    "\n",
    "        currentEpoch = self.trainer.current_epoch\n",
    "        if (self.last_epoch is not None and currentEpoch != self.last_epoch):\n",
    "            self.on_epoch_end_m(self.last_epoch)\n",
    "            self.loss_progression_epochs[self.last_epoch] = {}\n",
    "        \n",
    "        self.last_epoch = currentEpoch\n",
    "        \n",
    "        if currentEpoch not in self.loss_progression_epochs:\n",
    "            # Initialize the loss storage for the current epoch\n",
    "            self.loss_progression_epochs[currentEpoch] = {'loss': [], 'loss_clip': []}\n",
    "\n",
    "        # Append the current loss and loss_clip to the list for the current epoch\n",
    "        self.loss_progression_epochs[currentEpoch]['loss'].append(loss.item())\n",
    "        self.loss_progression_epochs[currentEpoch]['loss_clip'].append(loss_clip.item())\n",
    "\n",
    "        if self.return_cond:\n",
    "            return loss, loss_dict, c\n",
    "        return loss, loss_dict\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def on_epoch_end_m(self, currentEpoch):\n",
    "        if currentEpoch in self.loss_progression_epochs:\n",
    "            epoch_losses = self.loss_progression_epochs[currentEpoch]['loss']\n",
    "            epoch_loss_clips = self.loss_progression_epochs[currentEpoch]['loss_clip']\n",
    "\n",
    "            # Calculate mean loss and loss_clip\n",
    "            mean_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "            mean_loss_clip = sum(epoch_loss_clips) / len(epoch_loss_clips)\n",
    "            \n",
    "            print(f'loss end, here are the losses, loss: {mean_loss}, clip_loss: {mean_loss_clip}')\n",
    "\n",
    "            # Update the dictionary with mean values\n",
    "            self.loss_progression[currentEpoch] = {\n",
    "                'mean_loss': mean_loss,\n",
    "                'mean_loss_clip': mean_loss_clip,\n",
    "                'epoch': currentEpoch\n",
    "            }\n",
    "    \n",
    "    # def recon_loss(self, )\n",
    "    def recon_loss(self, imgs, pred):\n",
    "        \"\"\"\n",
    "        imgs: [N, 1, num_voxels]\n",
    "        pred: [N, L, p]\n",
    "        mask: [N, L], 0 is keep, 1 is remove,\n",
    "        \"\"\"\n",
    "        # target = self.patchify(imgs)\n",
    "\n",
    "        loss = (pred - imgs) ** 2\n",
    "        loss = loss.mean()\n",
    "        # loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        # loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
    "        return loss\n",
    "    # def cls_loss(self, label, pred):\n",
    "    #     return torch.nn.CrossEntropyLoss()(pred, label)\n",
    "\n",
    "    def _rescale_annotations(self, bboxes, crop_coordinates):  # TODO: move to dataset\n",
    "        def rescale_bbox(bbox):\n",
    "            x0 = torch.clamp((bbox[0] - crop_coordinates[0]) / crop_coordinates[2])\n",
    "            y0 = torch.clamp((bbox[1] - crop_coordinates[1]) / crop_coordinates[3])\n",
    "            w = min(bbox[2] / crop_coordinates[2], 1 - x0)\n",
    "            h = min(bbox[3] / crop_coordinates[3], 1 - y0)\n",
    "            return x0, y0, w, h\n",
    "\n",
    "        return [rescale_bbox(b) for b in bboxes]\n",
    "\n",
    "    def apply_model(self, x_noisy, t, cond, return_ids=False):\n",
    "\n",
    "        if isinstance(cond, dict):\n",
    "            # hybrid case, cond is exptected to be a dict\n",
    "            pass\n",
    "        else:\n",
    "            if not isinstance(cond, list):\n",
    "                cond = [cond]\n",
    "            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n",
    "            cond = {key: cond}\n",
    "\n",
    "        x_recon = self.model(x_noisy, t, **cond)\n",
    "        # print('x_recon')\n",
    "        # if isinstance(x_recon, tuple):\n",
    "        #     print('is tuple')\n",
    "        #     # print(len(x_recon))\n",
    "        #     # print(x_recon[0].shape)\n",
    "        # else:\n",
    "        #     print(x_recon.shape)\n",
    "\n",
    "        if isinstance(x_recon, tuple) and not return_ids:\n",
    "            return x_recon[0]\n",
    "        else:\n",
    "            return x_recon\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n",
    "               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def _prior_bpd(self, x_start):\n",
    "        \"\"\"\n",
    "        Get the prior KL term for the variational lower-bound, measured in\n",
    "        bits-per-dim.\n",
    "        This term can't be optimized, as it only depends on the encoder.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :return: a batch of [N] KL values (in bits), one per batch element.\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
    "        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n",
    "        return mean_flat(kl_prior) / np.log(2.0)\n",
    "\n",
    "    def p_losses(self, x_start, cond, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        # print('p_losses')\n",
    "        # print('noise.shape')\n",
    "        # print(noise.shape)\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        # print('x_noisy[0].shape')\n",
    "        # print(x_noisy[0].shape)\n",
    "        model_output = self.apply_model(x_noisy, t, cond)\n",
    "\n",
    "        loss_dict = {}\n",
    "        prefix = 'train' if self.training else 'val'\n",
    "\n",
    "        if self.parameterization == \"x0\":\n",
    "            target = x_start\n",
    "        elif self.parameterization == \"eps\":\n",
    "            target = noise\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
    "        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n",
    "\n",
    "        logvar_t = self.logvar[t].to(self.device)\n",
    "        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n",
    "        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n",
    "        if self.learn_logvar:\n",
    "            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n",
    "            loss_dict.update({'logvar': self.logvar.data.mean()})\n",
    "\n",
    "        loss = self.l_simple_weight * loss.mean()\n",
    "\n",
    "        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n",
    "        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n",
    "        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n",
    "        loss += (self.original_elbo_weight * loss_vlb)\n",
    "        loss_dict.update({f'{prefix}/loss': loss})\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n",
    "                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n",
    "        t_in = t\n",
    "        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n",
    "\n",
    "        if score_corrector is not None:\n",
    "            assert self.parameterization == \"eps\"\n",
    "            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n",
    "\n",
    "        if return_codebook_ids:\n",
    "            model_out, logits = model_out\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n",
    "        elif self.parameterization == \"x0\":\n",
    "            x_recon = model_out\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_recon.clamp_(-1., 1.)\n",
    "        if quantize_denoised:\n",
    "            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n",
    "        if return_codebook_ids:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, logits\n",
    "        elif return_x0:\n",
    "            return model_mean, posterior_variance, posterior_log_variance, x_recon\n",
    "        else:\n",
    "            return model_mean, posterior_variance, posterior_log_variance\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n",
    "                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n",
    "                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n",
    "        b, *_, device = *x.shape, x.device\n",
    "        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n",
    "                                       return_codebook_ids=return_codebook_ids,\n",
    "                                       quantize_denoised=quantize_denoised,\n",
    "                                       return_x0=return_x0,\n",
    "                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "        if return_x0:\n",
    "            model_mean, _, model_log_variance, x0 = outputs\n",
    "        else:\n",
    "            model_mean, _, model_log_variance = outputs\n",
    "\n",
    "        noise = noise_like(x.shape, device, repeat_noise) * temperature\n",
    "        if noise_dropout > 0.:\n",
    "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
    "        # no noise when t == 0\n",
    "        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n",
    "\n",
    "        if return_x0:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n",
    "        else:\n",
    "            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n",
    "                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n",
    "                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n",
    "                              log_every_t=None):\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        timesteps = self.num_timesteps\n",
    "        if batch_size is not None:\n",
    "            b = batch_size if batch_size is not None else shape[0]\n",
    "            shape = [batch_size] + list(shape)\n",
    "        else:\n",
    "            b = batch_size = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=self.device)\n",
    "        else:\n",
    "            img = x_T\n",
    "        intermediates = []\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n",
    "                        total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "        if type(temperature) == float:\n",
    "            temperature = [temperature] * timesteps\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img, x0_partial = self.p_sample(img, cond, ts,\n",
    "                                            clip_denoised=self.clip_denoised,\n",
    "                                            quantize_denoised=quantize_denoised, return_x0=True,\n",
    "                                            temperature=temperature[i], noise_dropout=noise_dropout,\n",
    "                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n",
    "            if mask is not None:\n",
    "                assert x0 is not None\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(x0_partial)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "        return img, intermediates\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, cond, shape, return_intermediates=False,\n",
    "                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n",
    "                      mask=None, x0=None, img_callback=None, start_T=None,\n",
    "                      log_every_t=None):\n",
    "\n",
    "        if not log_every_t:\n",
    "            log_every_t = self.log_every_t\n",
    "        device = self.betas.device\n",
    "        b = shape[0]\n",
    "        if x_T is None:\n",
    "            img = torch.randn(shape, device=device)\n",
    "        else:\n",
    "            img = x_T\n",
    "\n",
    "        intermediates = [img]\n",
    "        if timesteps is None:\n",
    "            timesteps = self.num_timesteps\n",
    "\n",
    "        if start_T is not None:\n",
    "            timesteps = min(timesteps, start_T)\n",
    "        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n",
    "            range(0, timesteps))\n",
    "\n",
    "        if mask is not None:\n",
    "            assert x0 is not None\n",
    "            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n",
    "\n",
    "        for i in iterator:\n",
    "            ts = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            if self.shorten_cond_schedule:\n",
    "                assert self.model.conditioning_key != 'hybrid'\n",
    "                tc = self.cond_ids[ts].to(cond.device)\n",
    "                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n",
    "\n",
    "            img = self.p_sample(img, cond, ts,\n",
    "                                clip_denoised=self.clip_denoised,\n",
    "                                quantize_denoised=quantize_denoised)\n",
    "            if mask is not None:\n",
    "                img_orig = self.q_sample(x0, ts)\n",
    "                img = img_orig * mask + (1. - mask) * img\n",
    "\n",
    "            if i % log_every_t == 0 or i == timesteps - 1:\n",
    "                intermediates.append(img)\n",
    "            if callback: callback(i)\n",
    "            if img_callback: img_callback(img, i)\n",
    "\n",
    "        if return_intermediates:\n",
    "            return img, intermediates\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n",
    "               verbose=True, timesteps=None, quantize_denoised=False,\n",
    "               mask=None, x0=None, shape=None,**kwargs):\n",
    "        if shape is None:\n",
    "            shape = (batch_size, self.channels, self.image_size, self.image_size)\n",
    "        if cond is not None:\n",
    "            if isinstance(cond, dict):\n",
    "                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n",
    "                list(map(lambda x: x[:batch_size], cond[key])) for key in cond}\n",
    "            else:\n",
    "                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n",
    "        return self.p_sample_loop(cond,\n",
    "                                  shape,\n",
    "                                  return_intermediates=return_intermediates, x_T=x_T,\n",
    "                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n",
    "                                  mask=mask, x0=x0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample_log(self,cond,batch_size,ddim, ddim_steps,**kwargs):\n",
    "\n",
    "        if ddim:\n",
    "            ddim_sampler = DDIMSampler(self)\n",
    "            shape = (self.channels, self.image_size, self.image_size)\n",
    "            samples, intermediates =ddim_sampler.sample(ddim_steps,batch_size,\n",
    "                                                        shape,cond,verbose=False,**kwargs)\n",
    "\n",
    "        else:\n",
    "            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n",
    "                                                 return_intermediates=True,**kwargs)\n",
    "\n",
    "        return samples, intermediates\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n",
    "                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n",
    "                   plot_diffusion_rows=True, **kwargs):\n",
    "\n",
    "        use_ddim = ddim_steps is not None\n",
    "\n",
    "        log = dict()\n",
    "        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n",
    "                                           return_first_stage_outputs=True,\n",
    "                                           force_c_encode=True,\n",
    "                                           return_original_cond=True,\n",
    "                                           bs=N)\n",
    "        N = min(x.shape[0], N)\n",
    "        n_row = min(x.shape[0], n_row)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstruction\"] = xrec\n",
    "        if self.model.conditioning_key is not None:\n",
    "            if hasattr(self.cond_stage_model, \"decode\"):\n",
    "                xc = self.cond_stage_model.decode(c)\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key in [\"caption\"]:\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"caption\"])\n",
    "                log[\"conditioning\"] = xc\n",
    "            elif self.cond_stage_key == 'class_label':\n",
    "                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"])\n",
    "                log['conditioning'] = xc\n",
    "            elif isimage(xc):\n",
    "                log[\"conditioning\"] = xc\n",
    "            if ismap(xc):\n",
    "                log[\"original_conditioning\"] = self.to_rgb(xc)\n",
    "\n",
    "        if plot_diffusion_rows:\n",
    "            # get diffusion row\n",
    "            diffusion_row = list()\n",
    "            z_start = z[:n_row]\n",
    "            for t in range(self.num_timesteps):\n",
    "                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n",
    "                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n",
    "                    t = t.to(self.device).long()\n",
    "                    noise = torch.randn_like(z_start)\n",
    "                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n",
    "                    diffusion_row.append(self.decode_first_stage(z_noisy))\n",
    "\n",
    "            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n",
    "            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n",
    "            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n",
    "            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n",
    "            log[\"diffusion_row\"] = diffusion_grid\n",
    "\n",
    "        if sample:\n",
    "            # get denoise row\n",
    "            with self.ema_scope(\"Plotting\"):\n",
    "                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                         ddim_steps=ddim_steps,eta=ddim_eta)\n",
    "                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n",
    "            x_samples = self.decode_first_stage(samples)\n",
    "            log[\"samples\"] = x_samples\n",
    "            if plot_denoise_rows:\n",
    "                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n",
    "                log[\"denoise_row\"] = denoise_grid\n",
    "\n",
    "            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n",
    "                    self.first_stage_model, IdentityFirstStage):\n",
    "                # also display when quantizing x0 while sampling\n",
    "                with self.ema_scope(\"Plotting Quantized Denoised\"):\n",
    "                    samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n",
    "                                                             ddim_steps=ddim_steps,eta=ddim_eta,\n",
    "                                                             quantize_denoised=True)\n",
    "                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n",
    "                    #                                      quantize_denoised=True)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_x0_quantized\"] = x_samples\n",
    "\n",
    "            if inpaint:\n",
    "                # make a simple center square\n",
    "                b, h, w = z.shape[0], z.shape[2], z.shape[3]\n",
    "                mask = torch.ones(N, h, w).to(self.device)\n",
    "                # zeros will be filled in\n",
    "                mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n",
    "                mask = mask[:, None, ...]\n",
    "                with self.ema_scope(\"Plotting Inpaint\"):\n",
    "\n",
    "                    samples, _ = self.sample_log(cond=c,batch_size=N,ddim=use_ddim, eta=ddim_eta,\n",
    "                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_inpainting\"] = x_samples\n",
    "                log[\"mask\"] = mask\n",
    "\n",
    "                # outpaint\n",
    "                with self.ema_scope(\"Plotting Outpaint\"):\n",
    "                    samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,eta=ddim_eta,\n",
    "                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n",
    "                x_samples = self.decode_first_stage(samples.to(self.device))\n",
    "                log[\"samples_outpainting\"] = x_samples\n",
    "\n",
    "        if plot_progressive_rows:\n",
    "            with self.ema_scope(\"Plotting Progressives\"):\n",
    "                img, progressives = self.progressive_denoising(c,\n",
    "                                                               shape=(self.channels, self.image_size, self.image_size),\n",
    "                                                               batch_size=N,\n",
    "                                                               temperature=self.temperature)\n",
    "            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n",
    "            log[\"progressive_row\"] = prog_row\n",
    "\n",
    "        if return_keys:\n",
    "            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n",
    "                return log\n",
    "            else:\n",
    "                return {key: log[key] for key in return_keys}\n",
    "        return log\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        if self.train_cond_stage_only:\n",
    "            print(f\"{self.__class__.__name__}: Only optimizing conditioner params!\")\n",
    "            cond_parms = [p for n, p in self.named_parameters() \n",
    "              if 'attn2' in n or 'time_embed_condtion' in n or 'norm2' in n]\n",
    "            # cond_parms = [p for n, p in self.named_parameters()\n",
    "                    # if 'time_embed_condtion' in n]\n",
    "            # cond_parms = []\n",
    "\n",
    "            params = list(self.cond_stage_model.parameters()) + cond_parms\n",
    "\n",
    "            for p in params:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        else:\n",
    "            params = list(self.model.parameters())\n",
    "            if self.cond_stage_trainable:\n",
    "                print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
    "                params = params + list(self.cond_stage_model.parameters())\n",
    "            if self.learn_logvar:\n",
    "                print('Diffusion model optimizing logvar')\n",
    "                params.append(self.logvar)\n",
    "\n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            assert 'target' in self.scheduler_config\n",
    "            scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "            print(\"Setting up LambdaLR scheduler...\")\n",
    "            scheduler = [\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                }]\n",
    "            return [opt], scheduler\n",
    "\n",
    "        return opt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def to_rgb(self, x):\n",
    "        x = x.float()\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n",
    "        x = nn.functional.conv2d(x, weight=self.colorize)\n",
    "        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffusionWrapper(pl.LightningModule):\n",
    "    def __init__(self, diff_model_config, conditioning_key):\n",
    "        super().__init__()\n",
    "        self.diffusion_model = instantiate_from_config(diff_model_config)\n",
    "        self.conditioning_key = conditioning_key\n",
    "        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']\n",
    "\n",
    "    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n",
    "        if self.conditioning_key is None:\n",
    "            out = self.diffusion_model(x, t)\n",
    "        elif self.conditioning_key == 'concat':\n",
    "            xc = torch.cat([x] + c_concat, dim=1)\n",
    "            out = self.diffusion_model(xc, t)\n",
    "        elif self.conditioning_key == 'crossattn':\n",
    "            cc = torch.cat(c_crossattn, 1)\n",
    "            out = self.diffusion_model(x, t, context=cc)\n",
    "        elif self.conditioning_key == 'hybrid':\n",
    "            xc = torch.cat([x] + [c_concat], dim=1)\n",
    "            cc = torch.cat([c_crossattn], dim=1)\n",
    "            out = self.diffusion_model(xc, t, context=cc)\n",
    "        elif self.conditioning_key == 'adm':\n",
    "            cc = c_crossattn[0]\n",
    "            out = self.diffusion_model(x, t, y=cc)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# class EEGClassifier(pl.LightningModule):\n",
    "#     \"\"\"main class\"\"\"\n",
    "#     def __init__(self,\n",
    "#                 first_stage_config,\n",
    "#                 cond_stage_config,\n",
    "#                 num_timesteps_cond=None,\n",
    "#                 cond_stage_key=\"image\",\n",
    "#                 cond_stage_trainable=True,\n",
    "#                 concat_mode=True,\n",
    "#                 cond_stage_forward=None,\n",
    "#                 conditioning_key=None,\n",
    "#                 scale_factor=1.0,\n",
    "#                 scale_by_std=False,\n",
    "#                 *args, **kwargs):\n",
    "#         super().__init__()\n",
    "#         # self.use_scheduler = scheduler_config is not None\n",
    "#         # if self.use_scheduler:\n",
    "#         #     self.scheduler_config = scheduler_config\n",
    "#         self.cond_stage_trainable = True\n",
    "#         self.main_config = None\n",
    "#         self.best_val = 0.0\n",
    "#         self.cond_stage_model = None\n",
    "#         self.validation_count = 0\n",
    "\n",
    "#     def forward(self, x, c, label, image_raw, *args, **kwargs):\n",
    "#         # print(self.num_timesteps)\n",
    "#         # t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n",
    "#         # print('t.shape')\n",
    "#         # print(t.shape)\n",
    "#         # if self.model.conditioning_key is not None:\n",
    "#         #     assert c is not None\n",
    "#         #     imgs = c\n",
    "#         #     if self.cond_stage_trainable:\n",
    "#                 # c = self.get_learned_conditioning(c)\n",
    "#         c, re_latent = self.get_learned_conditioning(c)\n",
    "#                 # print('c.shape')\n",
    "#                 # print(c.shape)\n",
    "\n",
    "#         prefix = 'train' if self.training else 'val'\n",
    "#         # loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "#         pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "\n",
    "#         loss = self.cls_loss(label, pre_cls)\n",
    "\n",
    "#         loss_dict = {}\n",
    "#         loss_dict.update({f'{prefix}/loss_cls': loss})\n",
    "#         # rencon = self.cond_stage_model.recon(re_latent)\n",
    "#         if self.clip_tune:\n",
    "#             image_embeds = self.image_embedder(image_raw)\n",
    "#             loss_clip = self.cond_stage_model.get_clip_loss(re_latent, image_embeds)\n",
    "#         # loss_recon = self.recon_loss(imgs, rencon)\n",
    "\n",
    "#             loss += loss_clip\n",
    "#         # loss += loss_cls # loss_recon +  #(self.original_elbo_weight * loss_vlb)\n",
    "#         # loss_dict.update({f'{prefix}/loss_recon': loss_recon})\n",
    "#         # loss_dict.update({f'{prefix}/loss_cls': loss_cls})\n",
    "#             loss_dict.update({f'{prefix}/loss_clip': loss_clip})\n",
    "#                 # if self.return_cond:\n",
    "#                     # return self.p_losses(x, c, t, *args, **kwargs), c\n",
    "#         # return self.p_losses(x, c, t, *args, **kwargs)\n",
    "#         # if self.return_cond:\n",
    "#         #     return loss, loss_dict, c\n",
    "#         return loss, loss_dict\n",
    "\n",
    "#     def shared_step(self, batch):\n",
    "#         x,c, label, image_raw  = self.get_input(batch)\n",
    "#         loss, loss_dict = self(x,c, label, image_raw)\n",
    "#         return loss, loss_dict\n",
    "\n",
    "#     def cls_loss(self, label, pred):\n",
    "#         return torch.nn.CrossEntropyLoss()(pred, label)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         self.train()\n",
    "#         self.cond_stage_model.train()  ###\n",
    "\n",
    "#         loss, loss_dict = self.shared_step(batch)\n",
    "\n",
    "#         self.log_dict(loss_dict, prog_bar=True,\n",
    "#                     logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "#         # if self.use_scheduler:\n",
    "#         #     lr = self.optimizers().param_groups[0]['lr']\n",
    "#         #     self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         lr = self.learning_rate\n",
    "#         # if self.train_cond_stage_only:\n",
    "#         #     print(f\"{self.__class__.__name__}: Only optimizing conditioner params!\")\n",
    "#         #     cond_parms = [p for n, p in self.named_parameters()\n",
    "#         #             if 'attn2' in n or 'time_embed_condtion' in n or 'norm2' in n]\n",
    "#         #     # cond_parms = [p for n, p in self.named_parameters()\n",
    "#         #             # if 'time_embed_condtion' in n]\n",
    "#         #     # cond_parms = []\n",
    "\n",
    "#         params = list(self.cond_stage_model.parameters()) # + cond_parms\n",
    "\n",
    "#         for p in params:\n",
    "#             p.requires_grad = True\n",
    "\n",
    "#         # else:\n",
    "#         #     params = list(self.model.parameters())\n",
    "#         #     if self.cond_stage_trainable:\n",
    "#         #         print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n",
    "#         #         params = params + list(self.cond_stage_model.parameters())\n",
    "#         #     if self.learn_logvar:\n",
    "#         #         print('Diffusion model optimizing logvar')\n",
    "#         #         params.append(self.logvar)\n",
    "\n",
    "#         opt = torch.optim.AdamW(params, lr=lr)\n",
    "\n",
    "#         # if self.use_scheduler:\n",
    "#         #     assert 'target' in self.scheduler_config\n",
    "#         #     scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "#         #     print(\"Setting up LambdaLR scheduler...\")\n",
    "#         #     scheduler = [\n",
    "#         #         {\n",
    "#         #             'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n",
    "#         #             'interval': 'step',\n",
    "#         #             'frequency': 1\n",
    "#         #         }]\n",
    "#         #     return [opt], scheduler\n",
    "\n",
    "#         return opt\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def get_input(self, batch, k='image', return_first_stage_outputs=False, force_c_encode=False,\n",
    "#                   cond_key=None, return_original_cond=False, bs=None):\n",
    "#         # x = super().get_input(batch, k)\n",
    "#         x = batch['image']\n",
    "#         if bs is not None:\n",
    "#             x = x[:bs]\n",
    "#         x = x.to(self.device)\n",
    "\n",
    "#         # print('z.shape')\n",
    "#         # print(z.shape)\n",
    "#         # print(cond_key)\n",
    "#         # print(self.cond_stage_key)\n",
    "#         # print(cond_key)\n",
    "#         xc = batch['eeg']\n",
    "#         c = xc\n",
    "#         # if self.model.conditioning_key is not None:\n",
    "#         #     if cond_key is None:\n",
    "#         #         cond_key = self.cond_stage_key\n",
    "#         #     if cond_key != self.first_stage_key:\n",
    "#         #         if cond_key in ['caption', 'coordinates_bbox','fmri', 'eeg']:\n",
    "#         #             xc = batch[cond_key]\n",
    "#         #         elif cond_key == 'class_label':\n",
    "#         #             xc = batch\n",
    "#         #         else:\n",
    "#         #             xc = super().get_input(batch, cond_key).to(self.device)\n",
    "#         #     else:\n",
    "#         #         xc = x\n",
    "#         #     # print('get input')\n",
    "#         #     # print(not self.cond_stage_trainable)\n",
    "#         #     # print(force_c_encode)\n",
    "#         #     if not self.cond_stage_trainable or force_c_encode :\n",
    "#         #         # print('get learned condition')\n",
    "#         #         if isinstance(xc, dict) or isinstance(xc, list):\n",
    "#         #             # import pudb; pudb.set_trace()\n",
    "#         #             c, re_latent = self.get_learned_conditioning(xc)\n",
    "#         #             # c = self.get_learned_conditioning(xc)\n",
    "#         #         else:\n",
    "#         #             c, re_latent = self.get_learned_conditioning(xc.to(self.device))\n",
    "#         #             # c = self.get_learned_conditioning(xc.to(self.device))\n",
    "#         #     else:\n",
    "#         #         c = xc\n",
    "#         #     if bs is not None:\n",
    "#         #         c = c[:bs]\n",
    "\n",
    "#         #     if self.use_positional_encodings:\n",
    "#         #         pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "#         #         ckey = __conditioning_keys__[self.model.conditioning_key]\n",
    "#         #         c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n",
    "\n",
    "#         # else:\n",
    "#         #     c = None\n",
    "#         #     xc = None\n",
    "#         #     if self.use_positional_encodings:\n",
    "#         #         pos_x, pos_y = self.compute_latent_shifts(batch)\n",
    "#         #         c = {'pos_x': pos_x, 'pos_y': pos_y}\n",
    "#         out = [x, c , batch['label'], batch['image_raw']]\n",
    "#         # if return_first_stage_outputs:\n",
    "#         #     xrec = self.decode_first_stage(z)\n",
    "#         #     out.extend([x, xrec])\n",
    "#         # if return_original_cond:\n",
    "#         #     out.append(xc)\n",
    "#         return out\n",
    "\n",
    "\n",
    "#     @torch.no_grad()\n",
    "\n",
    "#     def accuracy(self, output, target, topk=(1, )):\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             maxk = max(topk)\n",
    "#             batch_size = target.size(0)\n",
    "\n",
    "#             _, pred = output.topk(maxk, 1, True, True)\n",
    "#             pred = pred.t()\n",
    "#             correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "#             res = []\n",
    "#             for k in topk:\n",
    "#                 correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "#                 res.append(correct_k.mul_(100.0 / batch_size))\n",
    "#             return res\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         print('val step')\n",
    "#         print('batch_idx:', batch_idx)\n",
    "#         # if batch_idx != 0:\n",
    "#         #     return\n",
    "\n",
    "#         if self.validation_count % 1 == 0 and self.trainer.current_epoch != 0:\n",
    "#             self.full_validation(batch)\n",
    "#         # else:\n",
    "#         #     # pass\n",
    "#         #     grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=3, limit=5)\n",
    "#         #     metric, metric_list = self.get_eval_metric(all_samples, avg=self.eval_avg)\n",
    "#         #     grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "#         #     # self.logger.log_image(key=f'samples_test', images=[grid_imgs])\n",
    "#         #     metric_dict = {f'val/{k}':v for k, v in zip(metric_list, metric)}\n",
    "#         #     # self.logger.log_metrics(metric_dict)\n",
    "#         #     if metric[-1] > self.run_full_validation_threshold:\n",
    "#         #         self.full_validation(batch, state=state)\n",
    "#         self.validation_count += 1\n",
    "\n",
    "\n",
    "#     def full_validation(self, batch, state=None):\n",
    "#         print('###### run full validation! ######\\n')\n",
    "#         c = batch['eeg']\n",
    "\n",
    "#         c, re_latent = self.get_learned_conditioning(c)\n",
    "\n",
    "#         # loss, loss_dict = self.p_losses(x, c, t, *args, **kwargs)\n",
    "#         pre_cls = self.cond_stage_model.get_cls(re_latent)\n",
    "#         # grid, all_samples, state = self.generate(batch, ddim_steps=self.ddim_steps, num_samples=5, limit=None, state=state)\n",
    "#         # metric, metric_list = self.get_eval_metric(all_samples)\n",
    "#         # self.save_images(all_samples, suffix='%.4f'%metric[-1])\n",
    "#         # metric_dict = {f'val/{k}_full':v for k, v in zip(metric_list, metric)}\n",
    "#         # self.logger.log_metrics(metric_dict)\n",
    "#         acc1, acc5 = self.accuracy(pre_cls, batch['label'], topk=(1, 5))\n",
    "#         print(acc1, acc5)\n",
    "#         # acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n",
    "#         # grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "\n",
    "#         # self.logger.log_image(key=f'samples_test_full', images=[grid_imgs])\n",
    "#         if acc1[0] > self.best_val:\n",
    "#             self.best_val = acc1[0]\n",
    "#             torch.save(\n",
    "#                 {\n",
    "#                     'model_state_dict': self.state_dict(),\n",
    "#                     'config': self.main_config,\n",
    "#                     'state': state\n",
    "\n",
    "#                 },\n",
    "#                 os.path.join(self.output_path, 'checkpoint_best.pth')\n",
    "#             )\n",
    "#     def get_learned_conditioning(self, c):\n",
    "#         # self.cond_stage_model.eval()\n",
    "#         if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n",
    "#             c, re_latent = self.cond_stage_model.encode(c)\n",
    "#             # c = self.cond_stage_model.encode(c)\n",
    "#         else:\n",
    "#             c, re_latent = self.cond_stage_model(c)\n",
    "#             # c = self.cond_stage_model(c)\n",
    "#         # return c\n",
    "#         return c, re_latent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288244907,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "5QvgDIDTFu6B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Config\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Config_MAE_fMRI: # back compatibility\n",
    "    pass\n",
    "\n",
    "class Config_MBM_EEG(Config_MAE_fMRI):\n",
    "    # configs for fmri_pretrain.py\n",
    "    def __init__(self):\n",
    "    # --------------------------------------------\n",
    "    # MAE for fMRI\n",
    "        # Training Parameters\n",
    "        self.lr = 2.5e-4\n",
    "        self.min_lr = 0.\n",
    "        self.weight_decay = 0.05\n",
    "        self.num_epoch = 500\n",
    "        self.warmup_epochs = 40\n",
    "        self.batch_size = 100\n",
    "        self.clip_grad = 0.8\n",
    "\n",
    "        # Model Parameters\n",
    "        self.mask_ratio = 0.1\n",
    "        self.patch_size = 4 #  1\n",
    "        self.embed_dim = 1024 #256 # has to be a multiple of num_heads\n",
    "        self.decoder_embed_dim = 512 #128\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.decoder_num_heads = 16\n",
    "        self.mlp_ratio = 1.0\n",
    "\n",
    "        # Project setting\n",
    "        self.root_path = '/dreamdiffusion/'\n",
    "        self.output_path = '/dreamdiffusion/exps/'\n",
    "        self.seed = 2022\n",
    "        self.roi = 'VC'\n",
    "        self.aug_times = 1\n",
    "        self.num_sub_limit = None\n",
    "        self.include_hcp = True\n",
    "        self.include_kam = True\n",
    "        self.accum_iter = 1\n",
    "\n",
    "        self.use_nature_img_loss = False\n",
    "        self.img_recon_weight = 0.5\n",
    "        self.focus_range = None # [0, 1500] # None to disable it\n",
    "        self.focus_rate = 0.6\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = 0\n",
    "\n",
    "class Config_Generative_Model:\n",
    "    def __init__(self):\n",
    "        # project parameters\n",
    "        self.seed = 2022\n",
    "        self.root_path = 'dreamdiffusion/'\n",
    "        self.output_path = 'dreamdiffusion/output/'\n",
    "\n",
    "        #self.eeg_signals_path = os.path.join(self.root_path, 'datasets/eeg_5_95_std.pth')\n",
    "        #self.splits_path = os.path.join(self.root_path, 'datasets/block_splits_by_image_single.pth')\n",
    "        # self.splits_path = os.path.join(self.root_path, 'datasets/block_splits_by_image_all.pth')\n",
    "        self.roi = 'VC'\n",
    "        self.patch_size = 4 # 16\n",
    "        self.embed_dim = 1024\n",
    "        self.depth = 24\n",
    "        self.num_heads = 16\n",
    "        self.mlp_ratio = 1.0\n",
    "\n",
    "        self.pretrain_gm_path = os.path.join(self.root_path, 'pretrains')\n",
    "\n",
    "        self.dataset = 'EEG'\n",
    "        self.pretrain_mbm_path = None\n",
    "\n",
    "        self.img_size = 512\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        # finetune parameters\n",
    "        self.batch_size = 4\n",
    "        self.lr = 5.3e-5\n",
    "        self.num_epoch = 10\n",
    "\n",
    "        self.precision = 32\n",
    "        self.accumulate_grad = 1\n",
    "        self.crop_ratio = 0.2\n",
    "        self.global_pool = False\n",
    "        self.use_time_cond = True\n",
    "        self.clip_tune = True #False\n",
    "        self.cls_tune = False\n",
    "        self.subject = 4\n",
    "        self.eval_avg = True\n",
    "\n",
    "        # diffusion sampling parameters\n",
    "        self.num_samples = 5\n",
    "        self.ddim_steps = 250\n",
    "        self.HW = None\n",
    "        # resume check util\n",
    "        self.model_meta = None\n",
    "        self.checkpoint_path = None\n",
    "        self.temperature = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 641,
     "status": "ok",
     "timestamp": 1701288246545,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "rjpCZPsCFu6B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Eval Metrics\n",
    "from einops import rearrange\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity\n",
    "from torchmetrics.functional import accuracy\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from torchvision.models import ViT_H_14_Weights, vit_h_14\n",
    "import torch\n",
    "\n",
    "class fid_wrapper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.fid = FrechetInceptionDistance(feature=64)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, pred_imgs, gt_imgs):\n",
    "        self.fid.reset()\n",
    "        self.fid.update(torch.tensor(rearrange(gt_imgs, 'n w h c -> n c w h')), real=True)\n",
    "        self.fid.update(torch.tensor(rearrange(pred_imgs, 'n w h c -> n c w h')), real=False)\n",
    "        return self.fid.compute().item()\n",
    "\n",
    "def pair_wise_score(pred_imgs, gt_imgs, metric, is_sucess):\n",
    "    # pred_imgs: n, w, h, 3\n",
    "    # gt_imgs: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    assert len(pred_imgs) == len(gt_imgs)\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "    assert isinstance(metric, fid_wrapper) == False, 'FID not supported'\n",
    "    corrects = []\n",
    "    for idx, pred in enumerate(pred_imgs):\n",
    "        gt = gt_imgs[idx]\n",
    "        gt_score = metric(pred, gt)\n",
    "        rest = [img for i, img in enumerate(gt_imgs) if i != idx]\n",
    "        count = 0\n",
    "        for comp in rest:\n",
    "            comp_score = metric(pred, comp)\n",
    "            if is_sucess(gt_score, comp_score):\n",
    "                count += 1\n",
    "        corrects.append(count / len(rest))\n",
    "    return corrects\n",
    "\n",
    "def n_way_scores(pred_imgs, gt_imgs, metric, is_sucess, n=2, n_trials=100):\n",
    "    # pred_imgs: n, w, h, 3\n",
    "    # gt_imgs: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    assert len(pred_imgs) == len(gt_imgs)\n",
    "    assert n <= len(pred_imgs) and n >= 2\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "    assert isinstance(metric, fid_wrapper) == False, 'FID not supported'\n",
    "    corrects = []\n",
    "    for idx, pred in enumerate(pred_imgs):\n",
    "        gt = gt_imgs[idx]\n",
    "        gt_score = metric(pred, gt)\n",
    "        rest = np.stack([img for i, img in enumerate(gt_imgs) if i != idx])\n",
    "        correct_count = 0\n",
    "        for _ in range(n_trials):\n",
    "            n_imgs_idx = np.random.choice(len(rest), n-1, replace=False)\n",
    "            n_imgs = rest[n_imgs_idx]\n",
    "            count = 0\n",
    "            for comp in n_imgs:\n",
    "                comp_score = metric(pred, comp)\n",
    "                if is_sucess(gt_score, comp_score):\n",
    "                    count += 1\n",
    "            if count == len(n_imgs):\n",
    "                correct_count += 1\n",
    "        corrects.append(correct_count / n_trials)\n",
    "    return corrects\n",
    "\n",
    "def metrics_only(pred_imgs, gt_imgs, metric, *args, **kwargs):\n",
    "    assert np.min(pred_imgs) >= 0 and np.min(gt_imgs) >= 0\n",
    "\n",
    "    return metric(pred_imgs, gt_imgs)\n",
    "\n",
    "@torch.no_grad()\n",
    "def n_way_top_k_acc(pred, class_id, n_way, num_trials=40, top_k=1):\n",
    "    pick_range =[i for i in np.arange(len(pred)) if i != class_id]\n",
    "    acc_list = []\n",
    "    for t in range(num_trials):\n",
    "        idxs_picked = np.random.choice(pick_range, n_way-1, replace=False)\n",
    "        pred_picked = torch.cat([pred[class_id].unsqueeze(0), pred[idxs_picked]])\n",
    "        acc = accuracy(pred_picked.unsqueeze(0), torch.tensor([0], device=pred.device),\n",
    "                    top_k=top_k, task=\"multiclass\", num_classes=1)\n",
    "        acc_list.append(acc.item())\n",
    "    return np.mean(acc_list), np.std(acc_list)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_n_way_top_k_acc(pred_imgs, ground_truth, n_way, num_trials, top_k, device, return_std=False):\n",
    "    weights = ViT_H_14_Weights.DEFAULT\n",
    "    model = vit_h_14(weights=weights)\n",
    "    preprocess = weights.transforms()\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    acc_list = []\n",
    "    std_list = []\n",
    "    for pred, gt in zip(pred_imgs, ground_truth):\n",
    "        pred = preprocess(Image.fromarray(pred.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        gt = preprocess(Image.fromarray(gt.astype(np.uint8))).unsqueeze(0).to(device)\n",
    "        gt_class_id = model(gt).squeeze(0).softmax(0).argmax().item()\n",
    "        pred_out = model(pred).squeeze(0).softmax(0).detach()\n",
    "\n",
    "        acc, std = n_way_top_k_acc(pred_out, gt_class_id, n_way, num_trials, top_k)\n",
    "        acc_list.append(acc)\n",
    "        std_list.append(std)\n",
    "\n",
    "    if return_std:\n",
    "        return acc_list, std_list\n",
    "    return acc_list\n",
    "\n",
    "def mse_metric(img1, img2):\n",
    "    return (np.square(img1 - img2)).mean()\n",
    "\n",
    "def smaller_the_better(gt, comp):\n",
    "    return gt < comp\n",
    "\n",
    "def pcc_metric(img1, img2):\n",
    "    return np.corrcoef(img1.reshape(-1), img2.reshape(-1))[0, 1]\n",
    "\n",
    "def larger_the_better(gt, comp):\n",
    "    return gt > comp\n",
    "\n",
    "def ssim_metric(img1, img2):\n",
    "    return ssim(img1, img2, data_range=255, channel_axis=-1)\n",
    "\n",
    "class psm_wrapper:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.lpips = LearnedPerceptualImagePatchSimilarity(net_type='alex').to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img1, img2):\n",
    "        if img1.shape[-1] == 3:\n",
    "            img1 = rearrange(img1, 'w h c -> c w h')\n",
    "            img2 = rearrange(img2, 'w h c -> c w h')\n",
    "        img1 = img1 / 127.5 - 1.0\n",
    "        img2 = img2 / 127.5 - 1.0\n",
    "        img1 = np.expand_dims(img1, axis=0)\n",
    "        img2 = np.expand_dims(img2, axis=0)\n",
    "        return self.lpips(torch.FloatTensor(img1).to(self.device), torch.FloatTensor(img2).to(self.device)).item()\n",
    "\n",
    "\n",
    "def get_similarity_metric(img1, img2, method='pair-wise', metric_name='mse', **kwargs):\n",
    "    # img1: n, w, h, 3\n",
    "    # img2: n, w, h, 3\n",
    "    # all in pixel values: 0 ~ 255\n",
    "    # return: list of scores 0 ~ 1.\n",
    "    if img1.shape[-1] != 3:\n",
    "        img1 = rearrange(img1, 'n c w h -> n w h c')\n",
    "    if img2.shape[-1] != 3:\n",
    "        img2 = rearrange(img2, 'n c w h -> n w h c')\n",
    "\n",
    "    if method == 'pair-wise':\n",
    "        eval_procedure_func = pair_wise_score\n",
    "    elif method == 'n-way':\n",
    "        eval_procedure_func = n_way_scores\n",
    "    elif method == 'metrics-only':\n",
    "        eval_procedure_func = metrics_only\n",
    "    elif method == 'class':\n",
    "        return get_n_way_top_k_acc(img1, img2, **kwargs)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if metric_name == 'mse':\n",
    "        metric_func = mse_metric\n",
    "        decision_func = smaller_the_better\n",
    "    elif metric_name == 'pcc':\n",
    "        metric_func = pcc_metric\n",
    "        decision_func = larger_the_better\n",
    "    elif metric_name == 'ssim':\n",
    "        metric_func = ssim_metric\n",
    "        decision_func = larger_the_better\n",
    "    elif metric_name == 'psm':\n",
    "        metric_func = psm_wrapper()\n",
    "        decision_func = smaller_the_better\n",
    "    elif metric_name == 'fid':\n",
    "        metric_func = fid_wrapper()\n",
    "        decision_func = smaller_the_better\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return eval_procedure_func(img1, img2, metric_func, decision_func, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC LDM Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288246984,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "uw2We30C3UXQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title DC LDM Utils\n",
    "#@title other utils\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from collections import abc\n",
    "from einops import rearrange\n",
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "from inspect import isfunction\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "def log_txt_as_img(wh, xc, size=10):\n",
    "    # wh a tuple of (width, height)\n",
    "    # xc a list of captions to plot\n",
    "    b = len(xc)\n",
    "    txts = list()\n",
    "    for bi in range(b):\n",
    "        txt = Image.new(\"RGB\", wh, color=\"white\")\n",
    "        draw = ImageDraw.Draw(txt)\n",
    "        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n",
    "        nc = int(40 * (wh[0] / 256))\n",
    "        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n",
    "\n",
    "        try:\n",
    "            draw.text((0, 0), lines, fill=\"black\", font=font)\n",
    "        except UnicodeEncodeError:\n",
    "            print(\"Cant encode string for logging. Skipping.\")\n",
    "\n",
    "        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n",
    "        txts.append(txt)\n",
    "    txts = np.stack(txts)\n",
    "    txts = torch.tensor(txts)\n",
    "    return txts\n",
    "\n",
    "\n",
    "def ismap(x):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return False\n",
    "    return (len(x.shape) == 4) and (x.shape[1] > 3)\n",
    "\n",
    "\n",
    "def isimage(x):\n",
    "    if not isinstance(x,torch.Tensor):\n",
    "        return False\n",
    "    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n",
    "\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "def count_params(model, verbose=False):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    if verbose:\n",
    "        print(f\"{model.__class__.__name__} has {total_params*1.e-6:.2f} M params.\")\n",
    "    return total_params\n",
    "\n",
    "\n",
    "def instantiate_from_config(config):\n",
    "    if not \"target\" in config:\n",
    "        if config in ['__is_first_stage__', \"__is_unconditional__\"]:\n",
    "            return None\n",
    "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
    "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
    "\n",
    "def get_obj_from_str(string):\n",
    "    try:\n",
    "        # Directly get the global object from the global scope\n",
    "        obj = globals()[string]\n",
    "    except KeyError:\n",
    "        raise ValueError(f\"Object '{string}' not found in the global scope.\")\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _do_parallel_data_prefetch(func, Q, data, idx, idx_to_fn=False):\n",
    "    # create dummy dataset instance\n",
    "\n",
    "    # run prefetching\n",
    "    if idx_to_fn:\n",
    "        res = func(data, worker_id=idx)\n",
    "    else:\n",
    "        res = func(data)\n",
    "    Q.put([idx, res])\n",
    "    Q.put(\"Done\")\n",
    "\n",
    "\n",
    "def parallel_data_prefetch(\n",
    "        func: callable, data, n_proc, target_data_type=\"ndarray\", cpu_intensive=True, use_worker_id=False\n",
    "):\n",
    "    # if target_data_type not in [\"ndarray\", \"list\"]:\n",
    "    #     raise ValueError(\n",
    "    #         \"Data, which is passed to parallel_data_prefetch has to be either of type list or ndarray.\"\n",
    "    #     )\n",
    "    if isinstance(data, np.ndarray) and target_data_type == \"list\":\n",
    "        raise ValueError(\"list expected but function got ndarray.\")\n",
    "    elif isinstance(data, abc.Iterable):\n",
    "        if isinstance(data, dict):\n",
    "            print(\n",
    "                f'WARNING:\"data\" argument passed to parallel_data_prefetch is a dict: Using only its values and disregarding keys.'\n",
    "            )\n",
    "            data = list(data.values())\n",
    "        if target_data_type == \"ndarray\":\n",
    "            data = np.asarray(data)\n",
    "        else:\n",
    "            data = list(data)\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            f\"The data, that shall be processed parallel has to be either an np.ndarray or an Iterable, but is actually {type(data)}.\"\n",
    "        )\n",
    "\n",
    "    if cpu_intensive:\n",
    "        Q = mp.Queue(1000)\n",
    "        proc = mp.Process\n",
    "    else:\n",
    "        Q = Queue(1000)\n",
    "        proc = Thread\n",
    "    # spawn processes\n",
    "    if target_data_type == \"ndarray\":\n",
    "        arguments = [\n",
    "            [func, Q, part, i, use_worker_id]\n",
    "            for i, part in enumerate(np.array_split(data, n_proc))\n",
    "        ]\n",
    "    else:\n",
    "        step = (\n",
    "            int(len(data) / n_proc + 1)\n",
    "            if len(data) % n_proc != 0\n",
    "            else int(len(data) / n_proc)\n",
    "        )\n",
    "        arguments = [\n",
    "            [func, Q, part, i, use_worker_id]\n",
    "            for i, part in enumerate(\n",
    "                [data[i: i + step] for i in range(0, len(data), step)]\n",
    "            )\n",
    "        ]\n",
    "    processes = []\n",
    "    for i in range(n_proc):\n",
    "        p = proc(target=_do_parallel_data_prefetch, args=arguments[i])\n",
    "        processes += [p]\n",
    "\n",
    "    # start processes\n",
    "    print(f\"Start prefetching...\")\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    gather_res = [[] for _ in range(n_proc)]\n",
    "    try:\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "\n",
    "        k = 0\n",
    "        while k < n_proc:\n",
    "            # get result\n",
    "            res = Q.get()\n",
    "            if res == \"Done\":\n",
    "                k += 1\n",
    "            else:\n",
    "                gather_res[res[0]] = res[1]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Exception: \", e)\n",
    "        for p in processes:\n",
    "            p.terminate()\n",
    "\n",
    "        raise e\n",
    "    finally:\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        print(f\"Prefetching complete. [{time.time() - start} sec.]\")\n",
    "\n",
    "    if target_data_type == 'ndarray':\n",
    "        if not isinstance(gather_res[0], np.ndarray):\n",
    "            return np.concatenate([np.asarray(r) for r in gather_res], axis=0)\n",
    "\n",
    "        # order outputs\n",
    "        return np.concatenate(gather_res, axis=0)\n",
    "    elif target_data_type == 'list':\n",
    "        out = []\n",
    "        for r in gather_res:\n",
    "            out.extend(r)\n",
    "        return out\n",
    "    else:\n",
    "        return gather_res\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractDistribution:\n",
    "    def sample(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def mode(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class DiracDistribution(AbstractDistribution):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def sample(self):\n",
    "        return self.value\n",
    "\n",
    "    def mode(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, parameters, deterministic=False):\n",
    "        self.parameters = parameters\n",
    "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2, 3])\n",
    "            else:\n",
    "                return 0.5 * torch.sum(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
    "    Compute the KL divergence between two gaussians.\n",
    "    Shapes are automatically broadcasted, so batches can be compared to\n",
    "    scalars, among other use cases.\n",
    "    \"\"\"\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
    "\n",
    "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
    "    # Tensors, but it does not work for torch.exp().\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "\n",
    "    return 0.5 * (\n",
    "        -1.0\n",
    "        + logvar2\n",
    "        - logvar1\n",
    "        + torch.exp(logvar1 - logvar2)\n",
    "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288247867,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "S3AN6zgc3vwm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Distributions\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AbstractDistribution:\n",
    "    def sample(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def mode(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class DiracDistribution(AbstractDistribution):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def sample(self):\n",
    "        return self.value\n",
    "\n",
    "    def mode(self):\n",
    "        return self.value\n",
    "\n",
    "\n",
    "class DiagonalGaussianDistribution(object):\n",
    "    def __init__(self, parameters, deterministic=False):\n",
    "        self.parameters = parameters\n",
    "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
    "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
    "        self.deterministic = deterministic\n",
    "        self.std = torch.exp(0.5 * self.logvar)\n",
    "        self.var = torch.exp(self.logvar)\n",
    "        if self.deterministic:\n",
    "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
    "        return x\n",
    "\n",
    "    def kl(self, other=None):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        else:\n",
    "            if other is None:\n",
    "                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n",
    "                                       + self.var - 1.0 - self.logvar,\n",
    "                                       dim=[1, 2, 3])\n",
    "            else:\n",
    "                return 0.5 * torch.sum(\n",
    "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
    "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
    "                    dim=[1, 2, 3])\n",
    "\n",
    "    def nll(self, sample, dims=[1,2,3]):\n",
    "        if self.deterministic:\n",
    "            return torch.Tensor([0.])\n",
    "        logtwopi = np.log(2.0 * np.pi)\n",
    "        return 0.5 * torch.sum(\n",
    "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
    "            dim=dims)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
    "    Compute the KL divergence between two gaussians.\n",
    "    Shapes are automatically broadcasted, so batches can be compared to\n",
    "    scalars, among other use cases.\n",
    "    \"\"\"\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
    "\n",
    "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
    "    # Tensors, but it does not work for torch.exp().\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "\n",
    "    return 0.5 * (\n",
    "        -1.0\n",
    "        + logvar2\n",
    "        - logvar1\n",
    "        + torch.exp(logvar1 - logvar2)\n",
    "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1701288248475,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "9HYh2TW0305x",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title AutoEncoders\n",
    "#@title Auto Encoders\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
    "import torch.nn as nn\n",
    "from packaging import version\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import einsum\n",
    "from einops import rearrange\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved version over VectorQuantizer in taming, can be used as a drop-in replacement. Mostly\n",
    "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
    "    \"\"\"\n",
    "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
    "    # backwards compatibility we use the buggy version by default, but you can\n",
    "    # specify legacy=False to fix it.\n",
    "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\",\n",
    "                 sane_index_shape=False, legacy=True):\n",
    "        super().__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "        self.legacy = legacy\n",
    "\n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "\n",
    "        self.remap = remap\n",
    "        if self.remap is not None:\n",
    "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
    "            self.re_embed = self.used.shape[0]\n",
    "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
    "            if self.unknown_index == \"extra\":\n",
    "                self.unknown_index = self.re_embed\n",
    "                self.re_embed = self.re_embed+1\n",
    "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
    "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
    "        else:\n",
    "            self.re_embed = n_e\n",
    "\n",
    "        self.sane_index_shape = sane_index_shape\n",
    "\n",
    "    def remap_to_used(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape)>1\n",
    "        inds = inds.reshape(ishape[0],-1)\n",
    "        used = self.used.to(inds)\n",
    "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
    "        new = match.argmax(-1)\n",
    "        unknown = match.sum(2)<1\n",
    "        if self.unknown_index == \"random\":\n",
    "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
    "        else:\n",
    "            new[unknown] = self.unknown_index\n",
    "        return new.reshape(ishape)\n",
    "\n",
    "    def unmap_to_all(self, inds):\n",
    "        ishape = inds.shape\n",
    "        assert len(ishape)>1\n",
    "        inds = inds.reshape(ishape[0],-1)\n",
    "        used = self.used.to(inds)\n",
    "        if self.re_embed > self.used.shape[0]: # extra token\n",
    "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
    "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
    "        return back.reshape(ishape)\n",
    "\n",
    "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
    "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
    "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
    "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
    "        # reshape z -> (batch, height, width, channel) and flatten\n",
    "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)\n",
    "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
    "\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
    "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
    "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
    "\n",
    "        min_encoding_indices = torch.argmin(d, dim=1)\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
    "        perplexity = None\n",
    "        min_encodings = None\n",
    "\n",
    "        # compute loss for embedding\n",
    "        if not self.legacy:\n",
    "            loss = self.beta * torch.mean((z_q.detach()-z)**2) + \\\n",
    "                   torch.mean((z_q - z.detach()) ** 2)\n",
    "        else:\n",
    "            loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
    "                   torch.mean((z_q - z.detach()) ** 2)\n",
    "\n",
    "        # preserve gradients\n",
    "        z_q = z + (z_q - z).detach()\n",
    "\n",
    "        # reshape back to match original input shape\n",
    "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
    "\n",
    "        if self.remap is not None:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
    "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
    "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
    "\n",
    "        if self.sane_index_shape:\n",
    "            min_encoding_indices = min_encoding_indices.reshape(\n",
    "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
    "\n",
    "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
    "\n",
    "    def get_codebook_entry(self, indices, shape):\n",
    "        # shape specifying (batch, height, width, channel)\n",
    "        if self.remap is not None:\n",
    "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
    "            indices = self.unmap_to_all(indices)\n",
    "            indices = indices.reshape(-1) # flatten again\n",
    "\n",
    "        # get quantized latent vectors\n",
    "        z_q = self.embedding(indices)\n",
    "\n",
    "        if shape is not None:\n",
    "            z_q = z_q.view(shape)\n",
    "            # reshape back to match original input shape\n",
    "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        return z_q\n",
    "\n",
    "class VQModel(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 n_embed,\n",
    "                 embed_dim,\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 image_key=\"image\",\n",
    "                 colorize_nlabels=None,\n",
    "                 monitor=None,\n",
    "                 batch_resize_range=None,\n",
    "                 scheduler_config=None,\n",
    "                 lr_g_factor=1.0,\n",
    "                 remap=None,\n",
    "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
    "                 use_ema=False\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_embed = n_embed\n",
    "        self.image_key = image_key\n",
    "        self.encoder = DiffusionEncoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.loss = instantiate_from_config(lossconfig)\n",
    "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
    "                                        remap=remap,\n",
    "                                        sane_index_shape=sane_index_shape)\n",
    "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        if colorize_nlabels is not None:\n",
    "            assert type(colorize_nlabels)==int\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        self.batch_resize_range = batch_resize_range\n",
    "        if self.batch_resize_range is not None:\n",
    "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
    "\n",
    "        self.use_ema = use_ema\n",
    "        if self.use_ema:\n",
    "            self.model_ema = LitEma(self)\n",
    "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
    "\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
    "        self.scheduler_config = scheduler_config\n",
    "        self.lr_g_factor = lr_g_factor\n",
    "\n",
    "    @contextmanager\n",
    "    def ema_scope(self, context=None):\n",
    "        if self.use_ema:\n",
    "            self.model_ema.store(self.parameters())\n",
    "            self.model_ema.copy_to(self)\n",
    "            if context is not None:\n",
    "                print(f\"{context}: Switched to EMA weights\")\n",
    "        try:\n",
    "            yield None\n",
    "        finally:\n",
    "            if self.use_ema:\n",
    "                self.model_ema.restore(self.parameters())\n",
    "                if context is not None:\n",
    "                    print(f\"{context}: Restored training weights\")\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
    "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
    "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
    "        if len(missing) > 0:\n",
    "            print(f\"Missing Keys: {missing}\")\n",
    "            print(f\"Unexpected Keys: {unexpected}\")\n",
    "\n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        if self.use_ema:\n",
    "            self.model_ema(self)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        quant, emb_loss, info = self.quantize(h)\n",
    "        return quant, emb_loss, info\n",
    "\n",
    "    def encode_to_prequant(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "\n",
    "    def decode(self, quant):\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant)\n",
    "        return dec\n",
    "\n",
    "    def decode_code(self, code_b):\n",
    "        quant_b = self.quantize.embed_code(code_b)\n",
    "        dec = self.decode(quant_b)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, return_pred_indices=False):\n",
    "        quant, diff, (_,_,ind) = self.encode(input)\n",
    "        dec = self.decode(quant)\n",
    "        if return_pred_indices:\n",
    "            return dec, diff, ind\n",
    "        return dec, diff\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
    "        if self.batch_resize_range is not None:\n",
    "            lower_size = self.batch_resize_range[0]\n",
    "            upper_size = self.batch_resize_range[1]\n",
    "            if self.global_step <= 4:\n",
    "                # do the first few batches with max size to avoid later oom\n",
    "                new_resize = upper_size\n",
    "            else:\n",
    "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
    "            if new_resize != x.shape[2]:\n",
    "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
    "            x = x.detach()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # https://github.com/pytorch/pytorch/issues/37142\n",
    "        # try not to fool the heuristics\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # autoencode\n",
    "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
    "                                            predicted_indices=ind)\n",
    "\n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # discriminator\n",
    "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        log_dict = self._validation_step(batch, batch_idx)\n",
    "        with self.ema_scope():\n",
    "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
    "        return log_dict\n",
    "\n",
    "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
    "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
    "                                        self.global_step,\n",
    "                                        last_layer=self.get_last_layer(),\n",
    "                                        split=\"val\"+suffix,\n",
    "                                        predicted_indices=ind\n",
    "                                        )\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
    "                                            self.global_step,\n",
    "                                            last_layer=self.get_last_layer(),\n",
    "                                            split=\"val\"+suffix,\n",
    "                                            predicted_indices=ind\n",
    "                                            )\n",
    "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
    "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
    "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
    "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
    "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr_d = self.learning_rate\n",
    "        lr_g = self.lr_g_factor*self.learning_rate\n",
    "        print(\"lr_d\", lr_d)\n",
    "        print(\"lr_g\", lr_g)\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quantize.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr_g, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr_d, betas=(0.5, 0.9))\n",
    "\n",
    "        if self.scheduler_config is not None:\n",
    "            scheduler = instantiate_from_config(self.scheduler_config)\n",
    "\n",
    "            print(\"Setting up LambdaLR scheduler...\")\n",
    "            scheduler = [\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                },\n",
    "                {\n",
    "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
    "                    'interval': 'step',\n",
    "                    'frequency': 1\n",
    "                },\n",
    "            ]\n",
    "            return [opt_ae, opt_disc], scheduler\n",
    "        return [opt_ae, opt_disc], []\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.conv_out.weight\n",
    "\n",
    "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        x = x.to(self.device)\n",
    "        if only_inputs:\n",
    "            log[\"inputs\"] = x\n",
    "            return log\n",
    "        xrec, _ = self(x)\n",
    "        if x.shape[1] > 3:\n",
    "            # colorize with random projection\n",
    "            assert xrec.shape[1] > 3\n",
    "            x = self.to_rgb(x)\n",
    "            xrec = self.to_rgb(xrec)\n",
    "        log[\"inputs\"] = x\n",
    "        log[\"reconstructions\"] = xrec\n",
    "        if plot_ema:\n",
    "            with self.ema_scope():\n",
    "                xrec_ema, _ = self(x)\n",
    "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
    "                log[\"reconstructions_ema\"] = xrec_ema\n",
    "        return log\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
    "        x = F.conv2d(x, weight=self.colorize)\n",
    "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class VQModelInterface(VQModel):\n",
    "    def __init__(self, embed_dim, *args, **kwargs):\n",
    "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "\n",
    "    def decode(self, h, force_not_quantize=False):\n",
    "        # also go through quantization layer\n",
    "        if not force_not_quantize:\n",
    "            quant, emb_loss, info = self.quantize(h)\n",
    "        else:\n",
    "            quant = h\n",
    "        quant = self.post_quant_conv(quant)\n",
    "        dec = self.decoder(quant)\n",
    "        return dec\n",
    "\n",
    "\n",
    "class AutoencoderKL(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 ddconfig,\n",
    "                 lossconfig,\n",
    "                 embed_dim,\n",
    "                 ckpt_path=None,\n",
    "                 ignore_keys=[],\n",
    "                 image_key=\"image\",\n",
    "                 colorize_nlabels=None,\n",
    "                 monitor=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.image_key = image_key\n",
    "        self.encoder = DiffusionEncoder(**ddconfig)\n",
    "        self.decoder = Decoder(**ddconfig)\n",
    "        self.loss = instantiate_from_config(lossconfig)\n",
    "        assert ddconfig[\"double_z\"]\n",
    "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
    "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
    "        self.embed_dim = embed_dim\n",
    "        if colorize_nlabels is not None:\n",
    "            assert type(colorize_nlabels)==int\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
    "        if monitor is not None:\n",
    "            self.monitor = monitor\n",
    "        if ckpt_path is not None:\n",
    "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
    "        self.trainable = False\n",
    "\n",
    "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
    "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        keys = list(sd.keys())\n",
    "        for k in keys:\n",
    "            for ik in ignore_keys:\n",
    "                if k.startswith(ik):\n",
    "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
    "                    del sd[k]\n",
    "        self.load_state_dict(sd, strict=False)\n",
    "        print(f\"Restored from {path}\")\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        moments = self.quant_conv(h)\n",
    "        posterior = DiagonalGaussianDistribution(moments)\n",
    "        return posterior\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.post_quant_conv(z)\n",
    "        dec = self.decoder(z)\n",
    "        return dec\n",
    "\n",
    "    def forward(self, input, sample_posterior=True):\n",
    "        posterior = self.encode(input)\n",
    "        if sample_posterior:\n",
    "            z = posterior.sample()\n",
    "        else:\n",
    "            z = posterior.mode()\n",
    "        dec = self.decode(z)\n",
    "        return dec, posterior\n",
    "\n",
    "    def get_input(self, batch, k):\n",
    "        x = batch[k]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x[..., None]\n",
    "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        inputs = self.get_input(batch, self.image_key)\n",
    "        reconstructions, posterior = self(inputs)\n",
    "\n",
    "        if optimizer_idx == 0:\n",
    "            # train encoder+decoder+logvar\n",
    "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
    "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=False, on_epoch=False)\n",
    "            return aeloss\n",
    "\n",
    "        if optimizer_idx == 1:\n",
    "            # train the discriminator\n",
    "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
    "                                                last_layer=self.get_last_layer(), split=\"train\")\n",
    "\n",
    "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
    "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=False, on_epoch=False)\n",
    "            return discloss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs = self.get_input(batch, self.image_key)\n",
    "        reconstructions, posterior = self(inputs)\n",
    "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n",
    "                                        last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n",
    "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
    "\n",
    "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
    "        self.log_dict(log_dict_ae)\n",
    "        self.log_dict(log_dict_disc)\n",
    "        return self.log_dict\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
    "                                  list(self.decoder.parameters())+\n",
    "                                  list(self.quant_conv.parameters())+\n",
    "                                  list(self.post_quant_conv.parameters()),\n",
    "                                  lr=lr, betas=(0.5, 0.9))\n",
    "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
    "                                    lr=lr, betas=(0.5, 0.9))\n",
    "        return [opt_ae, opt_disc], []\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.decoder.conv_out.weight\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
    "        log = dict()\n",
    "        x = self.get_input(batch, self.image_key)\n",
    "        x = x.to(self.device)\n",
    "        if not only_inputs:\n",
    "            xrec, posterior = self(x)\n",
    "            if x.shape[1] > 3:\n",
    "                # colorize with random projection\n",
    "                assert xrec.shape[1] > 3\n",
    "                x = self.to_rgb(x)\n",
    "                xrec = self.to_rgb(xrec)\n",
    "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
    "            log[\"reconstructions\"] = xrec\n",
    "        log[\"inputs\"] = x\n",
    "        return log\n",
    "\n",
    "    def to_rgb(self, x):\n",
    "        assert self.image_key == \"segmentation\"\n",
    "        if not hasattr(self, \"colorize\"):\n",
    "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
    "        x = F.conv2d(x, weight=self.colorize)\n",
    "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
    "        return x\n",
    "\n",
    "\n",
    "class IdentityFirstStage(torch.nn.Module):\n",
    "    def __init__(self, *args, vq_interface=False, **kwargs):\n",
    "        self.vq_interface = vq_interface  # TODO: Should be true by default but check to not break older stuff\n",
    "        super().__init__()\n",
    "\n",
    "    def encode(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "    def quantize(self, x, *args, **kwargs):\n",
    "        if self.vq_interface:\n",
    "            return x, None, [None, None, None]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288249803,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "qr2KgMyR39nB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Linear Attention\n",
    "#@title Linear attention\n",
    "from inspect import isfunction\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def uniq(arr):\n",
    "    return{el: True for el in arr}.keys()\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def max_neg_value(t):\n",
    "    return -torch.finfo(t.dtype).max\n",
    "\n",
    "\n",
    "def init_(tensor):\n",
    "    dim = tensor.shape[-1]\n",
    "    std = 1 / math.sqrt(dim)\n",
    "    tensor.uniform_(-std, std)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# feedforward\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
    "        k = k.softmax(dim=-1)\n",
    "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
    "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
    "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
    "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
    "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
    "\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
    "        w_ = rearrange(w_, 'b i j -> b j i')\n",
    "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
    "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module): # Optimize this module as well\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0., cond_scale=1.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.cond_scale = cond_scale\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, query_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k = self.to_k(context)\n",
    "        v = self.to_v(context)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class BasicTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True, cond_scale=1.):\n",
    "        super().__init__()\n",
    "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout,cond_scale=cond_scale)  # is a self-attention\n",
    "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
    "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n",
    "                                    heads=n_heads, dim_head=d_head, dropout=dropout,cond_scale=cond_scale)  # is self-attn if context is none\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
    "\n",
    "    def _forward(self, x, context=None):\n",
    "        x = self.attn1(self.norm1(x)) + x\n",
    "        x = self.attn2(self.norm2(x), context=context) + x\n",
    "        x = self.ff(self.norm3(x)) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block for image-like data.\n",
    "    First, project the input (aka embedding)\n",
    "    and reshape to b, t, d.\n",
    "    Then apply standard transformer action.\n",
    "    Finally, reshape to image\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, n_heads, d_head,\n",
    "                 depth=1, dropout=0., context_dim=None, cond_scale=1.):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        inner_dim = n_heads * d_head\n",
    "        self.norm = Normalize(in_channels)\n",
    "\n",
    "        self.proj_in = nn.Conv2d(in_channels,\n",
    "                                 inner_dim,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim,cond_scale=cond_scale)\n",
    "                for d in range(depth)]\n",
    "        )\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv2d(inner_dim,\n",
    "                                              in_channels,\n",
    "                                              kernel_size=1,\n",
    "                                              stride=1,\n",
    "                                              padding=0))\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        # note: if no context is given, cross-attention defaults to self-attention\n",
    "        b, c, h, w = x.shape\n",
    "        x_in = x\n",
    "        x = self.norm(x)\n",
    "        x = self.proj_in(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, context=context)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
    "        x = self.proj_out(x)\n",
    "        return x + x_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701288250638,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "Z1LWrerB4F2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title OpenAI Model\n",
    "#@title OpenAI model\n",
    "from abc import abstractmethod\n",
    "from functools import partial\n",
    "import math\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# dummy replace\n",
    "def convert_module_to_f16(x):\n",
    "    pass\n",
    "\n",
    "def convert_module_to_f32(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "## go\n",
    "class AttentionPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from CLIP: https://github.com/openai/CLIP/blob/main/clip/model.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        spacial_dim: int,\n",
    "        embed_dim: int,\n",
    "        num_heads_channels: int,\n",
    "        output_dim: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(th.randn(embed_dim, spacial_dim ** 2 + 1) / embed_dim ** 0.5)\n",
    "        self.qkv_proj = conv_nd(1, embed_dim, 3 * embed_dim, 1)\n",
    "        self.c_proj = conv_nd(1, embed_dim, output_dim or embed_dim, 1)\n",
    "        self.num_heads = embed_dim // num_heads_channels\n",
    "        self.attention = QKVAttention(self.num_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, *_spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)  # NC(HW)\n",
    "        x = th.cat([x.mean(dim=-1, keepdim=True), x], dim=-1)  # NC(HW+1)\n",
    "        x = x + self.positional_embedding[None, :, :].to(x.dtype)  # NC(HW+1)\n",
    "        x = self.qkv_proj(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x[:, :, 0].contiguous()\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Any module where forward() takes timestep embeddings as a second argument.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)\n",
    "            elif isinstance(layer, SpatialTransformer):\n",
    "                x = layer(x, context)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleAI(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
    "                 upsampling occurs in the inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        if use_conv:\n",
    "            self.conv = conv_nd(dims, self.channels, self.out_channels, 3, padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.dims == 3:\n",
    "            x = F.interpolate(\n",
    "                x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\"\n",
    "            )\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class TransposedUpsample(nn.Module):\n",
    "    'Learned 2x upsampling without padding'\n",
    "    def __init__(self, channels, out_channels=None, ks=5):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(self.channels,self.out_channels,kernel_size=ks,stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class DownsampleAI(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then\n",
    "                 downsampling occurs in the inner-two dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, dims=2, out_channels=None,padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.dims = dims\n",
    "        stride = 2 if dims != 3 else (1, 2, 2)\n",
    "        if use_conv:\n",
    "            self.op = conv_nd(\n",
    "                dims, self.channels, self.out_channels, 3, stride=stride, padding=padding\n",
    "            )\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = avg_pool_nd(dims, kernel_size=stride, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "    :param channels: the number of input channels.\n",
    "    :param emb_channels: the number of timestep embedding channels.\n",
    "    :param dropout: the rate of dropout.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    :param use_conv: if True and out_channels is specified, use a spatial\n",
    "        convolution instead of a smaller 1x1 convolution to change the\n",
    "        channels in the skip connection.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param use_checkpoint: if True, use gradient checkpointing on this module.\n",
    "    :param up: if True, use this block for upsampling.\n",
    "    :param down: if True, use this block for downsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            normalization(channels),\n",
    "            nn.SiLU(),\n",
    "            conv_nd(dims, channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = UpsampleAI(channels, False, dims)\n",
    "            self.x_upd = UpsampleAI(channels, False, dims)\n",
    "        elif down:\n",
    "            self.h_upd = DownsampleAI(channels, False, dims)\n",
    "            self.x_upd = DownsampleAI(channels, False, dims)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels if use_scale_shift_norm else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            normalization(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                conv_nd(dims, self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = conv_nd(\n",
    "                dims, channels, self.out_channels, 3, padding=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = conv_nd(dims, channels, self.out_channels, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the block to a Tensor, conditioned on a timestep embedding.\n",
    "        :param x: an [N x C x ...] Tensor of features.\n",
    "        :param emb: an [N x emb_channels] Tensor of timestep embeddings.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        return checkpoint(\n",
    "            self._forward, (x, emb), self.parameters(), self.use_checkpoint\n",
    "        )\n",
    "\n",
    "\n",
    "    def _forward(self, x, emb):\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = th.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "        use_new_attention_order=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert (\n",
    "                channels % num_head_channels == 0\n",
    "            ), f\"q,k,v channels {channels} is not divisible by num_head_channels {num_head_channels}\"\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = normalization(channels)\n",
    "        self.qkv = conv_nd(1, channels, channels * 3, 1)\n",
    "        if use_new_attention_order:\n",
    "            # split qkv before split heads\n",
    "            self.attention = QKVAttention(self.num_heads)\n",
    "        else:\n",
    "            # split heads before split qkv\n",
    "            self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(conv_nd(1, channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return checkpoint(self._forward, (x,), self.parameters(), True)   # TODO: check checkpoint usage, is True # TODO: fix the .half call!!!\n",
    "        #return pt_checkpoint(self._forward, x)  # pytorch\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x)).contiguous()\n",
    "        h = self.attention(qkv).contiguous()\n",
    "        h = self.proj_out(h).contiguous()\n",
    "        return (x + h).reshape(b, c, *spatial).contiguous()\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial ** 2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention and splits in a different order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (3 * H * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\",\n",
    "            (q * scale).view(bs * self.n_heads, ch, length).contiguous(),\n",
    "            (k * scale).view(bs * self.n_heads, ch, length).contiguous(),\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v.reshape(bs * self.n_heads, ch, length))\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full UNet model with attention and timestep embedding.\n",
    "    :param in_channels: channels in the input Tensor.\n",
    "    :param model_channels: base channel count for the model.\n",
    "    :param out_channels: channels in the output Tensor.\n",
    "    :param num_res_blocks: number of residual blocks per downsample.\n",
    "    :param attention_resolutions: a collection of downsample rates at which\n",
    "        attention will take place. May be a set, list, or tuple.\n",
    "        For example, if this contains 4, then at 4x downsampling, attention\n",
    "        will be used.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param channel_mult: channel multiplier for each level of the UNet.\n",
    "    :param conv_resample: if True, use learned convolutions for upsampling and\n",
    "        downsampling.\n",
    "    :param dims: determines if the signal is 1D, 2D, or 3D.\n",
    "    :param num_classes: if specified (as an int), then this model will be\n",
    "        class-conditional with `num_classes` classes.\n",
    "    :param use_checkpoint: use gradient checkpointing to reduce memory usage.\n",
    "    :param num_heads: the number of attention heads in each attention layer.\n",
    "    :param num_heads_channels: if specified, ignore num_heads and instead use\n",
    "                               a fixed channel width per attention head.\n",
    "    :param num_heads_upsample: works with num_heads to set a different number\n",
    "                               of heads for upsampling. Deprecated.\n",
    "    :param use_scale_shift_norm: use a FiLM-like conditioning mechanism.\n",
    "    :param resblock_updown: use residual blocks for up/downsampling.\n",
    "    :param use_new_attention_order: use a different attention pattern for potentially\n",
    "                                    increased efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        num_classes=None,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=-1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        use_spatial_transformer=False,    # custom transformer support\n",
    "        transformer_depth=1,              # custom transformer support\n",
    "        context_dim=None,                 # custom transformer support\n",
    "        n_embed=None,                     # custom support for prediction of discrete ids into codebook of first stage vq model\n",
    "        legacy=True,\n",
    "        cond_scale=1.0,\n",
    "        global_pool=False,\n",
    "        use_time_cond=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if use_spatial_transformer:\n",
    "            assert context_dim is not None, 'Fool!! You forgot to include the dimension of your cross-attention conditioning...'\n",
    "\n",
    "        if context_dim is not None:\n",
    "            assert use_spatial_transformer, 'Fool!! You forgot to use the spatial transformer for your cross-attention conditioning...'\n",
    "            from omegaconf.listconfig import ListConfig\n",
    "            if type(context_dim) == ListConfig:\n",
    "                context_dim = list(context_dim)\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        if num_heads == -1:\n",
    "            assert num_head_channels != -1, 'Either num_heads or num_head_channels has to be set'\n",
    "\n",
    "        if num_head_channels == -1:\n",
    "            assert num_heads != -1, 'Either num_heads or num_head_channels has to be set'\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.num_classes = num_classes\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "        self.predict_codebook_ids = n_embed is not None\n",
    "        self.cond_scale = cond_scale\n",
    "        self.use_time_cond = use_time_cond\n",
    "        self.global_pool = global_pool\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            self.label_emb = nn.Embedding(num_classes, time_embed_dim)\n",
    "\n",
    "        # self.time_embed_condtion = nn.Linear(context_dim, time_embed_dim, bias=False)\n",
    "        if use_time_cond:\n",
    "            self.time_embed_condtion = nn.Sequential(\n",
    "                nn.Conv1d(77, 77//2, 1, bias=True),\n",
    "                nn.Conv1d(77//2, 1, 1, bias=True),\n",
    "                nn.Linear(context_dim, time_embed_dim, bias=True)\n",
    "            ) if global_pool == False else nn.Linear(context_dim, time_embed_dim, bias=True)\n",
    "\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TimestepEmbedSequential(\n",
    "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self._feature_size = model_channels\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=mult * model_channels,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "                    if legacy:\n",
    "                        #num_heads = 1\n",
    "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=dim_head,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,cond_scale=cond_scale\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else DownsampleAI(\n",
    "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        if num_head_channels == -1:\n",
    "            dim_head = ch // num_heads\n",
    "        else:\n",
    "            num_heads = ch // num_head_channels\n",
    "            dim_head = num_head_channels\n",
    "        if legacy:\n",
    "            #num_heads = 1\n",
    "            dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=dim_head,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim, cond_scale=cond_scale\n",
    "                        ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1):\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch + ich,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=model_channels * mult,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "                    if legacy:\n",
    "                        #num_heads = 1\n",
    "                        dim_head = ch // num_heads if use_spatial_transformer else num_head_channels\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads_upsample,\n",
    "                            num_head_channels=dim_head,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        ) if not use_spatial_transformer else SpatialTransformer(\n",
    "                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim,cond_scale=cond_scale\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            up=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else UpsampleAI(ch, conv_resample, dims=dims, out_channels=out_ch)\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(conv_nd(dims, model_channels, out_channels, 3, padding=1)),\n",
    "        )\n",
    "        if self.predict_codebook_ids:\n",
    "            self.id_predictor = nn.Sequential(\n",
    "            normalization(ch),\n",
    "            conv_nd(dims, model_channels, n_embed, 1),\n",
    "            #nn.LogSoftmax(dim=1)  # change to cross_entropy and produce non-normalized logits\n",
    "        )\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float16.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "        self.output_blocks.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float32.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "        self.output_blocks.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, x, timesteps=None, context=None, y=None,**kwargs):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :param context: conditioning plugged in via crossattn\n",
    "        :param y: an [N] Tensor of labels, if class-conditional.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        assert (y is not None) == (\n",
    "            self.num_classes is not None\n",
    "        ), \"must specify y if and only if the model is class-conditional\"\n",
    "        hs = []\n",
    "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        if self.num_classes is not None:\n",
    "            assert y.shape == (x.shape[0],)\n",
    "            emb = emb + self.label_emb(y)\n",
    "        if self.use_time_cond: # add time conditioning\n",
    "            c = self.time_embed_condtion(context)\n",
    "            assert c.shape[1] == 1, f'found {c.shape}'\n",
    "            emb = emb + torch.squeeze(c, dim=1)\n",
    "\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb, context)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb, context)\n",
    "        for module in self.output_blocks:\n",
    "            h = th.cat([h, hs.pop()], dim=1)\n",
    "            h = module(h, emb, context)\n",
    "        h = h.type(x.dtype)\n",
    "        if self.predict_codebook_ids:\n",
    "            return self.id_predictor(h)\n",
    "        else:\n",
    "            return self.out(h).contiguous()\n",
    "\n",
    "\n",
    "class EncoderUNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The half UNet model with attention and timestep embedding.\n",
    "    For usage, see UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size,\n",
    "        in_channels,\n",
    "        model_channels,\n",
    "        out_channels,\n",
    "        num_res_blocks,\n",
    "        attention_resolutions,\n",
    "        dropout=0,\n",
    "        channel_mult=(1, 2, 4, 8),\n",
    "        conv_resample=True,\n",
    "        dims=2,\n",
    "        use_checkpoint=False,\n",
    "        use_fp16=False,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        num_heads_upsample=-1,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        pool=\"adaptive\",\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if num_heads_upsample == -1:\n",
    "            num_heads_upsample = num_heads\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.conv_resample = conv_resample\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.dtype = th.float16 if use_fp16 else th.float32\n",
    "        self.num_heads = num_heads\n",
    "        self.num_head_channels = num_head_channels\n",
    "        self.num_heads_upsample = num_heads_upsample\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        self.input_blocks = nn.ModuleList(\n",
    "            [\n",
    "                TimestepEmbedSequential(\n",
    "                    conv_nd(dims, in_channels, model_channels, 3, padding=1)\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        self._feature_size = model_channels\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks):\n",
    "                layers = [\n",
    "                    ResBlock(\n",
    "                        ch,\n",
    "                        time_embed_dim,\n",
    "                        dropout,\n",
    "                        out_channels=mult * model_channels,\n",
    "                        dims=dims,\n",
    "                        use_checkpoint=use_checkpoint,\n",
    "                        use_scale_shift_norm=use_scale_shift_norm,\n",
    "                    )\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(\n",
    "                        AttentionBlock(\n",
    "                            ch,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            num_heads=num_heads,\n",
    "                            num_head_channels=num_head_channels,\n",
    "                            use_new_attention_order=use_new_attention_order,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                self._feature_size += ch\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(\n",
    "                    TimestepEmbedSequential(\n",
    "                        ResBlock(\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            dropout,\n",
    "                            out_channels=out_ch,\n",
    "                            dims=dims,\n",
    "                            use_checkpoint=use_checkpoint,\n",
    "                            use_scale_shift_norm=use_scale_shift_norm,\n",
    "                            down=True,\n",
    "                        )\n",
    "                        if resblock_updown\n",
    "                        else DownsampleAI(\n",
    "                            ch, conv_resample, dims=dims, out_channels=out_ch\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "                self._feature_size += ch\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "            AttentionBlock(\n",
    "                ch,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                num_heads=num_heads,\n",
    "                num_head_channels=num_head_channels,\n",
    "                use_new_attention_order=use_new_attention_order,\n",
    "            ),\n",
    "            ResBlock(\n",
    "                ch,\n",
    "                time_embed_dim,\n",
    "                dropout,\n",
    "                dims=dims,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                use_scale_shift_norm=use_scale_shift_norm,\n",
    "            ),\n",
    "        )\n",
    "        self._feature_size += ch\n",
    "        self.pool = pool\n",
    "        if pool == \"adaptive\":\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                zero_module(conv_nd(dims, ch, out_channels, 1)),\n",
    "                nn.Flatten(),\n",
    "            )\n",
    "        elif pool == \"attention\":\n",
    "            assert num_head_channels != -1\n",
    "            self.out = nn.Sequential(\n",
    "                normalization(ch),\n",
    "                nn.SiLU(),\n",
    "                AttentionPool2d(\n",
    "                    (image_size // ds), ch, num_head_channels, out_channels\n",
    "                ),\n",
    "            )\n",
    "        elif pool == \"spatial\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        elif pool == \"spatial_v2\":\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(self._feature_size, 2048),\n",
    "                normalization(2048),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(2048, self.out_channels),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unexpected {pool} pooling\")\n",
    "\n",
    "    def convert_to_fp16(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float16.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f16)\n",
    "        self.middle_block.apply(convert_module_to_f16)\n",
    "\n",
    "    def convert_to_fp32(self):\n",
    "        \"\"\"\n",
    "        Convert the torso of the model to float32.\n",
    "        \"\"\"\n",
    "        self.input_blocks.apply(convert_module_to_f32)\n",
    "        self.middle_block.apply(convert_module_to_f32)\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x K] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb = self.time_embed(timestep_embedding(timesteps, self.model_channels))\n",
    "\n",
    "        results = []\n",
    "        h = x.type(self.dtype)\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb)\n",
    "            if self.pool.startswith(\"spatial\"):\n",
    "                results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "        h = self.middle_block(h, emb)\n",
    "        if self.pool.startswith(\"spatial\"):\n",
    "            results.append(h.type(x.dtype).mean(dim=(2, 3)))\n",
    "            h = th.cat(results, axis=-1)\n",
    "            return self.out(h)\n",
    "        else:\n",
    "            h = h.type(x.dtype)\n",
    "            return self.out(h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288250894,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "XtlT6QlH4RGo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title X-Transformer\n",
    "#@title X-transformer\n",
    "\"\"\"shout-out to https://github.com/lucidrains/x-transformers/tree/main/x_transformers\"\"\"\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "from collections import namedtuple\n",
    "from einops import rearrange, repeat, reduce\n",
    "\n",
    "# constants\n",
    "\n",
    "DEFAULT_DIM_HEAD = 64\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'pre_softmax_attn',\n",
    "    'post_softmax_attn'\n",
    "])\n",
    "\n",
    "LayerIntermediates = namedtuple('Intermediates', [\n",
    "    'hiddens',\n",
    "    'attn_intermediates'\n",
    "])\n",
    "\n",
    "\n",
    "class AbsolutePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(max_seq_len, dim)\n",
    "        self.init_()\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.normal_(self.emb.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.arange(x.shape[-2], device=x.device)\n",
    "        return self.emb(n)[None, ...]\n",
    "\n",
    "\n",
    "class FixedPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "\n",
    "    def forward(self, x, seq_dim=1, offset=0):\n",
    "        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq) + offset\n",
    "        sinusoid_inp = torch.einsum('i , j -> i j', t, self.inv_freq)\n",
    "        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n",
    "        return emb[None, :, :]\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "\n",
    "def always(val):\n",
    "    def inner(*args, **kwargs):\n",
    "        return val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def not_equals(val):\n",
    "    def inner(x):\n",
    "        return x != val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def equals(val):\n",
    "    def inner(x):\n",
    "        return x == val\n",
    "    return inner\n",
    "\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "\n",
    "# keyword argument helpers\n",
    "\n",
    "def pick_and_pop(keys, d):\n",
    "    values = list(map(lambda key: d.pop(key), keys))\n",
    "    return dict(zip(keys, values))\n",
    "\n",
    "\n",
    "def group_dict_by_key(cond, d):\n",
    "    return_val = [dict(), dict()]\n",
    "    for key in d.keys():\n",
    "        match = bool(cond(key))\n",
    "        ind = int(not match)\n",
    "        return_val[ind][key] = d[key]\n",
    "    return (*return_val,)\n",
    "\n",
    "\n",
    "def string_begins_with(prefix, str):\n",
    "    return str.startswith(prefix)\n",
    "\n",
    "\n",
    "def group_by_key_prefix(prefix, d):\n",
    "    return group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "\n",
    "\n",
    "def groupby_prefix_and_trim(prefix, d):\n",
    "    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n",
    "    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "    return kwargs_without_prefix, kwargs\n",
    "\n",
    "\n",
    "# classes\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, value, fn):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x, *rest = self.fn(x, **kwargs)\n",
    "        return (x * self.value, *rest)\n",
    "\n",
    "\n",
    "class Rezero(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.g = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x, *rest = self.fn(x, **kwargs)\n",
    "        return (x * self.g, *rest)\n",
    "\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n",
    "        return x / norm.clamp(min=self.eps) * self.g\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** -0.5\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n",
    "        return x / norm.clamp(min=self.eps) * self.g\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def forward(self, x, residual):\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class GRUGating(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRUCell(dim, dim)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        gated_output = self.gru(\n",
    "            rearrange(x, 'b n d -> (b n) d'),\n",
    "            rearrange(residual, 'b n d -> (b n) d')\n",
    "        )\n",
    "\n",
    "        return gated_output.reshape_as(x)\n",
    "\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
    "        return x * F.gelu(gate)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = int(dim * mult)\n",
    "        dim_out = default(dim_out, dim)\n",
    "        project_in = nn.Sequential(\n",
    "            nn.Linear(dim, inner_dim),\n",
    "            nn.GELU()\n",
    "        ) if not glu else GEGLU(dim, inner_dim)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            project_in,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(inner_dim, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# attention.\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            dim_head=DEFAULT_DIM_HEAD,\n",
    "            heads=8,\n",
    "            causal=False,\n",
    "            mask=None,\n",
    "            talking_heads=False,\n",
    "            sparse_topk=None,\n",
    "            use_entmax15=False,\n",
    "            num_mem_kv=0,\n",
    "            dropout=0.,\n",
    "            on_attn=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if use_entmax15:\n",
    "            raise NotImplementedError(\"Check out entmax activation instead of softmax activation!\")\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.mask = mask\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # talking heads\n",
    "        self.talking_heads = talking_heads\n",
    "        if talking_heads:\n",
    "            self.pre_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n",
    "            self.post_softmax_proj = nn.Parameter(torch.randn(heads, heads))\n",
    "\n",
    "        # explicit topk sparse attention\n",
    "        self.sparse_topk = sparse_topk\n",
    "\n",
    "        # entmax\n",
    "        #self.attn_fn = entmax15 if use_entmax15 else F.softmax\n",
    "        self.attn_fn = F.softmax\n",
    "\n",
    "        # add memory key / values\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "        if num_mem_kv > 0:\n",
    "            self.mem_k = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "            self.mem_v = nn.Parameter(torch.randn(heads, num_mem_kv, dim_head))\n",
    "\n",
    "        # attention on attention\n",
    "        self.attn_on_attn = on_attn\n",
    "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim * 2), nn.GLU()) if on_attn else nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            context=None,\n",
    "            mask=None,\n",
    "            context_mask=None,\n",
    "            rel_pos=None,\n",
    "            sinusoidal_emb=None,\n",
    "            prev_attn=None,\n",
    "            mem=None\n",
    "    ):\n",
    "        b, n, _, h, talking_heads, device = *x.shape, self.heads, self.talking_heads, x.device\n",
    "        kv_input = default(context, x)\n",
    "\n",
    "        q_input = x\n",
    "        k_input = kv_input\n",
    "        v_input = kv_input\n",
    "\n",
    "        if exists(mem):\n",
    "            k_input = torch.cat((mem, k_input), dim=-2)\n",
    "            v_input = torch.cat((mem, v_input), dim=-2)\n",
    "\n",
    "        if exists(sinusoidal_emb):\n",
    "            # in shortformer, the query would start at a position offset depending on the past cached memory\n",
    "            offset = k_input.shape[-2] - q_input.shape[-2]\n",
    "            q_input = q_input + sinusoidal_emb(q_input, offset=offset)\n",
    "            k_input = k_input + sinusoidal_emb(k_input)\n",
    "\n",
    "        q = self.to_q(q_input)\n",
    "        k = self.to_k(k_input)\n",
    "        v = self.to_v(v_input)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "\n",
    "        input_mask = None\n",
    "        if any(map(exists, (mask, context_mask))):\n",
    "            q_mask = default(mask, lambda: torch.ones((b, n), device=device).bool())\n",
    "            k_mask = q_mask if not exists(context) else context_mask\n",
    "            k_mask = default(k_mask, lambda: torch.ones((b, k.shape[-2]), device=device).bool())\n",
    "            q_mask = rearrange(q_mask, 'b i -> b () i ()')\n",
    "            k_mask = rearrange(k_mask, 'b j -> b () () j')\n",
    "            input_mask = q_mask * k_mask\n",
    "\n",
    "        if self.num_mem_kv > 0:\n",
    "            mem_k, mem_v = map(lambda t: repeat(t, 'h n d -> b h n d', b=b), (self.mem_k, self.mem_v))\n",
    "            k = torch.cat((mem_k, k), dim=-2)\n",
    "            v = torch.cat((mem_v, v), dim=-2)\n",
    "            if exists(input_mask):\n",
    "                input_mask = F.pad(input_mask, (self.num_mem_kv, 0), value=True)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        mask_value = max_neg_value(dots)\n",
    "\n",
    "        if exists(prev_attn):\n",
    "            dots = dots + prev_attn\n",
    "\n",
    "        pre_softmax_attn = dots\n",
    "\n",
    "        if talking_heads:\n",
    "            dots = einsum('b h i j, h k -> b k i j', dots, self.pre_softmax_proj).contiguous()\n",
    "\n",
    "        if exists(rel_pos):\n",
    "            dots = rel_pos(dots)\n",
    "\n",
    "        if exists(input_mask):\n",
    "            dots.masked_fill_(~input_mask, mask_value)\n",
    "            del input_mask\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = dots.shape[-2:]\n",
    "            r = torch.arange(i, device=device)\n",
    "            mask = rearrange(r, 'i -> () () i ()') < rearrange(r, 'j -> () () () j')\n",
    "            mask = F.pad(mask, (j - i, 0), value=False)\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        if exists(self.sparse_topk) and self.sparse_topk < dots.shape[-1]:\n",
    "            top, _ = dots.topk(self.sparse_topk, dim=-1)\n",
    "            vk = top[..., -1].unsqueeze(-1).expand_as(dots)\n",
    "            mask = dots < vk\n",
    "            dots.masked_fill_(mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        attn = self.attn_fn(dots, dim=-1)\n",
    "        post_softmax_attn = attn\n",
    "\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        if talking_heads:\n",
    "            attn = einsum('b h i j, h k -> b k i j', attn, self.post_softmax_proj).contiguous()\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "\n",
    "        intermediates = Intermediates(\n",
    "            pre_softmax_attn=pre_softmax_attn,\n",
    "            post_softmax_attn=post_softmax_attn\n",
    "        )\n",
    "\n",
    "        return self.to_out(out), intermediates\n",
    "\n",
    "\n",
    "class AttentionLayers(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            depth,\n",
    "            heads=8,\n",
    "            causal=False,\n",
    "            cross_attend=False,\n",
    "            only_cross=False,\n",
    "            use_scalenorm=False,\n",
    "            use_rmsnorm=False,\n",
    "            use_rezero=False,\n",
    "            rel_pos_num_buckets=32,\n",
    "            rel_pos_max_distance=128,\n",
    "            position_infused_attn=False,\n",
    "            custom_layers=None,\n",
    "            sandwich_coef=None,\n",
    "            par_ratio=None,\n",
    "            residual_attn=False,\n",
    "            cross_residual_attn=False,\n",
    "            macaron=False,\n",
    "            pre_norm=True,\n",
    "            gate_residual=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        ff_kwargs, kwargs = groupby_prefix_and_trim('ff_', kwargs)\n",
    "        attn_kwargs, _ = groupby_prefix_and_trim('attn_', kwargs)\n",
    "\n",
    "        dim_head = attn_kwargs.get('dim_head', DEFAULT_DIM_HEAD)\n",
    "\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        self.has_pos_emb = position_infused_attn\n",
    "        self.pia_pos_emb = FixedPositionalEmbedding(dim) if position_infused_attn else None\n",
    "        self.rotary_pos_emb = always(None)\n",
    "\n",
    "        assert rel_pos_num_buckets <= rel_pos_max_distance, 'number of relative position buckets must be less than the relative position max distance'\n",
    "        self.rel_pos = None\n",
    "\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "        self.residual_attn = residual_attn\n",
    "        self.cross_residual_attn = cross_residual_attn\n",
    "\n",
    "        norm_class = ScaleNorm if use_scalenorm else nn.LayerNorm\n",
    "        norm_class = RMSNorm if use_rmsnorm else norm_class\n",
    "        norm_fn = partial(norm_class, dim)\n",
    "\n",
    "        norm_fn = nn.Identity if use_rezero else norm_fn\n",
    "        branch_fn = Rezero if use_rezero else None\n",
    "\n",
    "        if cross_attend and not only_cross:\n",
    "            default_block = ('a', 'c', 'f')\n",
    "        elif cross_attend and only_cross:\n",
    "            default_block = ('c', 'f')\n",
    "        else:\n",
    "            default_block = ('a', 'f')\n",
    "\n",
    "        if macaron:\n",
    "            default_block = ('f',) + default_block\n",
    "\n",
    "        if exists(custom_layers):\n",
    "            layer_types = custom_layers\n",
    "        elif exists(par_ratio):\n",
    "            par_depth = depth * len(default_block)\n",
    "            assert 1 < par_ratio <= par_depth, 'par ratio out of range'\n",
    "            default_block = tuple(filter(not_equals('f'), default_block))\n",
    "            par_attn = par_depth // par_ratio\n",
    "            depth_cut = par_depth * 2 // 3  # 2 / 3 attention layer cutoff suggested by PAR paper\n",
    "            par_width = (depth_cut + depth_cut // par_attn) // par_attn\n",
    "            assert len(default_block) <= par_width, 'default block is too large for par_ratio'\n",
    "            par_block = default_block + ('f',) * (par_width - len(default_block))\n",
    "            par_head = par_block * par_attn\n",
    "            layer_types = par_head + ('f',) * (par_depth - len(par_head))\n",
    "        elif exists(sandwich_coef):\n",
    "            assert sandwich_coef > 0 and sandwich_coef <= depth, 'sandwich coefficient should be less than the depth'\n",
    "            layer_types = ('a',) * sandwich_coef + default_block * (depth - sandwich_coef) + ('f',) * sandwich_coef\n",
    "        else:\n",
    "            layer_types = default_block * depth\n",
    "\n",
    "        self.layer_types = layer_types\n",
    "        self.num_attn_layers = len(list(filter(equals('a'), layer_types)))\n",
    "\n",
    "        for layer_type in self.layer_types:\n",
    "            if layer_type == 'a':\n",
    "                layer = Attention(dim, heads=heads, causal=causal, **attn_kwargs)\n",
    "            elif layer_type == 'c':\n",
    "                layer = Attention(dim, heads=heads, **attn_kwargs)\n",
    "            elif layer_type == 'f':\n",
    "                layer = FeedForward(dim, **ff_kwargs)\n",
    "                layer = layer if not macaron else Scale(0.5, layer)\n",
    "            else:\n",
    "                raise Exception(f'invalid layer type {layer_type}')\n",
    "\n",
    "            if isinstance(layer, Attention) and exists(branch_fn):\n",
    "                layer = branch_fn(layer)\n",
    "\n",
    "            if gate_residual:\n",
    "                residual_fn = GRUGating(dim)\n",
    "            else:\n",
    "                residual_fn = Residual()\n",
    "\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                norm_fn(),\n",
    "                layer,\n",
    "                residual_fn\n",
    "            ]))\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            context=None,\n",
    "            mask=None,\n",
    "            context_mask=None,\n",
    "            mems=None,\n",
    "            return_hiddens=False\n",
    "    ):\n",
    "        hiddens = []\n",
    "        intermediates = []\n",
    "        prev_attn = None\n",
    "        prev_cross_attn = None\n",
    "\n",
    "        mems = mems.copy() if exists(mems) else [None] * self.num_attn_layers\n",
    "\n",
    "        for ind, (layer_type, (norm, block, residual_fn)) in enumerate(zip(self.layer_types, self.layers)):\n",
    "            is_last = ind == (len(self.layers) - 1)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                hiddens.append(x)\n",
    "                layer_mem = mems.pop(0)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            if self.pre_norm:\n",
    "                x = norm(x)\n",
    "\n",
    "            if layer_type == 'a':\n",
    "                out, inter = block(x, mask=mask, sinusoidal_emb=self.pia_pos_emb, rel_pos=self.rel_pos,\n",
    "                                   prev_attn=prev_attn, mem=layer_mem)\n",
    "            elif layer_type == 'c':\n",
    "                out, inter = block(x, context=context, mask=mask, context_mask=context_mask, prev_attn=prev_cross_attn)\n",
    "            elif layer_type == 'f':\n",
    "                out = block(x)\n",
    "\n",
    "            x = residual_fn(out, residual)\n",
    "\n",
    "            if layer_type in ('a', 'c'):\n",
    "                intermediates.append(inter)\n",
    "\n",
    "            if layer_type == 'a' and self.residual_attn:\n",
    "                prev_attn = inter.pre_softmax_attn\n",
    "            elif layer_type == 'c' and self.cross_residual_attn:\n",
    "                prev_cross_attn = inter.pre_softmax_attn\n",
    "\n",
    "            if not self.pre_norm and not is_last:\n",
    "                x = norm(x)\n",
    "\n",
    "        if return_hiddens:\n",
    "            intermediates = LayerIntermediates(\n",
    "                hiddens=hiddens,\n",
    "                attn_intermediates=intermediates\n",
    "            )\n",
    "\n",
    "            return x, intermediates\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(AttentionLayers):\n",
    "    def __init__(self, **kwargs):\n",
    "        assert 'causal' not in kwargs, 'cannot set causality on encoder'\n",
    "        super().__init__(causal=False, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerWrapper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            num_tokens,\n",
    "            max_seq_len,\n",
    "            attn_layers,\n",
    "            emb_dim=None,\n",
    "            max_mem_len=0.,\n",
    "            emb_dropout=0.,\n",
    "            num_memory_tokens=None,\n",
    "            tie_embedding=False,\n",
    "            use_pos_emb=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert isinstance(attn_layers, AttentionLayers), 'attention layers must be one of Encoder or Decoder'\n",
    "\n",
    "        dim = attn_layers.dim\n",
    "        emb_dim = default(emb_dim, dim)\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_mem_len = max_mem_len\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n",
    "        self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len) if (\n",
    "                    use_pos_emb and not attn_layers.has_pos_emb) else always(0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.project_emb = nn.Linear(emb_dim, dim) if emb_dim != dim else nn.Identity()\n",
    "        self.attn_layers = attn_layers\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.init_()\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_tokens) if not tie_embedding else lambda t: t @ self.token_emb.weight.t()\n",
    "\n",
    "        # memory tokens (like [cls]) from Memory Transformers paper\n",
    "        num_memory_tokens = default(num_memory_tokens, 0)\n",
    "        self.num_memory_tokens = num_memory_tokens\n",
    "        if num_memory_tokens > 0:\n",
    "            self.memory_tokens = nn.Parameter(torch.randn(num_memory_tokens, dim))\n",
    "\n",
    "            # let funnel encoder know number of memory tokens, if specified\n",
    "            if hasattr(attn_layers, 'num_memory_tokens'):\n",
    "                attn_layers.num_memory_tokens = num_memory_tokens\n",
    "\n",
    "    def init_(self):\n",
    "        nn.init.normal_(self.token_emb.weight, std=0.02)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            return_embeddings=False,\n",
    "            mask=None,\n",
    "            return_mems=False,\n",
    "            return_attn=False,\n",
    "            mems=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        # b, n, device, num_mem = *x.shape, x.device, self.num_memory_tokens\n",
    "        b = x.shape[0]\n",
    "        device = x.device\n",
    "        num_mem = self.num_memory_tokens\n",
    "\n",
    "        x = self.token_emb(x)\n",
    "        x += self.pos_emb(x)\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x = self.project_emb(x)\n",
    "\n",
    "        if num_mem > 0:\n",
    "            mem = repeat(self.memory_tokens, 'n d -> b n d', b=b)\n",
    "            x = torch.cat((mem, x), dim=1)\n",
    "\n",
    "            # auto-handle masking after appending memory tokens\n",
    "            if exists(mask):\n",
    "                mask = F.pad(mask, (num_mem, 0), value=True)\n",
    "\n",
    "        x, intermediates = self.attn_layers(x, mask=mask, mems=mems, return_hiddens=True, **kwargs)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        mem, x = x[:, :num_mem], x[:, num_mem:]\n",
    "\n",
    "        out = self.to_logits(x) if not return_embeddings else x\n",
    "\n",
    "        if return_mems:\n",
    "            hiddens = intermediates.hiddens\n",
    "            new_mems = list(map(lambda pair: torch.cat(pair, dim=-2), zip(mems, hiddens))) if exists(mems) else hiddens\n",
    "            new_mems = list(map(lambda t: t[..., -self.max_mem_len:, :].detach(), new_mems))\n",
    "            return out, new_mems\n",
    "\n",
    "        if return_attn:\n",
    "            attn_maps = list(map(lambda t: t.post_softmax_attn, intermediates.attn_intermediates))\n",
    "            return out, attn_maps\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1701288252227,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "N1918LG8-5WZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Diffusion Models\n",
    "#@title Diffusion models - did this ppl copy the internet?\n",
    "# pytorch_diffusion + derived encoder decoder\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
    "    return emb\n",
    "\n",
    "\n",
    "def nonlinearity(x):\n",
    "    # swish\n",
    "    return x*torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def Normalize(in_channels, num_groups=32):\n",
    "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        if self.with_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, with_conv):\n",
    "        super().__init__()\n",
    "        self.with_conv = with_conv\n",
    "        if self.with_conv:\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=2,\n",
    "                                        padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.with_conv:\n",
    "            pad = (0,1,0,1)\n",
    "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "            x = self.conv(x)\n",
    "        else:\n",
    "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
    "                 dropout, temb_channels=512):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_conv_shortcut = conv_shortcut\n",
    "\n",
    "        self.norm1 = Normalize(in_channels)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if temb_channels > 0:\n",
    "            self.temb_proj = torch.nn.Linear(temb_channels,\n",
    "                                             out_channels)\n",
    "        self.norm2 = Normalize(out_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = torch.nn.Conv2d(out_channels,\n",
    "                                     out_channels,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n",
    "                                                     out_channels,\n",
    "                                                     kernel_size=3,\n",
    "                                                     stride=1,\n",
    "                                                     padding=1)\n",
    "            else:\n",
    "                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n",
    "                                                    out_channels,\n",
    "                                                    kernel_size=1,\n",
    "                                                    stride=1,\n",
    "                                                    padding=0)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = x\n",
    "        h = self.norm1(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        if temb is not None:\n",
    "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
    "\n",
    "        h = self.norm2(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            if self.use_conv_shortcut:\n",
    "                x = self.conv_shortcut(x)\n",
    "            else:\n",
    "                x = self.nin_shortcut(x)\n",
    "\n",
    "        return x+h\n",
    "\n",
    "\n",
    "class LinAttnBlock(LinearAttention):\n",
    "    \"\"\"to match AttnBlock usage\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.norm = Normalize(in_channels)\n",
    "        self.q = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.k = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.v = torch.nn.Conv2d(in_channels,\n",
    "                                 in_channels,\n",
    "                                 kernel_size=1,\n",
    "                                 stride=1,\n",
    "                                 padding=0)\n",
    "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=1,\n",
    "                                        stride=1,\n",
    "                                        padding=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_ = x\n",
    "        h_ = self.norm(h_)\n",
    "        q = self.q(h_)\n",
    "        k = self.k(h_)\n",
    "        v = self.v(h_)\n",
    "\n",
    "        # compute attention\n",
    "        b,c,h,w = q.shape\n",
    "        q = q.reshape(b,c,h*w)\n",
    "        q = q.permute(0,2,1).contiguous()   # b,hw,c\n",
    "        k = k.reshape(b,c,h*w) # b,c,hw\n",
    "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
    "        w_ = w_ * (int(c)**(-0.5))\n",
    "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
    "\n",
    "        # attend to values\n",
    "        v = v.reshape(b,c,h*w)\n",
    "        w_ = w_.permute(0,2,1).contiguous()   # b,hw,hw (first hw of k, second of q)\n",
    "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
    "        h_ = h_.reshape(b,c,h,w)\n",
    "\n",
    "        h_ = self.proj_out(h_)\n",
    "\n",
    "        return x+h_\n",
    "\n",
    "\n",
    "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
    "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
    "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
    "    if attn_type == \"vanilla\":\n",
    "        return AttnBlock(in_channels)\n",
    "    elif attn_type == \"none\":\n",
    "        return nn.Identity(in_channels)\n",
    "    else:\n",
    "        return LinAttnBlock(in_channels)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = self.ch*4\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.use_timestep = use_timestep\n",
    "        if self.use_timestep:\n",
    "            # timestep embedding\n",
    "            self.temb = nn.Module()\n",
    "            self.temb.dense = nn.ModuleList([\n",
    "                torch.nn.Linear(self.ch,\n",
    "                                self.temb_ch),\n",
    "                torch.nn.Linear(self.temb_ch,\n",
    "                                self.temb_ch),\n",
    "            ])\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
    "                                       self.ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch*in_ch_mult[i_level]\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            skip_in = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                if i_block == self.num_res_blocks:\n",
    "                    skip_in = ch*in_ch_mult[i_level]\n",
    "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x, t=None, context=None):\n",
    "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
    "        if context is not None:\n",
    "            # assume aligned context, cat along channel axis\n",
    "            x = torch.cat((x, context), dim=1)\n",
    "        if self.use_timestep:\n",
    "            # timestep embedding\n",
    "            assert t is not None\n",
    "            temb = get_timestep_embedding(t, self.ch)\n",
    "            temb = self.temb.dense[0](temb)\n",
    "            temb = nonlinearity(temb)\n",
    "            temb = self.temb.dense[1](temb)\n",
    "        else:\n",
    "            temb = None\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                h = self.up[i_level].block[i_block](\n",
    "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "    def get_last_layer(self):\n",
    "        return self.conv_out.weight\n",
    "\n",
    "\n",
    "class DiffusionEncoder(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
    "                 **ignore_kwargs):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # downsampling\n",
    "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
    "                                       self.ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        curr_res = resolution\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        self.in_ch_mult = in_ch_mult\n",
    "        self.down = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_in = ch*in_ch_mult[i_level]\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            down = nn.Module()\n",
    "            down.block = block\n",
    "            down.attn = attn\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res // 2\n",
    "            self.down.append(down)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        2*z_channels if double_z else z_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # downsampling\n",
    "        hs = [self.conv_in(x)]\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            for i_block in range(self.num_res_blocks):\n",
    "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
    "                if len(self.down[i_level].attn) > 0:\n",
    "                    h = self.down[i_level].attn[i_block](h)\n",
    "                hs.append(h)\n",
    "            if i_level != self.num_resolutions-1:\n",
    "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
    "\n",
    "        # middle\n",
    "        h = hs[-1]\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # end\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
    "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
    "                 attn_type=\"vanilla\", **ignorekwargs):\n",
    "        super().__init__()\n",
    "        if use_linear_attn: attn_type = \"linear\"\n",
    "        self.ch = ch\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.resolution = resolution\n",
    "        self.in_channels = in_channels\n",
    "        self.give_pre_end = give_pre_end\n",
    "        self.tanh_out = tanh_out\n",
    "\n",
    "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
    "        in_ch_mult = (1,)+tuple(ch_mult)\n",
    "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
    "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
    "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
    "        print(\"Working with z of shape {} = {} dimensions.\".format(\n",
    "            self.z_shape, np.prod(self.z_shape)))\n",
    "\n",
    "        # z to block_in\n",
    "        self.conv_in = torch.nn.Conv2d(z_channels,\n",
    "                                       block_in,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1)\n",
    "\n",
    "        # middle\n",
    "        self.mid = nn.Module()\n",
    "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
    "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
    "                                       out_channels=block_in,\n",
    "                                       temb_channels=self.temb_ch,\n",
    "                                       dropout=dropout)\n",
    "\n",
    "        # upsampling\n",
    "        self.up = nn.ModuleList()\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            block = nn.ModuleList()\n",
    "            attn = nn.ModuleList()\n",
    "            block_out = ch*ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "                if curr_res in attn_resolutions:\n",
    "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
    "            up = nn.Module()\n",
    "            up.block = block\n",
    "            up.attn = attn\n",
    "            if i_level != 0:\n",
    "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
    "                curr_res = curr_res * 2\n",
    "            self.up.insert(0, up) # prepend to get consistent order\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_ch,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        #assert z.shape[1:] == self.z_shape[1:]\n",
    "        self.last_z_shape = z.shape\n",
    "\n",
    "        # timestep embedding\n",
    "        temb = None\n",
    "\n",
    "        # z to block_in\n",
    "        h = self.conv_in(z)\n",
    "\n",
    "        # middle\n",
    "        h = self.mid.block_1(h, temb)\n",
    "        h = self.mid.attn_1(h)\n",
    "        h = self.mid.block_2(h, temb)\n",
    "\n",
    "        # upsampling\n",
    "        for i_level in reversed(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks+1):\n",
    "                h = self.up[i_level].block[i_block](h, temb)\n",
    "                if len(self.up[i_level].attn) > 0:\n",
    "                    h = self.up[i_level].attn[i_block](h)\n",
    "            if i_level != 0:\n",
    "                h = self.up[i_level].upsample(h)\n",
    "\n",
    "        # end\n",
    "        if self.give_pre_end:\n",
    "            return h\n",
    "\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        if self.tanh_out:\n",
    "            h = torch.tanh(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
    "                                     ResnetBlock(in_channels=in_channels,\n",
    "                                                 out_channels=2 * in_channels,\n",
    "                                                 temb_channels=0, dropout=0.0),\n",
    "                                     ResnetBlock(in_channels=2 * in_channels,\n",
    "                                                out_channels=4 * in_channels,\n",
    "                                                temb_channels=0, dropout=0.0),\n",
    "                                     ResnetBlock(in_channels=4 * in_channels,\n",
    "                                                out_channels=2 * in_channels,\n",
    "                                                temb_channels=0, dropout=0.0),\n",
    "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
    "                                     Upsample(in_channels, with_conv=True)])\n",
    "        # end\n",
    "        self.norm_out = Normalize(in_channels)\n",
    "        self.conv_out = torch.nn.Conv2d(in_channels,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.model):\n",
    "            if i in [1,2,3]:\n",
    "                x = layer(x, None)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        h = self.norm_out(x)\n",
    "        h = nonlinearity(h)\n",
    "        x = self.conv_out(h)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpsampleDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
    "                 ch_mult=(2,2), dropout=0.0):\n",
    "        super().__init__()\n",
    "        # upsampling\n",
    "        self.temb_ch = 0\n",
    "        self.num_resolutions = len(ch_mult)\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        block_in = in_channels\n",
    "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        self.upsample_blocks = nn.ModuleList()\n",
    "        for i_level in range(self.num_resolutions):\n",
    "            res_block = []\n",
    "            block_out = ch * ch_mult[i_level]\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                res_block.append(ResnetBlock(in_channels=block_in,\n",
    "                                         out_channels=block_out,\n",
    "                                         temb_channels=self.temb_ch,\n",
    "                                         dropout=dropout))\n",
    "                block_in = block_out\n",
    "            self.res_blocks.append(nn.ModuleList(res_block))\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                self.upsample_blocks.append(Upsample(block_in, True))\n",
    "                curr_res = curr_res * 2\n",
    "\n",
    "        # end\n",
    "        self.norm_out = Normalize(block_in)\n",
    "        self.conv_out = torch.nn.Conv2d(block_in,\n",
    "                                        out_channels,\n",
    "                                        kernel_size=3,\n",
    "                                        stride=1,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # upsampling\n",
    "        h = x\n",
    "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
    "            for i_block in range(self.num_res_blocks + 1):\n",
    "                h = self.res_blocks[i_level][i_block](h, None)\n",
    "            if i_level != self.num_resolutions - 1:\n",
    "                h = self.upsample_blocks[k](h)\n",
    "        h = self.norm_out(h)\n",
    "        h = nonlinearity(h)\n",
    "        h = self.conv_out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class LatentRescaler(nn.Module):\n",
    "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
    "        super().__init__()\n",
    "        # residual block, interpolate, residual block\n",
    "        self.factor = factor\n",
    "        self.conv_in = nn.Conv2d(in_channels,\n",
    "                                 mid_channels,\n",
    "                                 kernel_size=3,\n",
    "                                 stride=1,\n",
    "                                 padding=1)\n",
    "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
    "                                                     out_channels=mid_channels,\n",
    "                                                     temb_channels=0,\n",
    "                                                     dropout=0.0) for _ in range(depth)])\n",
    "        self.attn = AttnBlock(mid_channels)\n",
    "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
    "                                                     out_channels=mid_channels,\n",
    "                                                     temb_channels=0,\n",
    "                                                     dropout=0.0) for _ in range(depth)])\n",
    "\n",
    "        self.conv_out = nn.Conv2d(mid_channels,\n",
    "                                  out_channels,\n",
    "                                  kernel_size=1,\n",
    "                                  )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        for block in self.res_block1:\n",
    "            x = block(x, None)\n",
    "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
    "        x = self.attn(x)\n",
    "        for block in self.res_block2:\n",
    "            x = block(x, None)\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MergedRescaleEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
    "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
    "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
    "        super().__init__()\n",
    "        intermediate_chn = ch * ch_mult[-1]\n",
    "        self.encoder = DiffusionEncoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
    "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
    "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
    "                               out_ch=None)\n",
    "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
    "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.rescaler(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MergedRescaleDecoder(nn.Module):\n",
    "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
    "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
    "        super().__init__()\n",
    "        tmp_chn = z_channels*ch_mult[-1]\n",
    "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
    "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
    "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
    "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
    "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rescaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsampler(nn.Module):\n",
    "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
    "        super().__init__()\n",
    "        assert out_size >= in_size\n",
    "        num_blocks = int(np.log2(out_size//in_size))+1\n",
    "        factor_up = 1.+ (out_size % in_size)\n",
    "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
    "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
    "                                       out_channels=in_channels)\n",
    "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
    "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
    "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rescaler(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Resize(nn.Module):\n",
    "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
    "        super().__init__()\n",
    "        self.with_conv = learned\n",
    "        self.mode = mode\n",
    "        if self.with_conv:\n",
    "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
    "            raise NotImplementedError()\n",
    "            assert in_channels is not None\n",
    "            # no asymmetric padding in torch conv, must do it ourselves\n",
    "            self.conv = torch.nn.Conv2d(in_channels,\n",
    "                                        in_channels,\n",
    "                                        kernel_size=4,\n",
    "                                        stride=2,\n",
    "                                        padding=1)\n",
    "\n",
    "    def forward(self, x, scale_factor=1.0):\n",
    "        if scale_factor==1.0:\n",
    "            return x\n",
    "        else:\n",
    "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
    "        return x\n",
    "\n",
    "class FirstStagePostProcessor(nn.Module):\n",
    "\n",
    "    def __init__(self, ch_mult:list, in_channels,\n",
    "                 pretrained_model:nn.Module=None,\n",
    "                 reshape=False,\n",
    "                 n_channels=None,\n",
    "                 dropout=0.,\n",
    "                 pretrained_config=None):\n",
    "        super().__init__()\n",
    "        if pretrained_config is None:\n",
    "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
    "            self.pretrained_model = pretrained_model\n",
    "        else:\n",
    "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
    "            self.instantiate_pretrained(pretrained_config)\n",
    "\n",
    "        self.do_reshape = reshape\n",
    "\n",
    "        if n_channels is None:\n",
    "            n_channels = self.pretrained_model.encoder.ch\n",
    "\n",
    "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
    "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3,\n",
    "                            stride=1,padding=1)\n",
    "\n",
    "        blocks = []\n",
    "        downs = []\n",
    "        ch_in = n_channels\n",
    "        for m in ch_mult:\n",
    "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
    "            ch_in = m * n_channels\n",
    "            downs.append(Downsample(ch_in, with_conv=False))\n",
    "\n",
    "        self.model = nn.ModuleList(blocks)\n",
    "        self.downsampler = nn.ModuleList(downs)\n",
    "\n",
    "\n",
    "    def instantiate_pretrained(self, config):\n",
    "        model = instantiate_from_config(config)\n",
    "        self.pretrained_model = model.eval()\n",
    "        # self.pretrained_model.train = False\n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_with_pretrained(self,x):\n",
    "        c = self.pretrained_model.encode(x)\n",
    "        if isinstance(c, DiagonalGaussianDistribution):\n",
    "            c = c.mode()\n",
    "        return  c\n",
    "\n",
    "    def forward(self,x):\n",
    "        z_fs = self.encode_with_pretrained(x)\n",
    "        z = self.proj_norm(z_fs)\n",
    "        z = self.proj(z)\n",
    "        z = nonlinearity(z)\n",
    "\n",
    "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
    "            z = submodel(z,temb=None)\n",
    "            z = downmodel(z)\n",
    "\n",
    "        if self.do_reshape:\n",
    "            z = rearrange(z,'b c h w -> b (h w) c')\n",
    "        return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoders modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 499,
     "status": "ok",
     "timestamp": 1701288254374,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "ep9FsxRD_tv6",
    "outputId": "c622072f-8799-4930-e185-5a28654763ca",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.0226, -0.0035,  0.0202],\n",
      "        [-0.0226,  1.0000, -0.0069, -0.0411],\n",
      "        [-0.0035, -0.0069,  1.0000,  0.0068],\n",
      "        [ 0.0202, -0.0411,  0.0068,  1.0000]])\n",
      "tensor([[-0.7427, -1.7541, -1.7465, -1.7212],\n",
      "        [-1.7653, -0.7314, -1.7499, -1.7825],\n",
      "        [-1.7462, -1.7383, -0.7431, -1.7346],\n",
      "        [-1.7225, -1.7726, -1.7363, -0.7414]])\n",
      "tensor([-0.7427, -0.7314, -0.7431, -0.7414])\n",
      "tensor(0.7396)\n",
      "tensor([[-0.7427, -1.7653, -1.7462, -1.7225],\n",
      "        [-1.7541, -0.7314, -1.7383, -1.7726],\n",
      "        [-1.7465, -1.7499, -0.7431, -1.7363],\n",
      "        [-1.7212, -1.7825, -1.7346, -0.7414]])\n",
      "tensor([-0.7427, -0.7314, -0.7431, -0.7414])\n",
      "tensor(0.7396)\n",
      "tensor(0.7396)\n"
     ]
    }
   ],
   "source": [
    "#@title Encoders modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "# import clip\n",
    "import sys\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from transformers import CLIPTokenizer, CLIPTextModel, AutoProcessor, CLIPVisionModel, CLIPVisionModelWithProjection\n",
    "import kornia\n",
    "\n",
    "class AbstractEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "class ClassEmbedder(nn.Module):\n",
    "    def __init__(self, embed_dim, n_classes=1000, key='class'):\n",
    "        super().__init__()\n",
    "        self.key = key\n",
    "        self.embedding = nn.Embedding(n_classes, embed_dim)\n",
    "\n",
    "    def forward(self, batch, key=None):\n",
    "        if key is None:\n",
    "            key = self.key\n",
    "        # this is for use in crossattn\n",
    "        c = batch[key][:, None]\n",
    "        c = self.embedding(c)\n",
    "        return c\n",
    "\n",
    "\n",
    "class TransformerEmbedder(AbstractEncoder):\n",
    "    \"\"\"Some transformer encoder layers\"\"\"\n",
    "    def __init__(self, n_embed, n_layer, vocab_size, max_seq_len=77, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,\n",
    "                                              attn_layers=Encoder(dim=n_embed, depth=n_layer))\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        tokens = tokens.to(self.device)  # meh\n",
    "        z = self.transformer(tokens, return_embeddings=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self(x)\n",
    "\n",
    "\n",
    "class BERTTokenizer(AbstractEncoder):\n",
    "    \"\"\" Uses a pretrained BERT tokenizer by huggingface. Vocab size: 30522 (?)\"\"\"\n",
    "    def __init__(self, device=\"cuda\", vq_interface=True, max_length=77):\n",
    "        super().__init__()\n",
    "        from transformers import BertTokenizerFast  # TODO: add to reuquirements\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "        self.device = device\n",
    "        self.vq_interface = vq_interface\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, text):\n",
    "        batch_encoding = self.tokenizer(text, truncation=True, max_length=self.max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        return tokens\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, text):\n",
    "        tokens = self(text)\n",
    "        if not self.vq_interface:\n",
    "            return tokens\n",
    "        return None, None, [None, None, tokens]\n",
    "\n",
    "    def decode(self, text):\n",
    "        return text\n",
    "\n",
    "\n",
    "class BERTEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the BERT tokenizr model and add some transformer encoder layers\"\"\"\n",
    "    def __init__(self, n_embed, n_layer, vocab_size=30522, max_seq_len=77,\n",
    "                 device=\"cuda\",use_tokenizer=True, embedding_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.use_tknz_fn = use_tokenizer\n",
    "        if self.use_tknz_fn:\n",
    "            self.tknz_fn = BERTTokenizer(vq_interface=False, max_length=max_seq_len)\n",
    "        self.device = device\n",
    "        self.transformer = TransformerWrapper(num_tokens=vocab_size, max_seq_len=max_seq_len,\n",
    "                                              attn_layers=Encoder(dim=n_embed, depth=n_layer),\n",
    "                                              emb_dropout=embedding_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        if self.use_tknz_fn:\n",
    "            tokens = self.tknz_fn(text)#.to(self.device)\n",
    "        else:\n",
    "            tokens = text\n",
    "        z = self.transformer(tokens, return_embeddings=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        # output of length 77\n",
    "        return self(text)\n",
    "\n",
    "\n",
    "class SpatialRescaler(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_stages=1,\n",
    "                 method='bilinear',\n",
    "                 multiplier=0.5,\n",
    "                 in_channels=3,\n",
    "                 out_channels=None,\n",
    "                 bias=False):\n",
    "        super().__init__()\n",
    "        self.n_stages = n_stages\n",
    "        assert self.n_stages >= 0\n",
    "        assert method in ['nearest','linear','bilinear','trilinear','bicubic','area']\n",
    "        self.multiplier = multiplier\n",
    "        self.interpolator = partial(torch.nn.functional.interpolate, mode=method)\n",
    "        self.remap_output = out_channels is not None\n",
    "        if self.remap_output:\n",
    "            print(f'Spatial Rescaler mapping from {in_channels} to {out_channels} channels after resizing.')\n",
    "            self.channel_mapper = nn.Conv2d(in_channels,out_channels,1,bias=bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        for stage in range(self.n_stages):\n",
    "            x = self.interpolator(x, scale_factor=self.multiplier)\n",
    "\n",
    "\n",
    "        if self.remap_output:\n",
    "            x = self.channel_mapper(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self(x)\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "class FrozenCLIPEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(version)\n",
    "        self.transformer = CLIPTextModel.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def synonym_replacement(self, text, replacement_prob=0.1):\n",
    "        words = text.split()\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if random.random() < replacement_prob:\n",
    "                synonyms = [syn.lemmas()[0].name() for syn in wordnet.synsets(word)]\n",
    "                if synonyms:\n",
    "                    new_words.append(random.choice(synonyms))\n",
    "                else:\n",
    "                    new_words.append(word)\n",
    "            else:\n",
    "                new_words.append(word)\n",
    "        return ' '.join(new_words)\n",
    "\n",
    "    def forward(self, text):\n",
    "        augmented_text = self.synonym_replacement(text)\n",
    "        dynamic_max_length = min(len(augmented_text.split()), self.max_length)\n",
    "        batch_encoding = self.tokenizer(augmented_text, truncation=True, max_length=dynamic_max_length, return_length=True,\n",
    "                                        return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        tokens = batch_encoding[\"input_ids\"].to(self.device)\n",
    "        outputs = self.transformer(input_ids=tokens)\n",
    "\n",
    "        z = outputs.last_hidden_state\n",
    "\n",
    "        # Adding noise to the embeddings\n",
    "        noise = torch.randn_like(z) * 0.01  # Noise scale can be adjusted\n",
    "        z = z + noise\n",
    "\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self(text)\n",
    "\n",
    "class FrozenImageEmbedder(AbstractEncoder):\n",
    "    \"\"\"Uses the CLIP transformer encoder for text (from Hugging Face)\"\"\"\n",
    "    def __init__(self, version=\"openai/clip-vit-large-patch14\", device=\"cuda\", max_length=77):\n",
    "        super().__init__()\n",
    "        # self.processor = AutoProcessor.from_pretrained(version)\n",
    "        self.transformer = CLIPVisionModelWithProjection.from_pretrained(version)\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.freeze()\n",
    "\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.transformer = self.transformer.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # image = Image.open(requests.get(url, stream=True).raw)\n",
    "        # inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "        outputs = self.transformer(**inputs)\n",
    "        image_embeds = outputs.image_embeds\n",
    "        return image_embeds\n",
    "        # z = outputs.last_hidden_state\n",
    "\n",
    "        # return z\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        return self(inputs)\n",
    "\n",
    "\n",
    "class FrozenCLIPTextEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Uses the CLIP transformer encoder for text.\n",
    "    \"\"\"\n",
    "    def __init__(self, version='ViT-L/14', device=\"cuda\", max_length=77, n_repeat=1, normalize=True):\n",
    "        super().__init__()\n",
    "        self.model, _ = clip.load(version, jit=False, device=\"cpu\")\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.n_repeat = n_repeat\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def freeze(self):\n",
    "        self.model = self.model.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = clip.tokenize(text).to(self.device)\n",
    "        z = self.model.encode_text(tokens)\n",
    "        if self.normalize:\n",
    "            z = z / torch.linalg.norm(z, dim=1, keepdim=True)\n",
    "        return z\n",
    "\n",
    "    def encode(self, text):\n",
    "        z = self(text)\n",
    "        if z.ndim==2:\n",
    "            z = z[:, None, :]\n",
    "        z = repeat(z, 'b 1 d -> b k d', k=self.n_repeat)\n",
    "        return z\n",
    "\n",
    "\n",
    "class FrozenClipImageEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "        Uses the CLIP image encoder.\n",
    "        \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            model = 'ViT-L/14',\n",
    "            jit=False,\n",
    "            device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "            antialias=False,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.model, _ = clip.load(name=model, device=device, jit=jit)\n",
    "\n",
    "        self.antialias = antialias\n",
    "\n",
    "        self.register_buffer('mean', torch.Tensor([0.48145466, 0.4578275, 0.40821073]), persistent=False)\n",
    "        self.register_buffer('std', torch.Tensor([0.26862954, 0.26130258, 0.27577711]), persistent=False)\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        # normalize to [0,1]\n",
    "        x = kornia.geometry.resize(x, (224, 224),\n",
    "                                   interpolation='bicubic',align_corners=True,\n",
    "                                   antialias=self.antialias)\n",
    "        x = (x + 1.) / 2.\n",
    "        # renormalize according to clip\n",
    "        x = kornia.enhance.normalize(x, self.mean, self.std)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is assumed to be in range [-1,1]\n",
    "        return self.model.encode_image(self.preprocess(x))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from dc_ldm.util import count_params\n",
    "    # text_model = FrozenCLIPEmbedder()\n",
    "    # text = ['a dog']\n",
    "    # text_out = text_model(text)\n",
    "    # print(text_out.shape)\n",
    "    # FrozenCLIPEmbedder\n",
    "\n",
    "#     def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n",
    "#         return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n",
    "\n",
    "\n",
    "#     def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "#         caption_loss = contrastive_loss(similarity)\n",
    "#         image_loss = contrastive_loss(similarity.t())\n",
    "#         return (caption_loss + image_loss) / 2.0\n",
    "\n",
    "#     input = Image.open('../dreamdiffusion/datasets/imageNet_images/n02106662/n02106662_1451.JPEG')\n",
    "\n",
    "#     from transformers import AutoProcessor, CLIPModel\n",
    "\n",
    "#     model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "#     processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# # url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# # image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "#     inputs = processor(\n",
    "#         text=[\"a photo of a cat\", \"a photo of a dog\"], images=input, return_tensors=\"pt\", padding=True\n",
    "#     )\n",
    "    def contrastive_loss(logits, dim):\n",
    "        m = nn.functional.log_softmax(logits, dim=dim)\n",
    "        print(m)\n",
    "        neg_ce = torch.diag(m)\n",
    "        print(neg_ce)\n",
    "        print(-neg_ce.mean())\n",
    "        return -neg_ce.mean()\n",
    "\n",
    "    def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
    "        caption_loss = contrastive_loss(similarity, dim=0)\n",
    "        image_loss = contrastive_loss(similarity, dim=1)\n",
    "        return (caption_loss + image_loss) / 2.0\n",
    "#     outputs = model(**inputs)\n",
    "#     logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "#     probs = logits_per_image.softmax(dim=1)\n",
    "#     print(probs)\n",
    "#     print(outputs.text_embeds.shape)\n",
    "#     print(outputs.image_embeds.shape)\n",
    "#     f = torch.cosine_similarity(outputs.text_embeds, outputs.image_embeds, dim=-1)\n",
    "#     print(f)\n",
    "#     print(model.logit_scale.exp())\n",
    "# # logits_per_text\n",
    "#     logits_per_text = torch.matmul(outputs.text_embeds, outputs.image_embeds.t()) * model.logit_scale.exp()\n",
    "#     logits_per_image = logits_per_text.t()\n",
    "#     print(logits_per_text)\n",
    "#     print(logits_per_image)\n",
    "#     print(clip_loss(logits_per_text))\n",
    "    z_i = torch.randn(4, 768)\n",
    "    z_j = z_i\n",
    "    # representations = torch.cat([z_i, z_j], dim=0)          # repre: (2*bs, dim)\n",
    "    # print(representations.shape)\n",
    "    # print(representations.unsqueeze(1).shape)\n",
    "    # print(representations.unsqueeze(0).shape)\n",
    "    similarity_matrix = nn.functional.cosine_similarity(z_i.unsqueeze(1), z_j.unsqueeze(0), dim=2)\n",
    "    print(similarity_matrix)\n",
    "    print(clip_loss(similarity_matrix))\n",
    "\n",
    "    # model = FrozenImageEmbedder()\n",
    "    # # out = model(input)\n",
    "    # # print(out.shape)\n",
    "\n",
    "    # # model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "\n",
    "\n",
    "    # # input = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    # inputs = processor(images=input, return_tensors=\"pt\")\n",
    "    # # for k, v in inputs.items():\n",
    "    # #     print(k)\n",
    "    # #     print(v.shape)\n",
    "    # # print()\n",
    "    # # print(inputs)\n",
    "\n",
    "    # outputs = model(inputs)\n",
    "    # # image_embeds = outputs.image_embeds\n",
    "    # print(outputs.shape)\n",
    "\n",
    "\n",
    "    # from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "    # model_text = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "    # inputs_text = tokenizer([\"a dog\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # outputs_text = model_text(**inputs_text)\n",
    "    # text_embeds = outputs_text.text_embeds\n",
    "    # f = torch.cosine_similarity(outputs, text_embeds, dim=-1)\n",
    "    # print(f)\n",
    "\n",
    "    # image_embeds = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "    # text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "    #     # cosine similarity as logits\n",
    "    # logit_scale = torch.tensor([2.6592]).exp()\n",
    "    # logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale\n",
    "    # print(logits_per_text)\n",
    "    # logits_per_image = logits_per_text.t()\n",
    "    # print(logits_per_image)\n",
    "\n",
    "\n",
    "\n",
    "    # print(outputs)\n",
    "    # count_params(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils for the main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1701288255780,
     "user": {
      "displayName": "Alin Dumitru",
      "userId": "05156977386176288841"
     },
     "user_tz": -60
    },
    "id": "oZz8wx7fFu6C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Utils for the main part\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def create_trainer(num_epoch, precision=32, accumulate_grad_batches=2,logger=None,check_val_every_n_epoch=10):\n",
    "    acc = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    return pl.Trainer(accelerator=acc, max_epochs=num_epoch, logger=logger,\n",
    "            precision=precision, accumulate_grad_batches=accumulate_grad_batches,\n",
    "            enable_checkpointing=False, enable_model_summary=False, gradient_clip_val=0.5,\n",
    "            check_val_every_n_epoch=check_val_every_n_epoch, limit_val_batches=0.15, limit_test_batches=0.15, limit_predict_batches=0.5)\n",
    "\n",
    "def normalize(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "class random_crop:\n",
    "    def __init__(self, size, p):\n",
    "        self.size = size\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.RandomCrop(size=(self.size, self.size))(img)\n",
    "        return img\n",
    "\n",
    "def channel_last(img):\n",
    "        if img.shape[-1] == 3:\n",
    "            return img\n",
    "        return rearrange(img, 'c h w -> h w c')\n",
    "\n",
    "def get_eval_metric(samples, avg=True):\n",
    "    metric_list = ['mse', 'pcc', 'ssim', 'psm']\n",
    "    res_list = []\n",
    "\n",
    "    gt_images = [img[0] for img in samples]\n",
    "    gt_images = rearrange(np.stack(gt_images), 'n c h w -> n h w c')\n",
    "    samples_to_run = np.arange(1, len(samples[0])) if avg else [1]\n",
    "    for m in metric_list:\n",
    "        res_part = []\n",
    "        for s in samples_to_run:\n",
    "            pred_images = [img[s] for img in samples]\n",
    "            pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "            res = get_similarity_metric(pred_images, gt_images, method='pair-wise', metric_name=m)\n",
    "            res_part.append(np.mean(res))\n",
    "        res_list.append(np.mean(res_part))\n",
    "    # No class metric for now\n",
    "    # res_part = []\n",
    "    # for s in samples_to_run:\n",
    "    #     pred_images = [img[s] for img in samples]\n",
    "    #     pred_images = rearrange(np.stack(pred_images), 'n c h w -> n h w c')\n",
    "    #     res = get_similarity_metric(pred_images, gt_images, 'class', None,\n",
    "    #                     n_way=50, num_trials=50, top_k=1, device='cuda')\n",
    "    #     res_part.append(np.mean(res))\n",
    "    # res_list.append(np.mean(res_part))\n",
    "    # res_list.append(np.max(res_part))\n",
    "    # metric_list.append('top-1-class')\n",
    "    # metric_list.append('top-1-class (max)')\n",
    "    return res_list, metric_list\n",
    "\n",
    "def generate_images(generative_model, eeg_latents_dataset_train, eeg_latents_dataset_test, config):\n",
    "    grid, _ = generative_model.generate(eeg_latents_dataset_train, config.num_samples,\n",
    "                config.ddim_steps, config.HW, 3) # generate 3\n",
    "    grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "    grid_imgs.save(os.path.join(config.output_path, 'samples_train.png'))\n",
    "    # wandb.log({'summary/samples_train': wandb.Image(grid_imgs)})\n",
    "\n",
    "    grid, samples = generative_model.generate(eeg_latents_dataset_test, config.num_samples,\n",
    "                config.ddim_steps, config.HW, 3)\n",
    "    grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "    grid_imgs.save(os.path.join(config.output_path,f'./samples_test.png'))\n",
    "    for sp_idx, imgs in enumerate(samples):\n",
    "        for copy_idx, img in enumerate(imgs[1:]):\n",
    "            img = rearrange(img, 'c h w -> h w c')\n",
    "            Image.fromarray(img).save(os.path.join(config.output_path,\n",
    "                            f'./test{sp_idx}-{copy_idx}.png'))\n",
    "\n",
    "    # wandb.log({f'summary/samples_test': wandb.Image(grid_imgs)})\n",
    "\n",
    "    metric, metric_list = get_eval_metric(samples, avg=config.eval_avg)\n",
    "    metric_dict = {f'summary/pair-wise_{k}':v for k, v in zip(metric_list[:-2], metric[:-2])}\n",
    "    metric_dict[f'summary/{metric_list[-2]}'] = metric[-2]\n",
    "    metric_dict[f'summary/{metric_list[-1]}'] = metric[-1]\n",
    "    # wandb.log(metric_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion\n",
      "Loss Type is: l2\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 860.51 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### Stage One: only optimize conditional encoders #####\n",
      "batch_size is: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion: Only optimizing conditioner params!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/optim/adamw.py:91: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(AdamW, self).__init__(params, defaults)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007702350616455078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.047661781311035156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58fa8219f35437aa91de1fab6de3ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss end, here are the losses, loss: 0.34073332558956837, clip_loss: 0.17027908130999533\n",
      "loss end, here are the losses, loss: 0.27609749850366383, clip_loss: 0.11780399300398366\n",
      "loss end, here are the losses, loss: 0.26228471040245027, clip_loss: 0.09618528403582112\n",
      "loss end, here are the losses, loss: 0.24341441600793792, clip_loss: 0.07942668276448403\n",
      "loss end, here are the losses, loss: 0.23378061728491897, clip_loss: 0.06606034309633317\n",
      "loss end, here are the losses, loss: 0.22126789928804483, clip_loss: 0.054906687188533046\n",
      "loss end, here are the losses, loss: 0.21268874250592723, clip_loss: 0.04544160779445402\n",
      "loss end, here are the losses, loss: 0.19942858031079655, clip_loss: 0.03649770901087792\n",
      "loss end, here are the losses, loss: 0.19214485542127682, clip_loss: 0.02881723185700755\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005757331848144531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "###### run full validation! ######\n",
      "\n",
      "rendering 3 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PLMS Sampler:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   0%|          | 1/250 [00:00<01:44,  2.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 2/250 [00:00<01:13,  3.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 3/250 [00:00<01:03,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 4/250 [00:01<00:58,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 5/250 [00:01<00:56,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 6/250 [00:01<00:54,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 7/250 [00:01<00:53,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 8/250 [00:01<00:52,  4.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 9/250 [00:02<00:51,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 10/250 [00:02<00:51,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 11/250 [00:02<00:50,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 12/250 [00:02<00:50,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 13/250 [00:02<00:50,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 14/250 [00:03<00:50,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 15/250 [00:03<00:50,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 16/250 [00:03<00:49,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 17/250 [00:03<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 18/250 [00:04<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 19/250 [00:04<00:48,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 20/250 [00:04<00:48,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 21/250 [00:04<00:48,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 22/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 23/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 24/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 25/250 [00:05<00:47,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 26/250 [00:05<00:47,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 27/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 28/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 29/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 30/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 31/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 32/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 33/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 34/250 [00:07<00:45,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 35/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 36/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 37/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 38/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 39/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 40/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 41/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 42/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 43/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 44/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 45/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 46/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 47/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 48/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 49/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 50/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 51/250 [00:10<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 52/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 53/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 54/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 55/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 56/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 57/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 58/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 59/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 60/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 61/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 62/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 63/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 64/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 65/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 66/250 [00:14<00:38,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 67/250 [00:14<00:38,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 68/250 [00:14<00:38,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 69/250 [00:14<00:38,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 70/250 [00:14<00:37,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 71/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 72/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 73/250 [00:15<00:37,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 74/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 75/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 76/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 77/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 78/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 79/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 80/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 81/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 82/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 83/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 84/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 85/250 [00:18<00:34,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 86/250 [00:18<00:34,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 87/250 [00:18<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 88/250 [00:18<00:34,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 89/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 90/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 91/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 92/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 93/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 94/250 [00:20<00:32,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 95/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 96/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 97/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 98/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 99/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 100/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 101/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 102/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 103/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 104/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 105/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 106/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 107/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 108/250 [00:23<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 109/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 110/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 111/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 112/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 113/250 [00:24<00:29,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 114/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 115/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 116/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 117/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 118/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 119/250 [00:25<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 120/250 [00:25<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 121/250 [00:25<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 122/250 [00:25<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 123/250 [00:26<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 124/250 [00:26<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 125/250 [00:26<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 126/250 [00:26<00:26,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 127/250 [00:27<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 128/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 129/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 130/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 131/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 132/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 133/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 134/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 135/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 136/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 137/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 138/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 139/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 140/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 141/250 [00:29<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 142/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 143/250 [00:30<00:22,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 144/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 145/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 146/250 [00:31<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 147/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 148/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 149/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 150/250 [00:31<00:21,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 151/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 152/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 153/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 154/250 [00:32<00:20,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 155/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 156/250 [00:33<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 157/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 158/250 [00:33<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 159/250 [00:33<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 160/250 [00:34<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 161/250 [00:34<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 162/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 163/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 164/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 165/250 [00:35<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 166/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 167/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 168/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 169/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 170/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 171/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 172/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 173/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 174/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 175/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 176/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 177/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 178/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 179/250 [00:38<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 180/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 181/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 182/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 183/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 184/250 [00:39<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 185/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 186/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 187/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 188/250 [00:39<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 189/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 190/250 [00:40<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 191/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 192/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 193/250 [00:41<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 194/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 195/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 196/250 [00:41<00:11,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 197/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 198/250 [00:42<00:11,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 199/250 [00:42<00:10,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 200/250 [00:42<00:10,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 201/250 [00:42<00:10,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 202/250 [00:42<00:10,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 203/250 [00:43<00:10,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 204/250 [00:43<00:09,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 205/250 [00:43<00:09,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 206/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 207/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 208/250 [00:44<00:08,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 209/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 210/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 211/250 [00:44<00:08,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 212/250 [00:45<00:08,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 213/250 [00:45<00:07,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 214/250 [00:45<00:07,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 215/250 [00:45<00:07,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 216/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 217/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 218/250 [00:46<00:06,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 219/250 [00:46<00:06,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 220/250 [00:46<00:06,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 221/250 [00:46<00:06,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 222/250 [00:47<00:05,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 223/250 [00:47<00:05,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 224/250 [00:47<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 225/250 [00:47<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 226/250 [00:48<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 227/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 228/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 229/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 230/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 231/250 [00:49<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 232/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 233/250 [00:49<00:03,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 234/250 [00:49<00:03,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 235/250 [00:49<00:03,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 236/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 237/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 238/250 [00:50<00:02,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 239/250 [00:50<00:02,  4.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 240/250 [00:51<00:02,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 241/250 [00:51<00:01,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 242/250 [00:51<00:01,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 243/250 [00:51<00:01,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 244/250 [00:51<00:01,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 245/250 [00:52<00:01,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 246/250 [00:52<00:00,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 247/250 [00:52<00:00,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 248/250 [00:52<00:00,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 249/250 [00:52<00:00,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 250/250 [00:53<00:00,  4.71it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PLMS Sampler:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   0%|          | 1/250 [00:00<01:43,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 2/250 [00:00<01:13,  3.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 3/250 [00:00<01:03,  3.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 4/250 [00:01<00:58,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 5/250 [00:01<00:56,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 6/250 [00:01<00:54,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 7/250 [00:01<00:53,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 8/250 [00:01<00:52,  4.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 9/250 [00:02<00:51,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 10/250 [00:02<00:51,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 11/250 [00:02<00:50,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 12/250 [00:02<00:50,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 13/250 [00:02<00:50,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 14/250 [00:03<00:50,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 15/250 [00:03<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 16/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 17/250 [00:03<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 18/250 [00:04<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 19/250 [00:04<00:48,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 20/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 21/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 22/250 [00:04<00:48,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 23/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 24/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 25/250 [00:05<00:47,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 26/250 [00:05<00:47,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 27/250 [00:05<00:47,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 28/250 [00:06<00:47,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 29/250 [00:06<00:46,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 30/250 [00:06<00:46,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 31/250 [00:06<00:46,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 32/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 33/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 34/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 35/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 36/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 37/250 [00:08<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 38/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 39/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 40/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 41/250 [00:08<00:44,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 42/250 [00:09<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 43/250 [00:09<00:43,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 44/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 45/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 46/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 47/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 48/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 49/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 50/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 51/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 52/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 53/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 54/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 55/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 56/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 57/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 58/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 59/250 [00:12<00:40,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 60/250 [00:12<00:40,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 61/250 [00:13<00:40,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 62/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 63/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 64/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 65/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 66/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 67/250 [00:14<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 68/250 [00:14<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 69/250 [00:14<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 70/250 [00:15<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 71/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 72/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 73/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 74/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 75/250 [00:16<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 76/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 77/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 78/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 79/250 [00:16<00:36,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 80/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 81/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 82/250 [00:17<00:35,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 83/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 84/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 85/250 [00:18<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 86/250 [00:18<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 87/250 [00:18<00:34,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 88/250 [00:18<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 89/250 [00:19<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 90/250 [00:19<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 91/250 [00:19<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 92/250 [00:19<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 93/250 [00:19<00:33,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 94/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 95/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 96/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 97/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 98/250 [00:20<00:32,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 99/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 100/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 101/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 102/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 103/250 [00:21<00:31,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 104/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 105/250 [00:22<00:30,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 106/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 107/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 108/250 [00:23<00:29,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 109/250 [00:23<00:29,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 110/250 [00:23<00:29,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 111/250 [00:23<00:29,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 112/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 113/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 114/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 115/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 116/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 117/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 118/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 119/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 120/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 121/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 122/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 123/250 [00:26<00:26,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 124/250 [00:26<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 125/250 [00:26<00:26,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 126/250 [00:26<00:26,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 127/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 128/250 [00:27<00:25,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 129/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 130/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 131/250 [00:27<00:25,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 132/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 133/250 [00:28<00:24,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 134/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 135/250 [00:28<00:24,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 136/250 [00:28<00:24,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 137/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 138/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 139/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 140/250 [00:29<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 141/250 [00:30<00:23,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 142/250 [00:30<00:22,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 143/250 [00:30<00:22,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 144/250 [00:30<00:22,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 145/250 [00:30<00:22,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 146/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 147/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 148/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 149/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 150/250 [00:31<00:21,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 151/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 152/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 153/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 154/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 155/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 156/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 157/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 158/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 159/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 160/250 [00:34<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 161/250 [00:34<00:18,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 162/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 163/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 164/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 165/250 [00:35<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 166/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 167/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 168/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 169/250 [00:35<00:17,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 170/250 [00:36<00:16,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 171/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 172/250 [00:36<00:16,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 173/250 [00:36<00:16,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 174/250 [00:36<00:16,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 175/250 [00:37<00:15,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 176/250 [00:37<00:15,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 177/250 [00:37<00:15,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 178/250 [00:37<00:15,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 179/250 [00:38<00:15,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 180/250 [00:38<00:14,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 181/250 [00:38<00:14,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 182/250 [00:38<00:14,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 183/250 [00:38<00:14,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 184/250 [00:39<00:13,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 185/250 [00:39<00:13,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 186/250 [00:39<00:13,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 187/250 [00:39<00:13,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 188/250 [00:39<00:13,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 189/250 [00:40<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 190/250 [00:40<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 191/250 [00:40<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 192/250 [00:40<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 193/250 [00:41<00:12,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 194/250 [00:41<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 195/250 [00:41<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 196/250 [00:41<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 197/250 [00:41<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 198/250 [00:42<00:10,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 199/250 [00:42<00:10,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 200/250 [00:42<00:10,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 201/250 [00:42<00:10,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 202/250 [00:42<00:10,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 203/250 [00:43<00:09,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 204/250 [00:43<00:09,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 205/250 [00:43<00:09,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 206/250 [00:43<00:09,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 207/250 [00:43<00:09,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 208/250 [00:44<00:08,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 209/250 [00:44<00:08,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 210/250 [00:44<00:08,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 211/250 [00:44<00:08,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 212/250 [00:45<00:08,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 213/250 [00:45<00:07,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 214/250 [00:45<00:07,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 215/250 [00:45<00:07,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 216/250 [00:45<00:07,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 217/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 218/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 219/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 220/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 221/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 222/250 [00:47<00:05,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 223/250 [00:47<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 224/250 [00:47<00:05,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 225/250 [00:47<00:05,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 226/250 [00:47<00:05,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 227/250 [00:48<00:04,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 228/250 [00:48<00:04,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 229/250 [00:48<00:04,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 230/250 [00:48<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 231/250 [00:49<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 232/250 [00:49<00:03,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 233/250 [00:49<00:03,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 234/250 [00:49<00:03,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 235/250 [00:49<00:03,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 236/250 [00:50<00:02,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 237/250 [00:50<00:02,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 238/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 239/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 240/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 241/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 242/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 243/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 244/250 [00:51<00:01,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 245/250 [00:52<00:01,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 246/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 247/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 248/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 249/250 [00:52<00:00,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 250/250 [00:53<00:00,  4.71it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PLMS Sampler:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   0%|          | 1/250 [00:00<01:43,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 2/250 [00:00<01:13,  3.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 3/250 [00:00<01:03,  3.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 4/250 [00:01<00:58,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 5/250 [00:01<00:56,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 6/250 [00:01<00:54,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 7/250 [00:01<00:53,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 8/250 [00:01<00:52,  4.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 9/250 [00:02<00:51,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 10/250 [00:02<00:51,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 11/250 [00:02<00:50,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 12/250 [00:02<00:50,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 13/250 [00:02<00:50,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 14/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 15/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 16/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 17/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 18/250 [00:04<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 19/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 20/250 [00:04<00:48,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 21/250 [00:04<00:48,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 22/250 [00:04<00:48,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 23/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 24/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 25/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 26/250 [00:05<00:47,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 27/250 [00:05<00:47,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 28/250 [00:06<00:47,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 29/250 [00:06<00:46,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 30/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 31/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 32/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 33/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 34/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 35/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 36/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 37/250 [00:08<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 38/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 39/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 40/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 41/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 42/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 43/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 44/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 45/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 46/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 47/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 48/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 49/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 50/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 51/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 52/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 53/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 54/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 55/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 56/250 [00:12<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 57/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 58/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 59/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 60/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 61/250 [00:13<00:40,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 62/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 63/250 [00:13<00:39,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 64/250 [00:13<00:39,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 65/250 [00:13<00:39,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 66/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 67/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 68/250 [00:14<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 69/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 70/250 [00:15<00:38,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 71/250 [00:15<00:37,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 72/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 73/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 74/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 75/250 [00:16<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 76/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 77/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 78/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 79/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 80/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 81/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 82/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 83/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 84/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 85/250 [00:18<00:34,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 86/250 [00:18<00:34,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 87/250 [00:18<00:34,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 88/250 [00:18<00:34,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 89/250 [00:19<00:34,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 90/250 [00:19<00:34,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 91/250 [00:19<00:34,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 92/250 [00:19<00:33,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 93/250 [00:19<00:33,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 94/250 [00:20<00:33,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 95/250 [00:20<00:33,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 96/250 [00:20<00:32,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 97/250 [00:20<00:32,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 98/250 [00:20<00:32,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 99/250 [00:21<00:32,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 100/250 [00:21<00:32,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 101/250 [00:21<00:31,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 102/250 [00:21<00:31,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 103/250 [00:22<00:31,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 104/250 [00:22<00:31,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 105/250 [00:22<00:30,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 106/250 [00:22<00:30,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 107/250 [00:22<00:30,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 108/250 [00:23<00:30,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 109/250 [00:23<00:30,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 110/250 [00:23<00:29,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 111/250 [00:23<00:29,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 112/250 [00:23<00:29,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 113/250 [00:24<00:29,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 114/250 [00:24<00:29,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 115/250 [00:24<00:28,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 116/250 [00:24<00:28,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 117/250 [00:25<00:28,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 118/250 [00:25<00:28,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 119/250 [00:25<00:28,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 120/250 [00:25<00:27,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 121/250 [00:25<00:27,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 122/250 [00:26<00:27,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 123/250 [00:26<00:27,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 124/250 [00:26<00:26,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 125/250 [00:26<00:26,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 126/250 [00:26<00:26,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 127/250 [00:27<00:26,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 128/250 [00:27<00:25,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 129/250 [00:27<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 130/250 [00:27<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 131/250 [00:27<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 132/250 [00:28<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 133/250 [00:28<00:24,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 134/250 [00:28<00:24,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 135/250 [00:28<00:24,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 136/250 [00:29<00:24,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 137/250 [00:29<00:24,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 138/250 [00:29<00:23,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 139/250 [00:29<00:23,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 140/250 [00:29<00:23,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 141/250 [00:30<00:23,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 142/250 [00:30<00:22,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 143/250 [00:30<00:22,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 144/250 [00:30<00:22,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 145/250 [00:30<00:22,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 146/250 [00:31<00:22,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 147/250 [00:31<00:22,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 148/250 [00:31<00:21,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 149/250 [00:31<00:21,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 150/250 [00:32<00:21,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 151/250 [00:32<00:21,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 152/250 [00:32<00:20,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 153/250 [00:32<00:20,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 154/250 [00:32<00:20,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 155/250 [00:33<00:20,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 156/250 [00:33<00:19,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 157/250 [00:33<00:19,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 158/250 [00:33<00:19,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 159/250 [00:33<00:19,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 160/250 [00:34<00:19,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 161/250 [00:34<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 162/250 [00:34<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 163/250 [00:34<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 164/250 [00:35<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 165/250 [00:35<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 166/250 [00:35<00:17,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 167/250 [00:35<00:17,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 168/250 [00:35<00:17,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 169/250 [00:36<00:17,  4.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 170/250 [00:36<00:17,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 171/250 [00:36<00:16,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 172/250 [00:36<00:16,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 173/250 [00:36<00:16,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 174/250 [00:37<00:16,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 175/250 [00:37<00:15,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 176/250 [00:37<00:15,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 177/250 [00:37<00:15,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 178/250 [00:38<00:15,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 179/250 [00:38<00:15,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 180/250 [00:38<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 181/250 [00:38<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 182/250 [00:38<00:14,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 183/250 [00:39<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 184/250 [00:39<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 185/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 186/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 187/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 188/250 [00:40<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 189/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 190/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 191/250 [00:40<00:12,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 192/250 [00:40<00:12,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 193/250 [00:41<00:12,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 194/250 [00:41<00:11,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 195/250 [00:41<00:11,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 196/250 [00:41<00:11,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 197/250 [00:42<00:11,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 198/250 [00:42<00:11,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 199/250 [00:42<00:10,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 200/250 [00:42<00:10,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 201/250 [00:42<00:10,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 202/250 [00:43<00:10,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 203/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 204/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 205/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 206/250 [00:43<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 207/250 [00:44<00:09,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 208/250 [00:44<00:09,  4.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 209/250 [00:44<00:08,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 210/250 [00:44<00:08,  4.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 211/250 [00:45<00:08,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 212/250 [00:45<00:08,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 213/250 [00:45<00:07,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 214/250 [00:45<00:07,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 215/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 216/250 [00:46<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 217/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 218/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 219/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 220/250 [00:46<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 221/250 [00:47<00:06,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 222/250 [00:47<00:05,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 223/250 [00:47<00:05,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 224/250 [00:47<00:05,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 225/250 [00:47<00:05,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 226/250 [00:48<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 227/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 228/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 229/250 [00:48<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 230/250 [00:49<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 231/250 [00:49<00:04,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 232/250 [00:49<00:03,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 233/250 [00:49<00:03,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 234/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 235/250 [00:50<00:03,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 236/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 237/250 [00:50<00:02,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 238/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 239/250 [00:50<00:02,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 240/250 [00:51<00:02,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 241/250 [00:51<00:01,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 242/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 243/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 244/250 [00:52<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 245/250 [00:52<00:01,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 246/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 247/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 248/250 [00:52<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 249/250 [00:53<00:00,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 250/250 [00:53<00:00,  4.69it/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering 3 examples in 250 steps.\n",
      "Data shape for PLMS sampling is (3, 4, 64, 64)\n",
      "Running PLMS Sampling with 250 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "PLMS Sampler:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   0%|          | 1/250 [00:00<01:43,  2.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 2/250 [00:00<01:13,  3.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   1%|          | 3/250 [00:00<01:03,  3.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 4/250 [00:01<00:59,  4.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 5/250 [00:01<00:56,  4.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   2%|         | 6/250 [00:01<00:54,  4.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 7/250 [00:01<00:53,  4.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   3%|         | 8/250 [00:01<00:52,  4.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 9/250 [00:02<00:51,  4.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 10/250 [00:02<00:51,  4.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   4%|         | 11/250 [00:02<00:50,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 12/250 [00:02<00:50,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   5%|         | 13/250 [00:02<00:50,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 14/250 [00:03<00:50,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 15/250 [00:03<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   6%|         | 16/250 [00:03<00:49,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 17/250 [00:03<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   7%|         | 18/250 [00:04<00:49,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 19/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 20/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   8%|         | 21/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 22/250 [00:04<00:48,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:   9%|         | 23/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 24/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 25/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  10%|         | 26/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 27/250 [00:05<00:47,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  11%|         | 28/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 29/250 [00:06<00:46,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 30/250 [00:06<00:46,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  12%|        | 31/250 [00:06<00:46,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 32/250 [00:06<00:46,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  13%|        | 33/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 34/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 35/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  14%|        | 36/250 [00:07<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 37/250 [00:08<00:45,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  15%|        | 38/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 39/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 40/250 [00:08<00:44,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  16%|        | 41/250 [00:08<00:44,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 42/250 [00:09<00:43,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  17%|        | 43/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 44/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 45/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  18%|        | 46/250 [00:09<00:43,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 47/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  19%|        | 48/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 49/250 [00:10<00:42,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 50/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  20%|        | 51/250 [00:10<00:42,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 52/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  21%|        | 53/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 54/250 [00:11<00:41,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 55/250 [00:11<00:41,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  22%|       | 56/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 57/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  23%|       | 58/250 [00:12<00:40,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 59/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 60/250 [00:12<00:40,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  24%|       | 61/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 62/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  25%|       | 63/250 [00:13<00:39,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 64/250 [00:13<00:39,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 65/250 [00:13<00:39,  4.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  26%|       | 66/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 67/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  27%|       | 68/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 69/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 70/250 [00:14<00:38,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  28%|       | 71/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 72/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  29%|       | 73/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 74/250 [00:15<00:37,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 75/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  30%|       | 76/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 77/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  31%|       | 78/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 79/250 [00:16<00:36,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 80/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  32%|      | 81/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 82/250 [00:17<00:35,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  33%|      | 83/250 [00:17<00:35,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 84/250 [00:17<00:35,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 85/250 [00:18<00:34,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  34%|      | 86/250 [00:18<00:34,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 87/250 [00:18<00:34,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  35%|      | 88/250 [00:18<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 89/250 [00:19<00:34,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 90/250 [00:19<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  36%|      | 91/250 [00:19<00:33,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 92/250 [00:19<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  37%|      | 93/250 [00:19<00:33,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 94/250 [00:20<00:33,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 95/250 [00:20<00:32,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  38%|      | 96/250 [00:20<00:32,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 97/250 [00:20<00:32,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  39%|      | 98/250 [00:20<00:32,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 99/250 [00:21<00:31,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 100/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  40%|      | 101/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 102/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  41%|      | 103/250 [00:21<00:31,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 104/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 105/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  42%|     | 106/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 107/250 [00:22<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  43%|     | 108/250 [00:23<00:30,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 109/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 110/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  44%|     | 111/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 112/250 [00:23<00:29,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  45%|     | 113/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 114/250 [00:24<00:28,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 115/250 [00:24<00:28,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  46%|     | 116/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 117/250 [00:24<00:28,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  47%|     | 118/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 119/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 120/250 [00:25<00:27,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  48%|     | 121/250 [00:25<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 122/250 [00:26<00:27,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  49%|     | 123/250 [00:26<00:26,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 124/250 [00:26<00:26,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 125/250 [00:26<00:26,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  50%|     | 126/250 [00:26<00:26,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 127/250 [00:27<00:26,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  51%|     | 128/250 [00:27<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 129/250 [00:27<00:25,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 130/250 [00:27<00:25,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  52%|    | 131/250 [00:27<00:25,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 132/250 [00:28<00:25,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  53%|    | 133/250 [00:28<00:24,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 134/250 [00:28<00:24,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 135/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  54%|    | 136/250 [00:28<00:24,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 137/250 [00:29<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  55%|    | 138/250 [00:29<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 139/250 [00:29<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 140/250 [00:29<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  56%|    | 141/250 [00:30<00:23,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 142/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  57%|    | 143/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 144/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 145/250 [00:30<00:22,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  58%|    | 146/250 [00:31<00:22,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 147/250 [00:31<00:21,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  59%|    | 148/250 [00:31<00:21,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 149/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 150/250 [00:31<00:21,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  60%|    | 151/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 152/250 [00:32<00:20,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  61%|    | 153/250 [00:32<00:20,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 154/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 155/250 [00:32<00:20,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  62%|   | 156/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 157/250 [00:33<00:19,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  63%|   | 158/250 [00:33<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 159/250 [00:33<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 160/250 [00:34<00:19,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  64%|   | 161/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 162/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  65%|   | 163/250 [00:34<00:18,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 164/250 [00:34<00:18,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 165/250 [00:35<00:18,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  66%|   | 166/250 [00:35<00:17,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 167/250 [00:35<00:17,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  67%|   | 168/250 [00:35<00:17,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 169/250 [00:35<00:17,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 170/250 [00:36<00:16,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  68%|   | 171/250 [00:36<00:16,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 172/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  69%|   | 173/250 [00:36<00:16,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 174/250 [00:37<00:16,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 175/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  70%|   | 176/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 177/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  71%|   | 178/250 [00:37<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 179/250 [00:38<00:15,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 180/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  72%|  | 181/250 [00:38<00:14,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 182/250 [00:38<00:14,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  73%|  | 183/250 [00:38<00:14,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 184/250 [00:39<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 185/250 [00:39<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  74%|  | 186/250 [00:39<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 187/250 [00:39<00:13,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  75%|  | 188/250 [00:39<00:13,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 189/250 [00:40<00:12,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 190/250 [00:40<00:12,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  76%|  | 191/250 [00:40<00:12,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 192/250 [00:40<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  77%|  | 193/250 [00:41<00:12,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 194/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 195/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  78%|  | 196/250 [00:41<00:11,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 197/250 [00:41<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  79%|  | 198/250 [00:42<00:11,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 199/250 [00:42<00:10,  4.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 200/250 [00:42<00:10,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  80%|  | 201/250 [00:42<00:10,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 202/250 [00:42<00:10,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  81%|  | 203/250 [00:43<00:09,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 204/250 [00:43<00:09,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 205/250 [00:43<00:09,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  82%| | 206/250 [00:43<00:09,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 207/250 [00:44<00:09,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  83%| | 208/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 209/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 210/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  84%| | 211/250 [00:44<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 212/250 [00:45<00:08,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  85%| | 213/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 214/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 215/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  86%| | 216/250 [00:45<00:07,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 217/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  87%| | 218/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 219/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 220/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  88%| | 221/250 [00:46<00:06,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 222/250 [00:47<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  89%| | 223/250 [00:47<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 224/250 [00:47<00:05,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 225/250 [00:47<00:05,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  90%| | 226/250 [00:48<00:05,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 227/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  91%| | 228/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 229/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 230/250 [00:48<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  92%|| 231/250 [00:49<00:04,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 232/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  93%|| 233/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 234/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 235/250 [00:49<00:03,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  94%|| 236/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 237/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  95%|| 238/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 239/250 [00:50<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 240/250 [00:51<00:02,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  96%|| 241/250 [00:51<00:01,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 242/250 [00:51<00:01,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  97%|| 243/250 [00:51<00:01,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 244/250 [00:51<00:01,  4.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 245/250 [00:52<00:01,  4.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  98%|| 246/250 [00:52<00:00,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 247/250 [00:52<00:00,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler:  99%|| 248/250 [00:52<00:00,  4.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 249/250 [00:52<00:00,  4.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "PLMS Sampler: 100%|| 250/250 [00:53<00:00,  4.71it/s]\u001b[A\u001b[A\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import Identity\n",
    "import lpips\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d\n",
    "\n",
    "def transform_normalize(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def transform_channel_last(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        return img\n",
    "    return rearrange(img, 'c h w -> h w c')\n",
    "\n",
    "def main(config):\n",
    "    # project setup\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    crop_pix = int(config.crop_ratio*config.img_size)\n",
    "    img_transform_train = transforms.Compose([\n",
    "        transform_normalize,\n",
    "\n",
    "        transforms.Resize((512, 512)),\n",
    "        random_crop(config.img_size-crop_pix, p=0.5),\n",
    "\n",
    "        transforms.Resize((512, 512)),\n",
    "        transform_channel_last\n",
    "    ])\n",
    "    img_transform_test = transforms.Compose([\n",
    "        transform_normalize,\n",
    "\n",
    "        transforms.Resize((512, 512)),\n",
    "        transform_channel_last\n",
    "    ])\n",
    "    if config.dataset == 'EEG':\n",
    "\n",
    "        eeg_latents_dataset_train, eeg_latents_dataset_test = create_EEG_dataset(image_transform=[img_transform_train, img_transform_test], subject = config.subject)\n",
    "        # eeg_latents_dataset_train, eeg_latents_dataset_test = create_EEG_dataset_viz( image_transform=[img_transform_train, img_transform_test])\n",
    "        num_voxels = eeg_latents_dataset_train.data_len\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    # print(num_voxels)\n",
    "\n",
    "    # prepare pretrained mbm\n",
    "\n",
    "    pretrain_mbm_metafile = torch.load(config.pretrain_mbm_path, map_location='cpu')\n",
    "\n",
    "    # create generateive model\n",
    "    generative_model = eLDM(pretrain_mbm_metafile, num_voxels,\n",
    "                device=device, pretrain_root=config.pretrain_gm_path, logger=config.logger,\n",
    "                ddim_steps=config.ddim_steps, global_pool=config.global_pool, use_time_cond=config.use_time_cond, clip_tune = config.clip_tune, cls_tune = config.cls_tune, temperature=config.temperature)\n",
    "\n",
    "    # resume training if applicable\n",
    "    if config.checkpoint_path is not None:\n",
    "        model_meta = torch.load(config.checkpoint_path, map_location='cpu')\n",
    "        generative_model.model.load_state_dict(model_meta['model_state_dict'])\n",
    "        print('model resumed')\n",
    "\n",
    "    # finetune the model\n",
    "    trainer = create_trainer(config.num_epoch, config.precision, config.accumulate_grad, config.logger, check_val_every_n_epoch=config.num_epoch)\n",
    "    generative_model.finetune(trainer, eeg_latents_dataset_train, eeg_latents_dataset_test,\n",
    "                config.batch_size, config.lr, config.output_path, config=config)\n",
    "\n",
    "    # generate images\n",
    "    # generate limited train images and generate images for subjects seperately\n",
    "    #generate_images(generative_model, eeg_latents_dataset_train, eeg_latents_dataset_test, config)\n",
    "\n",
    "    return\n",
    "\n",
    "# Start\n",
    "config = Config_Generative_Model()\n",
    "config.pretrain_mbm_path = 'checkpoint-2650.pth'\n",
    "config.dataset = 'EEG'\n",
    "config.checkpoint_path = 'phase1_gen/checkpoint_best.pth'\n",
    "\n",
    "if config.checkpoint_path is not None:\n",
    "    model_meta = torch.load(config.checkpoint_path, map_location='cpu')\n",
    "    ckp = config.checkpoint_path\n",
    "    print(config)\n",
    "    config.checkpoint_path = ckp\n",
    "    print('Resuming from checkpoint: {}'.format(config.checkpoint_path))\n",
    "\n",
    "output_path = os.path.join(config.output_path, 'results', 'generation',  '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
    "config.output_path = output_path\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# logger = WandbLogger()\n",
    "config.logger = None # logger\n",
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDiffusion\n",
      "Loss Type is: l2\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 860.51 M params.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "missing keys: ['decoder_pos_embed', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias']\n",
      "unexpected keys: ['mask_token']\n",
      "{'eeg': tensor([[ 31.2758,  29.4653,   9.0771,  ...,  31.8139,  28.7686,  17.1586],\n",
      "        [ 15.0796,  14.4005,   3.5336,  ...,   2.8776,   9.4266,  12.0622],\n",
      "        [ -1.5369, -10.3915,  -4.7796,  ..., -11.0496,  -8.2661,   2.1402],\n",
      "        ...,\n",
      "        [  0.1162,   2.5756,  -5.3151,  ...,  34.1711,  29.8769,  10.0787],\n",
      "        [ 17.1160,   7.2740,   5.8176,  ...,  35.4442,  33.4681,  29.4932],\n",
      "        [  4.0641,  12.0446,   9.8263,  ..., -15.1004, -11.0612,  -5.2252]]), 'image': tensor([[[-0.9882, -0.9954, -0.9882],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -0.9935, -1.0000],\n",
      "         [-1.0000, -1.0000, -0.9996]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -0.9950]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -0.9969]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -0.9924],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -0.9946],\n",
      "         [-1.0000, -1.0000, -0.9935],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -0.9928],\n",
      "         [-1.0000, -1.0000, -0.9993],\n",
      "         [-1.0000, -1.0000, -0.9928]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923]],\n",
      "\n",
      "        [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         ...,\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521]],\n",
      "\n",
      "        [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         ...,\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],\n",
      "         [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:23<00:00,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[-7.0797e+00,  7.9666e+00,  3.0304e+01,  ..., -1.2379e+01,\n",
      "         -1.0021e+01, -1.2739e+00],\n",
      "        [-7.6720e+00,  3.0936e+00,  1.3218e+01,  ...,  3.1919e+00,\n",
      "         -1.3593e+00,  2.2564e-01],\n",
      "        [-2.4797e+00,  1.4357e+01,  3.4905e+01,  ...,  5.9600e+00,\n",
      "         -2.1045e+00,  1.7418e+00],\n",
      "        ...,\n",
      "        [ 9.0438e+00,  2.7886e+01,  4.3657e+01,  ..., -3.3696e+00,\n",
      "         -3.2369e+00, -1.4621e+00],\n",
      "        [-1.2324e+01, -4.3555e+00,  1.0778e+01,  ...,  7.3224e-01,\n",
      "          2.7229e+00,  1.5161e+00],\n",
      "        [-2.9408e+00,  1.8184e+01,  4.3147e+01,  ..., -1.0963e-01,\n",
      "         -3.5224e-02,  1.5783e+00]]), 'image': tensor([[[-0.7961, -0.9979, -0.9399],\n",
      "         [-0.7967, -0.9925, -0.9766],\n",
      "         [-0.7537, -0.9978, -0.9823],\n",
      "         ...,\n",
      "         [-0.7632, -0.8056, -0.6025],\n",
      "         [-0.7637, -0.8227, -0.6546],\n",
      "         [-0.8708, -0.9204, -0.7897]],\n",
      "\n",
      "        [[-0.7854, -0.9937, -0.9592],\n",
      "         [-0.8102, -0.9980, -0.9576],\n",
      "         [-0.8230, -0.9867, -0.8814],\n",
      "         ...,\n",
      "         [-0.8671, -0.9155, -0.7205],\n",
      "         [-0.8583, -0.9153, -0.7470],\n",
      "         [-0.8357, -0.8878, -0.7567]],\n",
      "\n",
      "        [[-0.8362, -0.9513, -0.8874],\n",
      "         [-0.7052, -0.8439, -0.7454],\n",
      "         [-0.7137, -0.7922, -0.5821],\n",
      "         ...,\n",
      "         [-0.2975, -0.3574, -0.1608],\n",
      "         [-0.8812, -0.9361, -0.7805],\n",
      "         [-0.8822, -0.9318, -0.8086]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7394, -0.7635, -0.6834],\n",
      "         [-0.7459, -0.7711, -0.6948],\n",
      "         [-0.7706, -0.7942, -0.7282],\n",
      "         ...,\n",
      "         [-0.8010, -0.8275, -0.6957],\n",
      "         [-0.7775, -0.8122, -0.6813],\n",
      "         [-0.7597, -0.7921, -0.6606]],\n",
      "\n",
      "        [[-0.7325, -0.7572, -0.6750],\n",
      "         [-0.7448, -0.7722, -0.6938],\n",
      "         [-0.7702, -0.7937, -0.7224],\n",
      "         ...,\n",
      "         [-0.8080, -0.8385, -0.7180],\n",
      "         [-0.7818, -0.8112, -0.6938],\n",
      "         [-0.7591, -0.7904, -0.6731]],\n",
      "\n",
      "        [[-0.7356, -0.7592, -0.6828],\n",
      "         [-0.7625, -0.7860, -0.7182],\n",
      "         [-0.7723, -0.7958, -0.7252],\n",
      "         ...,\n",
      "         [-0.8093, -0.8404, -0.7296],\n",
      "         [-0.7866, -0.8174, -0.7070],\n",
      "         [-0.7649, -0.7963, -0.6863]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.3981, -1.3835, -1.4565,  ..., -1.4127, -1.4273, -1.4711],\n",
      "         [-1.3981, -1.4419, -1.4857,  ..., -1.3981, -1.4127, -1.5003],\n",
      "         [-1.4857, -1.4857, -1.5003,  ..., -1.3251, -1.5441, -1.4857],\n",
      "         ...,\n",
      "         [-1.3835, -1.4273, -1.4273,  ..., -1.4711, -1.4711, -1.4565],\n",
      "         [-1.3251, -1.3835, -1.4419,  ..., -1.4419, -1.4419, -1.3981],\n",
      "         [-1.3251, -1.3689, -1.4419,  ..., -1.4273, -1.4273, -1.3689]],\n",
      "\n",
      "        [[-1.7071, -1.6921, -1.6170,  ..., -1.4669, -1.4820, -1.5270],\n",
      "         [-1.5120, -1.5720, -1.6020,  ..., -1.4669, -1.4669, -1.5570],\n",
      "         [-1.6470, -1.6470, -1.5870,  ..., -1.4069, -1.6020, -1.5570],\n",
      "         ...,\n",
      "         [-1.3769, -1.4820, -1.5870,  ..., -1.5720, -1.4970, -1.4519],\n",
      "         [-1.3169, -1.3919, -1.4669,  ..., -1.5720, -1.4669, -1.3919],\n",
      "         [-1.3169, -1.3769, -1.4519,  ..., -1.5570, -1.4669, -1.3769]],\n",
      "\n",
      "        [[-1.3665, -1.1816, -1.0394,  ..., -0.6412, -0.7977, -0.9825],\n",
      "         [-0.9683, -0.9683, -1.0394,  ..., -0.6839, -0.8830, -1.0394],\n",
      "         [-1.0536, -1.0821, -1.0678,  ..., -0.6839, -1.0678, -1.0821],\n",
      "         ...,\n",
      "         [-0.8830, -0.9541, -1.0252,  ..., -0.9399, -0.9256, -0.9114],\n",
      "         [-0.9114, -0.9967, -1.0963,  ..., -1.0394, -0.9541, -0.8972],\n",
      "         [-0.9256, -1.0110, -1.1247,  ..., -1.0678, -0.9967, -0.9114]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[-12.4580, -18.7775, -10.7446,  ...,  -1.3661,  -0.8444,  -0.2966],\n",
      "        [  5.3483,   6.1079,   6.7741,  ..., -14.6720, -14.0955,  -7.8217],\n",
      "        [  6.5793,  -2.7611,  -1.9318,  ..., -18.4684, -15.3131,  -5.1520],\n",
      "        ...,\n",
      "        [  2.3603,   5.0218,   7.3112,  ...,   7.1982,  -1.8517,   4.0997],\n",
      "        [  3.2593,   9.2146,  14.4333,  ...,  17.4339,   2.4229,   5.3870],\n",
      "        [  1.5259,  -6.9375,  -3.3509,  ...,   7.7043,   6.1644,   3.5184]]), 'image': tensor([[[ 0.5648, -0.8734, -0.8367],\n",
      "         [ 0.5883, -0.8961, -0.8198],\n",
      "         [ 0.5753, -0.8895, -0.8263],\n",
      "         ...,\n",
      "         [ 0.6196, -0.8264, -0.8186],\n",
      "         [ 0.4696, -0.3567, -0.3973],\n",
      "         [ 0.9249,  0.9385,  0.9309]],\n",
      "\n",
      "        [[ 0.5600, -0.8766, -0.8183],\n",
      "         [ 0.5573, -0.8781, -0.8142],\n",
      "         [ 0.5575, -0.8777, -0.8146],\n",
      "         ...,\n",
      "         [ 0.6077, -0.8306, -0.8059],\n",
      "         [ 0.4437, -0.3547, -0.3705],\n",
      "         [ 0.9110,  0.9197,  0.9180]],\n",
      "\n",
      "        [[ 0.5600, -0.8766, -0.8198],\n",
      "         [ 0.5459, -0.8670, -0.8193],\n",
      "         [ 0.5515, -0.8672, -0.8191],\n",
      "         ...,\n",
      "         [ 0.5997, -0.8407, -0.8109],\n",
      "         [ 0.4611, -0.3562, -0.3721],\n",
      "         [ 0.9370,  0.9186,  0.8911]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.5658, -0.8766, -0.8068],\n",
      "         [ 0.5608, -0.8674, -0.8204],\n",
      "         [ 0.5648, -0.8745, -0.8249],\n",
      "         ...,\n",
      "         [ 0.5986, -0.8692, -0.7776],\n",
      "         [ 0.4232, -0.4977, -0.4982],\n",
      "         [ 0.9417,  0.9134,  0.8556]],\n",
      "\n",
      "        [[ 0.5772, -0.8842, -0.8079],\n",
      "         [ 0.5608, -0.8729, -0.8204],\n",
      "         [ 0.5623, -0.8745, -0.8224],\n",
      "         ...,\n",
      "         [ 0.5835, -0.8683, -0.7742],\n",
      "         [ 0.3997, -0.4850, -0.4936],\n",
      "         [ 0.9481,  0.9294,  0.8847]],\n",
      "\n",
      "        [[ 0.5592, -0.8525, -0.8314],\n",
      "         [ 0.5601, -0.8666, -0.8269],\n",
      "         [ 0.5535, -0.8600, -0.8269],\n",
      "         ...,\n",
      "         [ 0.5861, -0.8414, -0.7847],\n",
      "         [ 0.3709, -0.4579, -0.5002],\n",
      "         [ 0.9442,  0.9289,  0.8698]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[ 1.5362,  1.5070,  1.4778,  ...,  1.5362,  1.5800,  1.6238],\n",
      "         [ 1.6092,  1.5800,  1.5362,  ...,  1.5946,  1.6384,  1.6676],\n",
      "         [ 1.6676,  1.6530,  1.6092,  ...,  1.6530,  1.6676,  1.6384],\n",
      "         ...,\n",
      "         [ 0.4413,  0.4413,  0.4413,  ...,  0.5289,  0.5727,  0.6457],\n",
      "         [ 0.7187,  0.7333,  0.7625,  ...,  0.4997,  0.4851,  0.5143],\n",
      "         [-0.4346, -0.4346, -0.4200,  ...,  0.5143,  0.4997,  0.5143]],\n",
      "\n",
      "        [[ 1.5046,  1.4145,  1.3245,  ...,  1.4295,  1.5646,  1.6997],\n",
      "         [ 1.6847,  1.6096,  1.5196,  ...,  1.6847,  1.8198,  1.8948],\n",
      "         [ 1.8498,  1.8047,  1.7147,  ...,  1.8798,  1.8948,  1.8047],\n",
      "         ...,\n",
      "         [ 1.3395,  1.3695,  1.3995,  ...,  1.3095,  1.2945,  1.2795],\n",
      "         [ 1.6697,  1.7147,  1.7297,  ...,  1.4295,  1.3845,  1.3395],\n",
      "         [ 0.0188,  0.0338,  0.0488,  ...,  1.5346,  1.4896,  1.4596]],\n",
      "\n",
      "        [[-0.7123, -0.7834, -0.8688,  ..., -0.7834, -0.6697, -0.4990],\n",
      "         [-0.3853, -0.5133, -0.6412,  ..., -0.4422, -0.2573, -0.1009],\n",
      "         [-0.1578, -0.2289, -0.3426,  ..., -0.1009, -0.0867, -0.2431],\n",
      "         ...,\n",
      "         [ 1.1789,  1.2785,  1.3496,  ...,  1.0652,  0.8519,  0.6386],\n",
      "         [ 1.7762,  1.8188,  1.8615,  ...,  1.3211,  1.2358,  1.1078],\n",
      "         [ 0.2546,  0.2546,  0.2688,  ...,  1.5913,  1.5202,  1.4349]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[  1.5370, -10.5735, -15.6729,  ...,   3.0918,   3.3127,   9.9382],\n",
      "        [  5.3387,   6.0025,   4.7585,  ...,   9.4224,   3.8163,   3.4321],\n",
      "        [ -4.0111, -19.4994, -28.0562,  ...,  -4.7116,   4.9076,  12.8581],\n",
      "        ...,\n",
      "        [-15.0518, -27.9166, -22.5870,  ...,   4.6657,   8.1840,  15.4685],\n",
      "        [  3.1407,  -0.5851,  -5.0565,  ...,   0.6358,   1.7760,   2.9292],\n",
      "        [  0.8762,   1.4519,   4.1812,  ...,   1.6729,   5.7100,  13.7418]]), 'image': tensor([[[-0.9083, -0.9480, -0.9685],\n",
      "         [-0.8018, -0.8193, -0.8599],\n",
      "         [-0.1280, -0.1944, -0.3461],\n",
      "         ...,\n",
      "         [-0.8716, -0.9741, -0.9806],\n",
      "         [-0.8813, -0.9774, -0.9903],\n",
      "         [-0.8172, -0.9522, -0.9879]],\n",
      "\n",
      "        [[-0.6924, -0.7637, -0.8274],\n",
      "         [-0.8642, -0.8894, -0.9236],\n",
      "         [-0.1490, -0.2037, -0.3234],\n",
      "         ...,\n",
      "         [-0.8988, -0.9784, -0.9870],\n",
      "         [-0.8836, -0.9589, -0.9845],\n",
      "         [-0.8363, -0.9318, -0.9769]],\n",
      "\n",
      "        [[-0.8721, -0.9087, -0.9592],\n",
      "         [-0.9390, -0.9495, -0.9720],\n",
      "         [-0.2411, -0.3073, -0.3929],\n",
      "         ...,\n",
      "         [-0.9262, -0.9864, -0.9909],\n",
      "         [-0.9026, -0.9637, -0.9901],\n",
      "         [-0.8327, -0.9516, -0.9786]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9934, -0.9934, -0.9948],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [ 0.2084,  0.0543, -0.1596],\n",
      "         [-0.6725, -0.7696, -0.8897],\n",
      "         [-0.7330, -0.8178, -0.9493]],\n",
      "\n",
      "        [[-0.9934, -0.9934, -1.0000],\n",
      "         [-0.9980, -0.9980, -1.0000],\n",
      "         [-0.9959, -0.9959, -0.9967],\n",
      "         ...,\n",
      "         [ 0.1864,  0.0229, -0.1804],\n",
      "         [-0.6272, -0.7240, -0.8292],\n",
      "         [-0.4908, -0.5746, -0.7126]],\n",
      "\n",
      "        [[-0.9934, -0.9879, -1.0000],\n",
      "         [-0.9900, -0.9900, -0.9906],\n",
      "         [-0.9947, -0.9947, -0.9950],\n",
      "         ...,\n",
      "         [ 0.1880,  0.0231, -0.2024],\n",
      "         [-0.7113, -0.8150, -0.9013],\n",
      "         [-0.7021, -0.7935, -0.8774]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.2813, -1.0915, -1.5003,  ..., -1.5733, -1.5587, -1.5441],\n",
      "         [-1.3689, -1.0185, -1.4857,  ..., -1.6463, -1.5879, -1.5733],\n",
      "         [-1.4273, -1.0623, -1.4419,  ..., -1.6609, -1.6025, -1.5587],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.6755,  ..., -1.3105, -0.5076, -0.7120],\n",
      "         [-1.7923, -1.7923, -1.7485,  ..., -1.3981, -0.5368, -0.7850],\n",
      "         [-1.7923, -1.0331,  0.0471,  ..., -1.2229, -0.6098, -0.8726]],\n",
      "\n",
      "        [[-1.3019, -1.1218, -1.5570,  ..., -1.7371, -1.6921, -1.6771],\n",
      "         [-1.3769, -1.0317, -1.5420,  ..., -1.7521, -1.7071, -1.6621],\n",
      "         [-1.4669, -1.0918, -1.5120,  ..., -1.7371, -1.6921, -1.6921],\n",
      "         ...,\n",
      "         [-1.7521, -1.7521, -1.6320,  ..., -1.4069, -0.6565, -0.8366],\n",
      "         [-1.7521, -1.7521, -1.7071,  ..., -1.4820, -0.6865, -0.9117],\n",
      "         [-1.7521, -0.9867,  0.1389,  ..., -1.3019, -0.7616, -1.0017]],\n",
      "\n",
      "        [[-1.1532, -0.9967, -1.3380,  ..., -1.4660, -1.4376, -1.4518],\n",
      "         [-1.2243, -0.8830, -1.3096,  ..., -1.4660, -1.4376, -1.4518],\n",
      "         [-1.2954, -0.9683, -1.3096,  ..., -1.4802, -1.4376, -1.4376],\n",
      "         ...,\n",
      "         [-1.4802, -1.4802, -1.3949,  ..., -1.2811, -0.7266, -0.9114],\n",
      "         [-1.4802, -1.4802, -1.4518,  ..., -1.3238, -0.7266, -0.9683],\n",
      "         [-1.4802, -0.7408,  0.3684,  ..., -1.1816, -0.7977, -0.9967]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[ 1.3160e+01,  1.0829e+01,  1.3189e+01,  ...,  3.2699e+00,\n",
      "          1.1619e+01,  2.2731e+01],\n",
      "        [ 1.3933e+01, -2.8788e+00, -1.7695e+01,  ...,  4.3660e-02,\n",
      "          3.8211e+00,  1.3390e+01],\n",
      "        [ 2.0555e+01,  1.3363e+01,  1.3315e+01,  ...,  5.1273e+01,\n",
      "          4.2128e+01,  3.5597e+01],\n",
      "        ...,\n",
      "        [-1.1408e+01, -3.3456e+00,  5.8924e+00,  ...,  2.0493e+00,\n",
      "         -4.4148e+00, -7.0397e+00],\n",
      "        [-6.8439e+00, -4.0863e+00,  6.3766e-02,  ...,  1.1415e+01,\n",
      "          5.7782e+00,  8.3836e+00],\n",
      "        [ 2.1796e+01,  2.8897e+01,  2.8756e+01,  ...,  1.4106e+01,\n",
      "          2.2142e+01,  2.8879e+01]]), 'image': tensor([[[-0.4768, -0.3327, -0.2551],\n",
      "         [-0.4745, -0.3320, -0.2543],\n",
      "         [-0.4739, -0.3255, -0.2548],\n",
      "         ...,\n",
      "         [ 0.1457,  0.2006,  0.1596],\n",
      "         [ 0.1457,  0.2012,  0.1606],\n",
      "         [ 0.1457,  0.2081,  0.1608]],\n",
      "\n",
      "        [[-0.4695, -0.3248, -0.2559],\n",
      "         [-0.4721, -0.3255, -0.2472],\n",
      "         [-0.4667, -0.3255, -0.2524],\n",
      "         ...,\n",
      "         [ 0.1532,  0.2078,  0.1608],\n",
      "         [ 0.1548,  0.2149,  0.1662],\n",
      "         [ 0.1548,  0.2101,  0.1614]],\n",
      "\n",
      "        [[-0.4684, -0.3218, -0.2520],\n",
      "         [-0.4667, -0.3255, -0.2475],\n",
      "         [-0.4667, -0.3255, -0.2479],\n",
      "         ...,\n",
      "         [ 0.1538,  0.2078,  0.1608],\n",
      "         [ 0.1605,  0.2122,  0.1635],\n",
      "         [ 0.1589,  0.2109,  0.1637]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -0.9962],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -0.9996],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-0.5368, -0.5514, -0.5660,  ..., -0.2886, -0.2740, -0.2740],\n",
      "         [-0.5222, -0.5222, -0.5368,  ..., -0.2594, -0.2594, -0.2448],\n",
      "         [-0.4930, -0.5076, -0.5222,  ..., -0.2302, -0.2302, -0.2302],\n",
      "         ...,\n",
      "         [-0.9164, -0.8726, -0.8872,  ..., -0.0259, -0.0988, -0.1280],\n",
      "         [-0.9164, -0.9018, -0.8726,  ...,  0.0033, -0.0550, -0.1134],\n",
      "         [-0.9310, -0.9018, -0.8726,  ...,  0.0325, -0.0113, -0.0842]],\n",
      "\n",
      "        [[-0.1313, -0.1463, -0.1463,  ...,  0.0338,  0.0338,  0.0488],\n",
      "         [-0.1163, -0.1163, -0.1313,  ...,  0.0488,  0.0638,  0.0638],\n",
      "         [-0.1012, -0.1012, -0.1163,  ...,  0.0638,  0.0789,  0.0789],\n",
      "         ...,\n",
      "         [-0.9717, -0.9567, -0.9717,  ..., -1.7521, -1.7371, -1.7071],\n",
      "         [-0.9717, -0.9567, -0.9567,  ..., -1.7521, -1.7371, -1.7071],\n",
      "         [-0.9717, -0.9717, -0.9567,  ..., -1.7521, -1.7521, -1.7221]],\n",
      "\n",
      "        [[ 0.0840,  0.0840,  0.0698,  ...,  0.1551,  0.1693,  0.1693],\n",
      "         [ 0.0982,  0.0982,  0.0840,  ...,  0.1693,  0.1693,  0.1693],\n",
      "         [ 0.1124,  0.1124,  0.0982,  ...,  0.1835,  0.1835,  0.1835],\n",
      "         ...,\n",
      "         [-0.8972, -0.8830, -0.8972,  ..., -1.4802, -1.4802, -1.4518],\n",
      "         [-0.8830, -0.8830, -0.8972,  ..., -1.4802, -1.4660, -1.4518],\n",
      "         [-0.8830, -0.8830, -0.8972,  ..., -1.4802, -1.4660, -1.4660]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[ -1.0935,  -7.1480, -21.0310,  ...,  35.2090,  22.4752,   5.8799],\n",
      "        [ 10.0292,  -1.9445,   2.2350,  ...,  28.1039,  26.5749,  27.7880],\n",
      "        [-29.1906, -27.4488, -25.3835,  ...,   8.4390,  -7.2072, -18.6615],\n",
      "        ...,\n",
      "        [ -5.2269, -18.0210,  -7.7746,  ..., -15.9044, -23.4851,  -5.4836],\n",
      "        [ 16.2214,  16.0033,  13.8564,  ...,  10.5857,   6.2667,  -4.1376],\n",
      "        [ -6.0644,  -3.1729,   7.4400,  ...,  16.2562, -10.6197, -29.8906]]), 'image': tensor([[[-0.8510, -0.8510, -0.8118],\n",
      "         [-0.8510, -0.8395, -0.8060],\n",
      "         [-0.8467, -0.8310, -0.8039],\n",
      "         ...,\n",
      "         [-0.5835, -0.8231, -0.7839],\n",
      "         [-0.5922, -0.8317, -0.7988],\n",
      "         [-0.5922, -0.8431, -0.8275]],\n",
      "\n",
      "        [[-0.8510, -0.8453, -0.8060],\n",
      "         [-0.8468, -0.8338, -0.8045],\n",
      "         [-0.8409, -0.8284, -0.8008],\n",
      "         ...,\n",
      "         [-0.5752, -0.8263, -0.7896],\n",
      "         [-0.5776, -0.8255, -0.7968],\n",
      "         [-0.5692, -0.8202, -0.8045]],\n",
      "\n",
      "        [[-0.8510, -0.8388, -0.7996],\n",
      "         [-0.8421, -0.8274, -0.7996],\n",
      "         [-0.8393, -0.8255, -0.8000],\n",
      "         ...,\n",
      "         [-0.5467, -0.8145, -0.7859],\n",
      "         [-0.5490, -0.8138, -0.7843],\n",
      "         [-0.5565, -0.8118, -0.7874]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9843, -0.9808, -0.9573],\n",
      "         [-0.9786, -0.9776, -0.9598],\n",
      "         [-0.9784, -0.9765, -0.9608],\n",
      "         ...,\n",
      "         [-0.9686, -0.9667, -0.9553],\n",
      "         [-0.9686, -0.9666, -0.9541],\n",
      "         [-0.9686, -0.9729, -0.9573]],\n",
      "\n",
      "        [[-0.9843, -0.9786, -0.9551],\n",
      "         [-0.9786, -0.9770, -0.9592],\n",
      "         [-0.9808, -0.9776, -0.9608],\n",
      "         ...,\n",
      "         [-0.9707, -0.9698, -0.9529],\n",
      "         [-0.9713, -0.9692, -0.9541],\n",
      "         [-0.9729, -0.9707, -0.9572]],\n",
      "\n",
      "        [[-0.9843, -0.9843, -0.9608],\n",
      "         [-0.9786, -0.9786, -0.9608],\n",
      "         [-0.9808, -0.9808, -0.9608],\n",
      "         ...,\n",
      "         [-0.9765, -0.9729, -0.9529],\n",
      "         [-0.9786, -0.9707, -0.9572],\n",
      "         [-0.9843, -0.9765, -0.9686]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.5149, -1.5003, -1.4857,  ..., -0.9602, -0.9602, -1.0039],\n",
      "         [-1.5003, -1.4857, -1.4857,  ..., -0.8872, -0.9018, -0.9310],\n",
      "         [-1.4857, -1.4711, -1.4565,  ..., -0.8872, -0.8726, -0.8872],\n",
      "         ...,\n",
      "         [-1.7631, -1.7485, -1.7485,  ..., -1.7339, -1.7339, -1.7339],\n",
      "         [-1.7631, -1.7485, -1.7485,  ..., -1.6901, -1.7193, -1.7339],\n",
      "         [-1.7631, -1.7631, -1.7631,  ..., -1.7339, -1.7339, -1.7485]],\n",
      "\n",
      "        [[-1.4369, -1.4219, -1.4069,  ..., -1.4069, -1.4069, -1.4219],\n",
      "         [-1.4369, -1.4069, -1.4069,  ..., -1.3919, -1.3919, -1.3919],\n",
      "         [-1.4219, -1.3919, -1.3769,  ..., -1.3769, -1.3919, -1.3769],\n",
      "         ...,\n",
      "         [-1.7071, -1.7071, -1.7071,  ..., -1.6921, -1.6921, -1.6921],\n",
      "         [-1.7071, -1.7071, -1.7071,  ..., -1.6621, -1.6771, -1.6921],\n",
      "         [-1.7221, -1.7221, -1.7071,  ..., -1.6921, -1.6921, -1.7071]],\n",
      "\n",
      "        [[-1.1247, -1.1247, -1.1105,  ..., -1.0821, -1.0821, -1.1105],\n",
      "         [-1.1105, -1.1247, -1.1105,  ..., -1.0678, -1.0963, -1.0963],\n",
      "         [-1.1247, -1.1105, -1.0821,  ..., -1.0821, -1.0963, -1.0963],\n",
      "         ...,\n",
      "         [-1.4091, -1.4091, -1.4091,  ..., -1.4091, -1.4091, -1.4091],\n",
      "         [-1.4091, -1.4091, -1.4091,  ..., -1.3665, -1.3949, -1.4091],\n",
      "         [-1.4091, -1.4091, -1.4233,  ..., -1.3949, -1.3949, -1.4091]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[-12.0669, -17.2922, -19.4811,  ...,  22.2766,   3.5682,   2.3549],\n",
      "        [ -6.6401,  -6.8917,  -4.8481,  ...,  24.5323,   2.1332,   1.2564],\n",
      "        [-11.2560, -18.7233, -23.2539,  ...,  14.9340,  10.4460,   1.5253],\n",
      "        ...,\n",
      "        [-10.4898, -12.9331, -18.1434,  ...,  17.2675,  21.4036,   5.6655],\n",
      "        [ -8.2212, -16.2164, -19.8437,  ...,  12.5858,   7.2360,   0.3850],\n",
      "        [-19.8649, -42.4707, -39.1617,  ...,   8.2883,   9.1557,   3.4725]]), 'image': tensor([[[-0.9877, -0.9943, -0.9792],\n",
      "         [-0.9959, -0.9988, -0.9894],\n",
      "         [-0.9948, -0.9935, -0.9869],\n",
      "         ...,\n",
      "         [-0.9749, -0.9552, -0.9461],\n",
      "         [-0.9686, -0.9611, -0.9520],\n",
      "         [-0.9752, -0.9674, -0.9649]],\n",
      "\n",
      "        [[-0.9956, -0.9988, -0.9894],\n",
      "         [-0.9941, -0.9922, -0.9943],\n",
      "         [-0.9992, -0.9922, -0.9922],\n",
      "         ...,\n",
      "         [-0.9749, -0.9670, -0.9513],\n",
      "         [-0.9686, -0.9608, -0.9451],\n",
      "         [-0.9752, -0.9608, -0.9485]],\n",
      "\n",
      "        [[-0.9922, -0.9988, -0.9869],\n",
      "         [-0.9922, -0.9922, -0.9892],\n",
      "         [-0.9984, -0.9971, -0.9959],\n",
      "         ...,\n",
      "         [-0.9811, -0.9670, -0.9576],\n",
      "         [-0.9749, -0.9608, -0.9513],\n",
      "         [-0.9762, -0.9660, -0.9566]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9370, -0.9212, -0.8824],\n",
      "         [-0.9424, -0.9278, -0.9002],\n",
      "         [-0.9510, -0.9341, -0.9011],\n",
      "         ...,\n",
      "         [-0.9811, -0.9517, -0.9200],\n",
      "         [-0.9779, -0.9491, -0.9137],\n",
      "         [-0.9762, -0.9474, -0.9151]],\n",
      "\n",
      "        [[-0.9250, -0.9068, -0.8535],\n",
      "         [-0.9291, -0.9153, -0.8742],\n",
      "         [-0.9394, -0.9237, -0.8783],\n",
      "         ...,\n",
      "         [-0.9719, -0.9521, -0.9200],\n",
      "         [-0.9686, -0.9469, -0.9137],\n",
      "         [-0.9752, -0.9482, -0.9238]],\n",
      "\n",
      "        [[-0.9084, -0.8927, -0.8326],\n",
      "         [-0.9156, -0.8999, -0.8541],\n",
      "         [-0.9225, -0.9015, -0.8547],\n",
      "         ...,\n",
      "         [-0.9762, -0.9461, -0.9147],\n",
      "         [-0.9787, -0.9485, -0.9172],\n",
      "         [-0.9829, -0.9517, -0.9280]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.7777, -1.7777, -1.7777,  ..., -1.7339, -1.7485, -1.7339],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7485, -1.7485, -1.7485],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7485, -1.7485, -1.7485],\n",
      "         ...,\n",
      "         [-1.7193, -1.6901, -1.5295,  ..., -1.7485, -1.7485, -1.7485],\n",
      "         [-1.7047, -1.7047, -1.6171,  ..., -1.7485, -1.7485, -1.7485],\n",
      "         [-1.6463, -1.6755, -1.6317,  ..., -1.7339, -1.7485, -1.7485]],\n",
      "\n",
      "        [[-1.7371, -1.7371, -1.7371,  ..., -1.6771, -1.6771, -1.6771],\n",
      "         [-1.7371, -1.7371, -1.7371,  ..., -1.6771, -1.6771, -1.6771],\n",
      "         [-1.7371, -1.7371, -1.7371,  ..., -1.6771, -1.6771, -1.6771],\n",
      "         ...,\n",
      "         [-1.6621, -1.6320, -1.5420,  ..., -1.6470, -1.6470, -1.6621],\n",
      "         [-1.6320, -1.6470, -1.6020,  ..., -1.6470, -1.6470, -1.6621],\n",
      "         [-1.5720, -1.6020, -1.5870,  ..., -1.6470, -1.6470, -1.6621]],\n",
      "\n",
      "        [[-1.4660, -1.4660, -1.4660,  ..., -1.3949, -1.3949, -1.3949],\n",
      "         [-1.4660, -1.4660, -1.4660,  ..., -1.3949, -1.3949, -1.3949],\n",
      "         [-1.4660, -1.4660, -1.4660,  ..., -1.3949, -1.3949, -1.3949],\n",
      "         ...,\n",
      "         [-1.3665, -1.3238, -1.1816,  ..., -1.3238, -1.3380, -1.3238],\n",
      "         [-1.3238, -1.3380, -1.2527,  ..., -1.3238, -1.3238, -1.3380],\n",
      "         [-1.2243, -1.2669, -1.2527,  ..., -1.3096, -1.3238, -1.3380]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[-10.2126, -21.1907, -22.5725,  ...,  -4.1931,  -5.4504,   4.5759],\n",
      "        [-10.9528, -17.4639, -22.8224,  ...,   6.3629,   3.6012,   3.5655],\n",
      "        [ -2.6261, -18.3654, -27.7041,  ...,  -1.7913,  -2.0377,   2.8459],\n",
      "        ...,\n",
      "        [-14.5955, -33.0501, -34.7390,  ...,   1.8582,   2.8449,   0.8964],\n",
      "        [-16.5236, -22.4322, -21.9410,  ...,  18.5069,  10.6918,   4.4523],\n",
      "        [-14.6201, -20.6616, -18.6393,  ...,  17.4349,  10.0631,   5.0351]]), 'image': tensor([[[-0.8937, -0.8828, -0.6612],\n",
      "         [-0.9925, -0.9931, -0.8666],\n",
      "         [-0.9948, -1.0000, -0.9135],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137]],\n",
      "\n",
      "        [[-0.9755, -0.9793, -0.7948],\n",
      "         [-0.9959, -0.9980, -0.8829],\n",
      "         [-1.0000, -1.0000, -0.9129],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137]],\n",
      "\n",
      "        [[-0.9905, -0.9971, -0.8773],\n",
      "         [-0.9992, -0.9992, -0.8959],\n",
      "         [-1.0000, -1.0000, -0.9125],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137],\n",
      "         [-1.0000, -1.0000, -0.9137]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         ...,\n",
      "         [-0.9009, -0.8476, -0.7048],\n",
      "         [-0.8996, -0.8463, -0.7036],\n",
      "         [-0.8996, -0.8463, -0.7036]],\n",
      "\n",
      "        [[-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         ...,\n",
      "         [-0.9130, -0.8659, -0.7169],\n",
      "         [-0.9100, -0.8629, -0.7139],\n",
      "         [-0.9100, -0.8629, -0.7139]],\n",
      "\n",
      "        [[-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         [-0.8745, -0.8118, -0.6314],\n",
      "         ...,\n",
      "         [-0.9137, -0.8667, -0.7176],\n",
      "         [-0.9137, -0.8667, -0.7176],\n",
      "         [-0.9137, -0.8667, -0.7176]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.7631, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],\n",
      "         ...,\n",
      "         [-1.5441, -1.5295, -1.5003,  ..., -1.5441, -1.5441, -1.5587],\n",
      "         [-1.5587, -1.5587, -1.5295,  ..., -1.5733, -1.5879, -1.6025],\n",
      "         [-1.5587, -1.5587, -1.5587,  ..., -1.6025, -1.6317, -1.6317]],\n",
      "\n",
      "        [[-1.7221, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],\n",
      "         ...,\n",
      "         [-1.3319, -1.3319, -1.3319,  ..., -1.3619, -1.4069, -1.4219],\n",
      "         [-1.3769, -1.3769, -1.3619,  ..., -1.4219, -1.4519, -1.4519],\n",
      "         [-1.3919, -1.3919, -1.3919,  ..., -1.4669, -1.4970, -1.4820]],\n",
      "\n",
      "        [[-1.1958, -1.3096, -1.3238,  ..., -1.3238, -1.3238, -1.3238],\n",
      "         [-1.3096, -1.3096, -1.3238,  ..., -1.3238, -1.3238, -1.3238],\n",
      "         [-1.3238, -1.3238, -1.3238,  ..., -1.3238, -1.3238, -1.3238],\n",
      "         ...,\n",
      "         [-0.8119, -0.8119, -0.7977,  ..., -0.8972, -0.8972, -0.9114],\n",
      "         [-0.8119, -0.8119, -0.8261,  ..., -0.9256, -0.9399, -0.9399],\n",
      "         [-0.8119, -0.8261, -0.8545,  ..., -0.9399, -0.9683, -0.9683]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[ -7.5067, -12.7187, -13.8503,  ...,  25.6035,  11.2177,  -1.9779],\n",
      "        [ -1.6740,  -7.6568, -17.7656,  ...,  34.5545,  22.7529,   3.4322],\n",
      "        [  7.3958,  -1.8367, -14.8262,  ...,  15.3201,  10.8294,  -2.9495],\n",
      "        ...,\n",
      "        [ 22.5111,  24.7284,   7.3055,  ...,  17.1741,  14.9467,  -0.8001],\n",
      "        [  6.4045,   2.9902,  -8.9514,  ...,  11.5281,  -0.7750,  -4.3885],\n",
      "        [  3.1721,  -1.4323,  -9.3576,  ...,  24.0519,   8.5214,  -2.9430]]), 'image': tensor([[[ 3.7205e-01,  5.0111e-01,  5.7389e-01],\n",
      "         [ 3.0671e-01,  3.9122e-01,  4.8316e-01],\n",
      "         [-8.9344e-01, -8.6014e-01, -8.4859e-01],\n",
      "         ...,\n",
      "         [-2.9987e-01, -3.3745e-01, -2.5777e-01],\n",
      "         [-8.6270e-01, -8.8528e-01, -8.7238e-01],\n",
      "         [-1.7568e-01, -9.8495e-02, -1.2902e-02]],\n",
      "\n",
      "        [[ 3.5081e-01,  4.8632e-01,  5.4907e-01],\n",
      "         [ 2.8127e-01,  3.7443e-01,  4.5784e-01],\n",
      "         [-8.4804e-01, -8.1715e-01, -7.9580e-01],\n",
      "         ...,\n",
      "         [-3.1649e-01, -3.4764e-01, -2.5707e-01],\n",
      "         [-8.2334e-01, -8.2884e-01, -8.2843e-01],\n",
      "         [-2.2365e-01, -1.3701e-01, -4.0209e-02]],\n",
      "\n",
      "        [[ 3.2612e-01,  4.7400e-01,  5.3575e-01],\n",
      "         [ 2.7502e-01,  3.7935e-01,  4.5747e-01],\n",
      "         [-8.4253e-01, -8.2403e-01, -8.1148e-01],\n",
      "         ...,\n",
      "         [-3.5081e-01, -3.7905e-01, -2.8476e-01],\n",
      "         [-8.2039e-01, -8.3214e-01, -8.2744e-01],\n",
      "         [-2.6539e-01, -1.6821e-01, -7.6695e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0692e-01, -1.4691e-02, -1.1259e-02],\n",
      "         [-3.6866e-02,  3.8056e-02,  4.5409e-02],\n",
      "         [-1.1616e-01, -5.6357e-02, -8.8710e-02],\n",
      "         ...,\n",
      "         [-9.8645e-01, -9.8725e-01, -9.9363e-01],\n",
      "         [-7.4725e-01, -7.3647e-01, -7.5667e-01],\n",
      "         [-6.7002e-02,  3.0828e-02,  4.8627e-03]],\n",
      "\n",
      "        [[-1.1460e-01, -1.9762e-02, -1.9501e-02],\n",
      "         [-4.0500e-02,  3.0499e-02,  4.2834e-02],\n",
      "         [-1.3029e-01, -5.8712e-02, -8.6989e-02],\n",
      "         ...,\n",
      "         [-9.9314e-01, -9.8725e-01, -9.9363e-01],\n",
      "         [-7.4159e-01, -7.2582e-01, -7.4241e-01],\n",
      "         [-7.0023e-02,  3.3562e-02, -1.5136e-02]],\n",
      "\n",
      "        [[-1.2618e-01, -1.2454e-02, -3.2042e-02],\n",
      "         [-2.7787e-02,  4.2801e-02,  5.3158e-02],\n",
      "         [-1.1943e-01, -6.0735e-02, -8.1032e-02],\n",
      "         ...,\n",
      "         [-9.8742e-01, -9.8725e-01, -9.8742e-01],\n",
      "         [-7.2541e-01, -7.1449e-01, -7.2541e-01],\n",
      "         [-1.9934e-02,  5.5800e-02,  5.4573e-05]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[ 0.4559,  0.4559,  0.4705,  ...,  0.5727,  0.5727,  0.5727],\n",
      "         [ 0.4851,  0.4997,  0.4997,  ...,  0.6165,  0.6165,  0.6165],\n",
      "         [ 0.5289,  0.5289,  0.5289,  ...,  0.6457,  0.6457,  0.6457],\n",
      "         ...,\n",
      "         [-0.4930, -0.0842, -0.0259,  ..., -0.9018, -1.5587, -1.5295],\n",
      "         [-0.4638, -0.3470, -0.4346,  ..., -1.0039, -1.6901, -1.6609],\n",
      "         [ 0.0471,  0.0179,  0.0325,  ..., -0.7412, -1.3397, -1.4127]],\n",
      "\n",
      "        [[ 1.6847,  1.6847,  1.6847,  ...,  1.8198,  1.8047,  1.8047],\n",
      "         [ 1.6847,  1.6847,  1.6997,  ...,  1.8198,  1.8198,  1.8047],\n",
      "         [ 1.6997,  1.6997,  1.6997,  ...,  1.8198,  1.8198,  1.8198],\n",
      "         ...,\n",
      "         [-0.6565, -0.3714, -0.3714,  ..., -1.6470, -1.6771, -1.6921],\n",
      "         [-0.6115, -0.5065, -0.6115,  ..., -1.6320, -1.6921, -1.7071],\n",
      "         [-0.2063, -0.2663, -0.2663,  ..., -1.5420, -1.6020, -1.6170]],\n",
      "\n",
      "        [[ 1.8899,  1.9042,  1.9042,  ...,  2.0037,  2.0037,  1.9895],\n",
      "         [ 1.8757,  1.8899,  1.8899,  ...,  2.0037,  1.9895,  1.9895],\n",
      "         [ 1.8899,  1.8899,  1.8899,  ...,  1.9753,  1.9895,  1.9753],\n",
      "         ...,\n",
      "         [-0.8545, -0.7977, -0.8688,  ..., -1.3522, -1.4233, -1.4233],\n",
      "         [-0.8403, -0.7977, -0.8403,  ..., -1.3380, -1.4233, -1.4233],\n",
      "         [-0.5417, -0.6981, -0.7550,  ..., -1.3238, -1.3949, -1.4091]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eeg': tensor([[ 27.5612,  38.3106,  49.9205,  ...,   4.2898,  23.4379,  43.7045],\n",
      "        [ 14.4295,  20.5343,  26.5278,  ...,  23.3872,  18.6322,  14.4232],\n",
      "        [ 14.3318,  17.9328,  17.6227,  ..., -17.9592, -19.0506,  -3.5181],\n",
      "        ...,\n",
      "        [ 31.9147,  33.7406,  28.2354,  ...,  10.4832,  25.7123,  40.6025],\n",
      "        [ 22.2225,  29.0334,  27.0830,  ...,   5.0796,  18.4126,  23.0534],\n",
      "        [ 24.0264,  18.7102,  16.0720,  ...,   7.7126,   7.5124,  15.9575]]), 'image': tensor([[[-0.9926, -0.8678, -0.7397],\n",
      "         [-1.0000, -0.8673, -0.7534],\n",
      "         [-1.0000, -0.8696, -0.7604],\n",
      "         ...,\n",
      "         [-0.8184, -0.0179,  0.0332],\n",
      "         [-0.8278, -0.0178,  0.0154],\n",
      "         [-0.8078, -0.0396,  0.0280]],\n",
      "\n",
      "        [[-0.9960, -0.8746, -0.7548],\n",
      "         [-1.0000, -0.8703, -0.7606],\n",
      "         [-1.0000, -0.8687, -0.7624],\n",
      "         ...,\n",
      "         [-0.8155, -0.0254,  0.0270],\n",
      "         [-0.8096, -0.0237,  0.0192],\n",
      "         [-0.8098, -0.0354,  0.0297]],\n",
      "\n",
      "        [[-0.9960, -0.8715, -0.7537],\n",
      "         [-1.0000, -0.8703, -0.7647],\n",
      "         [-1.0000, -0.8703, -0.7647],\n",
      "         ...,\n",
      "         [-0.8168, -0.0238,  0.0240],\n",
      "         [-0.8084, -0.0225,  0.0233],\n",
      "         [-0.8118, -0.0281,  0.0244]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0000, -0.9091, -0.8140],\n",
      "         [-1.0000, -0.9030, -0.8180],\n",
      "         [-1.0000, -0.9059, -0.8189],\n",
      "         ...,\n",
      "         [-0.9364, -0.6533, -0.6584],\n",
      "         [-0.9315, -0.6453, -0.6573],\n",
      "         [-0.9246, -0.6471, -0.6589]],\n",
      "\n",
      "        [[-0.9979, -0.9038, -0.8057],\n",
      "         [-1.0000, -0.9059, -0.8139],\n",
      "         [-1.0000, -0.9059, -0.8161],\n",
      "         ...,\n",
      "         [-0.9393, -0.6493, -0.6590],\n",
      "         [-0.9293, -0.6508, -0.6664],\n",
      "         [-0.9278, -0.6450, -0.6568]],\n",
      "\n",
      "        [[-0.9961, -0.9019, -0.8103],\n",
      "         [-1.0000, -0.8958, -0.8155],\n",
      "         [-1.0000, -0.8963, -0.8227],\n",
      "         ...,\n",
      "         [-0.9553, -0.6448, -0.6598],\n",
      "         [-0.9463, -0.6370, -0.6633],\n",
      "         [-0.9432, -0.6432, -0.6615]]], dtype=torch.float64), 'image_raw': {'pixel_values': tensor([[[-1.7923, -1.7923, -1.7923,  ..., -1.3835, -1.3835, -1.3835],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.3835, -1.3835, -1.3835],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.3835, -1.3835, -1.3835],\n",
      "         ...,\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.2375, -1.1791, -1.1207],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.2959, -1.2375, -1.1791],\n",
      "         [-1.7923, -1.7923, -1.7923,  ..., -1.3397, -1.2813, -1.2229]],\n",
      "\n",
      "        [[-1.2268, -1.2268, -1.2118,  ...,  0.4090,  0.4390,  0.4841],\n",
      "         [-1.2118, -1.2118, -1.1968,  ...,  0.3790,  0.4240,  0.4390],\n",
      "         [-1.2118, -1.1968, -1.1968,  ...,  0.3490,  0.3640,  0.3640],\n",
      "         ...,\n",
      "         [-1.4519, -1.4369, -1.4369,  ..., -1.1818, -1.1968, -1.1968],\n",
      "         [-1.4519, -1.4669, -1.4519,  ..., -1.1818, -1.1818, -1.1968],\n",
      "         [-1.4519, -1.4519, -1.4519,  ..., -1.1968, -1.1968, -1.1968]],\n",
      "\n",
      "        [[-0.6128, -0.5986, -0.5844,  ...,  0.7097,  0.7239,  0.7523],\n",
      "         [-0.5986, -0.5844, -0.5844,  ...,  0.7097,  0.7097,  0.7097],\n",
      "         [-0.5844, -0.5844, -0.5701,  ...,  0.6670,  0.6670,  0.6528],\n",
      "         ...,\n",
      "         [-0.9683, -0.9683, -0.9541,  ..., -1.1247, -1.1389, -1.1532],\n",
      "         [-0.9683, -0.9683, -0.9683,  ..., -1.0963, -1.1247, -1.1247],\n",
      "         [-0.9683, -0.9683, -0.9683,  ..., -1.0963, -1.1105, -1.1247]]])}}\n",
      "rendering 2 examples in 150 steps.\n",
      "Data shape for PLMS sampling is (2, 4, 64, 64)\n",
      "Running PLMS Sampling with 167 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PLMS Sampler: 100%|| 167/167 [00:24<00:00,  6.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the Model some EEG data from the eeg14/eegData_npy and predict some images\n",
    "from torch.nn import Identity\n",
    "import random\n",
    "\n",
    "def transform_normalize(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = rearrange(img, 'h w c -> c h w')\n",
    "    img = torch.tensor(img)\n",
    "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
    "    return img\n",
    "\n",
    "def transform_channel_last(img):\n",
    "    if img.shape[-1] == 3:\n",
    "        return img\n",
    "    return rearrange(img, 'c h w -> h w c')\n",
    "\n",
    "crop_pix = int(0.2*512)\n",
    "img_transform_train = transforms.Compose([\n",
    "    transform_normalize,\n",
    "\n",
    "    transforms.Resize((512, 512)),\n",
    "    random_crop(512-crop_pix, p=0.5),\n",
    "\n",
    "    transforms.Resize((512, 512)),\n",
    "    transform_channel_last\n",
    "])\n",
    "\n",
    "img_transform_test = transforms.Compose([\n",
    "    transform_normalize,\n",
    "\n",
    "    transforms.Resize((512, 512)),\n",
    "    transform_channel_last\n",
    "])\n",
    "\n",
    "testingCheckpoint = 'phase1_gen/checkpoint_best.pth'\n",
    "eegModelPath = 'eeg14/checkpoint-4.pth'\n",
    "numEEG = 10\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def generate_images_T(generative_model, eeg_latents_dataset_test):\n",
    "    shuffled_dataset = list(eeg_latents_dataset_test)  # Convert to list if not already\n",
    "    random.shuffle(shuffled_dataset)\n",
    "\n",
    "    grid, samples = generative_model.generate(shuffled_dataset, 2, \n",
    "                150, None, numEEG, shouldSave = False)\n",
    "    grid_imgs = Image.fromarray(grid.astype(np.uint8))\n",
    "    grid_imgs.save(os.path.join('test-outputs',f'./samples_test.png'))\n",
    "    # for sp_idx, imgs in enumerate(samples):\n",
    "    #     for copy_idx, img in enumerate(imgs[0:]):\n",
    "    #         img = rearrange(img, 'c h w -> h w c')\n",
    "    #         Image.fromarray(img).save(os.path.join('test-outputs', \n",
    "    #                         f'./test{sp_idx}-{copy_idx}.png'))\n",
    "\n",
    "model_meta = torch.load(testingCheckpoint, map_location='cpu')\n",
    "pretrain_mbm_metafile = torch.load(eegModelPath, map_location='cpu')\n",
    "\n",
    "eeg_latents_dataset_train, eeg_latents_dataset_test = create_EEG_dataset( \n",
    "    image_transform=[img_transform_train, img_transform_test])\n",
    "num_voxels = eeg_latents_dataset_train.data_len\n",
    "\n",
    "generative_model = eLDM(pretrain_mbm_metafile, num_voxels,\n",
    "                device=device, pretrain_root='', logger=None, \n",
    "                ddim_steps=200, global_pool=False, use_time_cond=True, clip_tune = True, cls_tune = False)\n",
    "generative_model.model.load_state_dict(model_meta['model_state_dict'])\n",
    "\n",
    "# shuffle and choose a maximum of N elements\n",
    "\n",
    "generate_images_T(generative_model, eeg_latents_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "tensor([0.7580], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "model = FrozenImageEmbedder()\n",
    "\n",
    "image_path = 'dreamdiffusion/output/results/generation/10-12-2023-19-24-28/val/0_0/test1-1.png'\n",
    "\n",
    "# Open the image\n",
    "image = Image.open(image_path)\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "# for k, v in inputs.items():\n",
    "#     print(k)\n",
    "#     print(v.shape)\n",
    "# print()\n",
    "# print(inputs)\n",
    "\n",
    "outputs = model(inputs)\n",
    "# image_embeds = outputs.image_embeds\n",
    "print(outputs.shape)\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, CLIPTextModelWithProjection\n",
    "\n",
    "model_text = CLIPTextModelWithProjection.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "inputs_text = tokenizer([\"This image portrays a futuristic building with a sleek, modern design set against a backdrop that suggests a digital or virtual environment. The structure of the building is characterized by smooth curves and sharp edges, creating an interesting interplay of forms. Its design is unconventional, with parts that bulge and taper, suggesting advanced materials or construction techniques.\"], padding=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs_text = model_text(**inputs_text)\n",
    "text_embeds = outputs_text.text_embeds\n",
    "f = 1 - torch.cosine_similarity(outputs, text_embeds, dim=-1)\n",
    "print(f)\n",
    "\n",
    "#image_embeds = outputs / outputs.norm(p=2, dim=-1, keepdim=True)\n",
    "#text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "#print(outputs)\n",
    "#count_params(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
