{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZh0Ue-4o3CN",
        "outputId": "1ae65e09-dfb0-4edb-ade5-83817c34c76d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#@title Mount drive\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mcd\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/content/drive/My\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m Drive/eegdataset/\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#@title Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/eeg14/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07X_s3P1lZFV",
        "outputId": "e565e2ef-0dba-4f17-d769-bd60e263c640"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (14, 1023)\n",
            "Data Type: float64\n",
            "First few entries: [[ 2.5128431   1.25575384  8.54326063 ...  3.64350271 -4.44142088\n",
            "   2.04845304]\n",
            " [ 2.65274032 -1.18762717  4.06729127 ...  9.65386276  1.67943305\n",
            "   7.24508189]\n",
            " [ 3.29877056  0.47304377  2.11378861 ... 16.07193351 10.95105215\n",
            "   4.85196649]\n",
            " ...\n",
            " [-8.83621691 -5.65723093  7.2717154  ... 23.65754568 21.23433721\n",
            "   7.34318116]\n",
            " [-2.14915252 -3.11921504  2.6448006  ... 15.32879631 10.80656742\n",
            "  -0.65058027]\n",
            " [ 9.59863749 10.14065447 18.5562873  ... 18.9543754  22.2681214\n",
            "   7.37278267]]\n"
          ]
        }
      ],
      "source": [
        "#@title Check the format of the data from eeg files\n",
        "import numpy as np\n",
        "data = np.load('../../../data/processed/eegData_npy/1.npy')\n",
        "\n",
        "# Print its attributes\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"Data Type:\", data.dtype)\n",
        "print(\"First few entries:\", data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: einops in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (0.7.0)\n",
            "Requirement already satisfied: transformers in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
            "Requirement already satisfied: filelock in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: requests in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install einops transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t2NckeKMWZ16"
      },
      "outputs": [],
      "source": [
        "#!pip install --extra-index-url https://download.pytorch.org/whl/cu116 torch==1.12.1 torchvision==0.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "48GYiAlBBBN7"
      },
      "outputs": [],
      "source": [
        "#@title Handle dataset related tasks scripts:\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import interpolate\n",
        "from einops import rearrange\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import torchvision.transforms as transforms\n",
        "from scipy.interpolate import interp1d\n",
        "from typing import Callable, Optional, Tuple, Union\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import pickle\n",
        "\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "def pad_to_patch_size(x, patch_size):\n",
        "    assert x.ndim == 2\n",
        "    return np.pad(x, ((0,0),(0, patch_size-x.shape[1]%patch_size)), 'wrap')\n",
        "\n",
        "def pad_to_length(x, length):\n",
        "    assert x.ndim == 3\n",
        "    assert x.shape[-1] <= length\n",
        "    if x.shape[-1] == length:\n",
        "        return x\n",
        "\n",
        "    return np.pad(x, ((0,0),(0,0), (0, length - x.shape[-1])), 'wrap')\n",
        "\n",
        "def normalize(x, mean=None, std=None):\n",
        "    mean = np.mean(x) if mean is None else mean\n",
        "    std = np.std(x) if std is None else std\n",
        "    return (x - mean) / (std * 1.0)\n",
        "\n",
        "def img_norm(img):\n",
        "    if img.shape[-1] == 3:\n",
        "        img = rearrange(img, 'h w c -> c h w')\n",
        "    img = torch.tensor(img)\n",
        "    img = (img / 255.0) * 2.0 - 1.0 # to -1 ~ 1\n",
        "    return img\n",
        "\n",
        "def channel_first(img):\n",
        "        if img.shape[-1] == 3:\n",
        "            return rearrange(img, 'h w c -> c h w')\n",
        "        return img\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def file_ext(name: Union[str, Path]) -> str:\n",
        "    return str(name).split('.')[-1]\n",
        "\n",
        "def is_npy_ext(fname: Union[str, Path]) -> bool:\n",
        "    ext = file_ext(fname).lower()\n",
        "    return f'{ext}' == 'npy'# type: ignore\n",
        "\n",
        "class eeg_pretrain_dataset(Dataset):\n",
        "    def __init__(self, path='../../../data/processed/eegData_npy/', roi='VC', patch_size=16, transform=identity, aug_times=2,\n",
        "                num_sub_limit=None, include_kam=False, include_hcp=True):\n",
        "        super(eeg_pretrain_dataset, self).__init__()\n",
        "        data = []\n",
        "        images = []\n",
        "        self.input_paths = [str(f) for f in sorted(Path(path).rglob('*')) if is_npy_ext(f) and os.path.isfile(f)]\n",
        "\n",
        "        assert len(self.input_paths) != 0, 'No data found'\n",
        "        self.data_len  = 1024\n",
        "        self.data_chan = 14\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_path = self.input_paths[index]\n",
        "\n",
        "        data = np.load(data_path)\n",
        "\n",
        "        if data.shape[-1] > self.data_len:\n",
        "            idx = np.random.randint(0, int(data.shape[-1] - self.data_len)+1)\n",
        "\n",
        "            data = data[:, idx: idx+self.data_len]\n",
        "        else:\n",
        "            x = np.linspace(0, 1, data.shape[-1])\n",
        "            x2 = np.linspace(0, 1, self.data_len)\n",
        "            f = interp1d(x, data)\n",
        "            data = f(x2)\n",
        "        ret = np.zeros((self.data_chan, self.data_len))\n",
        "        if (self.data_chan > data.shape[-2]):\n",
        "            for i in range((self.data_chan//data.shape[-2])):\n",
        "\n",
        "                ret[i * data.shape[-2]: (i+1) * data.shape[-2], :] = data\n",
        "            if self.data_chan % data.shape[-2] != 0:\n",
        "\n",
        "                ret[ -(self.data_chan%data.shape[-2]):, :] = data[: (self.data_chan%data.shape[-2]), :]\n",
        "        elif(self.data_chan < data.shape[-2]):\n",
        "            idx2 = np.random.randint(0, int(data.shape[-2] - self.data_chan)+1)\n",
        "            ret = data[idx2: idx2+self.data_chan, :]\n",
        "        # print(ret.shape)\n",
        "        elif(self.data_chan == data.shape[-2]):\n",
        "            ret = data\n",
        "        ret = ret/10 # reduce an order\n",
        "        # torch.tensor()\n",
        "        ret = torch.from_numpy(ret).float()\n",
        "        return {'eeg': ret } #,\n",
        "\n",
        "class base_dataset(Dataset):\n",
        "    def __init__(self, x, y=None, transform=identity):\n",
        "        super(base_dataset, self).__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        if self.y is None:\n",
        "            return self.transform(self.x[index])\n",
        "        else:\n",
        "            return self.transform(self.x[index]), self.transform(self.y[index])\n",
        "\n",
        "def remove_repeats(fmri, img_lb):\n",
        "    assert len(fmri) == len(img_lb), 'len error'\n",
        "    fmri_dict = {}\n",
        "    for f, lb in zip(fmri, img_lb):\n",
        "        if lb in fmri_dict.keys():\n",
        "            fmri_dict[lb].append(f)\n",
        "        else:\n",
        "            fmri_dict[lb] = [f]\n",
        "    lbs = []\n",
        "    fmris = []\n",
        "    for k, v in fmri_dict.items():\n",
        "        lbs.append(k)\n",
        "        fmris.append(np.mean(np.stack(v), axis=0))\n",
        "    return np.stack(fmris), lbs\n",
        "\n",
        "\n",
        "def list_get_all_index(list, value):\n",
        "    return [i for i, v in enumerate(list) if v == value]\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class EEGDataset_r(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, image_transform=identity):\n",
        "\n",
        "        self.imagesource = 'eegData_images'\n",
        "        self.image_transform = image_transform\n",
        "        self.num_voxels = 440\n",
        "        self.data_len = 1024\n",
        "        # # Compute size\n",
        "        self.size = 100\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Process EEG\n",
        "        eeg = torch.randn(14,1024)\n",
        "\n",
        "        # print(image.shape)\n",
        "        label = torch.tensor(0).long()\n",
        "        image = torch.randn(3,675,675)\n",
        "        image_raw = image\n",
        "\n",
        "        return {'eeg': eeg, 'label': label, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
        "\n",
        "\n",
        "class EEGDataset_s(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, image_transform, eeg_signals_path):\n",
        "        # Load EEG signals\n",
        "        loaded = torch.load(eeg_signals_path)\n",
        "        # if opt.subject!=0:\n",
        "        #     self.data = [loaded['dataset'][i] for i in range(len(loaded['dataset']) ) if loaded['dataset'][i]['subject']==opt.subject]\n",
        "        # else:\n",
        "        self.eeg = loaded['dataset']\n",
        "        self.labels = loaded[\"labels\"]\n",
        "        self.images = loaded[\"images\"]\n",
        "        self.imagesource = '../../../data/processed/eegData_images'\n",
        "        self.image_transform = image_transform\n",
        "        self.num_voxels = 1024\n",
        "        # Compute size\n",
        "        self.size = len(self.data)\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Process EEG\n",
        "        eeg = self.data[i][\"eeg\"].float().t()\n",
        "\n",
        "        # Get label\n",
        "        image_name = self.images[self.data[i][\"image\"]]\n",
        "        # image_path = os.path.join(self.imagenet, image_name.split('_')[0], image_name+'.JPEG')\n",
        "        return image_name\n",
        "\n",
        "\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self, image_transform=identity):\n",
        "        eeg_dir = 'eegData_npy'\n",
        "        self.eeg = [np.load(os.path.join(eeg_dir, f'{i+1}.npy')) for i in range(len(os.listdir(eeg_dir)))]\n",
        "\n",
        "        images_dir = '../../../data/processed/eegData_images'\n",
        "        self.images = [np.load(os.path.join(images_dir, f'{i+1}.npy')) for i in range(len(os.listdir(images_dir)))]\n",
        "\n",
        "        #self.labels = loaded[\"labels\"]\n",
        "        self.imagesource = '../../../data/processed/eegData_images'\n",
        "        self.image_transform = image_transform\n",
        "        self.num_voxels = 1024\n",
        "        self.data_len = 1024\n",
        "        # Compute size\n",
        "        self.size = len(self.eeg)\n",
        "        self.processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        # Process EEG\n",
        "        eeg = self.eeg[i].float().t()\n",
        "\n",
        "        ##### 2023 2 13 add preprocess and transpose\n",
        "        eeg = np.array(eeg.transpose(0,1))\n",
        "        x = np.linspace(0, 1, eeg.shape[-1])\n",
        "        x2 = np.linspace(0, 1, self.data_len)\n",
        "        f = interp1d(x, eeg)\n",
        "        eeg = f(x2)\n",
        "        eeg = torch.from_numpy(eeg).float()\n",
        "        ##### 2023 2 13 add preprocess\n",
        "        image_raw = Image.open(self.images[i]).convert('RGB')\n",
        "\n",
        "        image = np.array(image_raw) / 255.0\n",
        "        image_raw = self.processor(images=image_raw, return_tensors=\"pt\")\n",
        "        image_raw['pixel_values'] = image_raw['pixel_values'].squeeze(0)\n",
        "\n",
        "\n",
        "        return {'eeg': eeg, 'image': self.image_transform(image), 'image_raw': image_raw}\n",
        "        # return eeg, label?\n",
        "\n",
        "class Splitter:\n",
        "\n",
        "    def __init__(self, dataset, split_name=\"train\"):\n",
        "        # Set EEG dataset\n",
        "        self.dataset = dataset\n",
        "        \n",
        "        # Compute the indices for the split based on the percentage\n",
        "        total_size = len(self.dataset.eeg)  # Changed to 'eeg'\n",
        "        split_index = int(total_size * 0.8)  # 80% for training\n",
        "        \n",
        "        if split_name == \"train\":\n",
        "            self.split_idx = list(range(split_index))  # First 80%\n",
        "        elif split_name == \"test\":\n",
        "            self.split_idx = list(range(split_index, total_size))  # Remaining 20%\n",
        "        else:\n",
        "            raise ValueError(\"Invalid split_name. Expected 'train' or 'test'.\")\n",
        "\n",
        "        # Compute size\n",
        "        self.size = len(self.split_idx)\n",
        "        self.num_voxels = 1024\n",
        "        self.data_len = 1024\n",
        "\n",
        "    # Get size\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "    # Get item\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset[self.split_idx[i]]\n",
        "\n",
        "\n",
        "def create_EEG_dataset(image_transform=identity, subject = 0):\n",
        "    if isinstance(image_transform, list):\n",
        "        dataset_train = EEGDataset(image_transform[0] )\n",
        "        dataset_test = EEGDataset(image_transform[1])\n",
        "    else:\n",
        "        dataset_train = EEGDataset(image_transform)\n",
        "        dataset_test = EEGDataset(image_transform)\n",
        "    split_train = Splitter(dataset_train, split_name = 'train')\n",
        "    split_test = Splitter(dataset_test, split_name = 'test')\n",
        "    return (split_train, split_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_EEG_dataset_r(\n",
        "            image_transform=identity):\n",
        "    if isinstance(image_transform, list):\n",
        "        dataset_train = EEGDataset_r(image_transform[0])\n",
        "        dataset_test = EEGDataset_r(image_transform[1])\n",
        "    else:\n",
        "        dataset_train = EEGDataset_r(image_transform)\n",
        "        dataset_test = EEGDataset_r(image_transform)\n",
        "    return (dataset_train,dataset_test)\n",
        "\n",
        "class random_crop:\n",
        "    def __init__(self, size, p):\n",
        "        self.size = size\n",
        "        self.p = p\n",
        "    def __call__(self, img):\n",
        "        if torch.rand(1) < self.p:\n",
        "            return transforms.RandomCrop(size=(self.size, self.size))(img)\n",
        "        return img\n",
        "def normalize2(img):\n",
        "    if img.shape[-1] == 3:\n",
        "        img = rearrange(img, 'h w c -> c h w')\n",
        "    img = torch.tensor(img)\n",
        "    img = img * 2.0 - 1.0 # to -1 ~ 1\n",
        "    return img\n",
        "def channel_last(img):\n",
        "        if img.shape[-1] == 3:\n",
        "            return img\n",
        "        return rearrange(img, 'c h w -> h w c')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNjJZQcAH0Q8",
        "outputId": "22aa504b-691a-4c11-e387-529123d6d41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm==0.5.4 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (0.5.4)\n",
            "Requirement already satisfied: torch>=1.4 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from timm==0.5.4) (2.0.1)\n",
            "Requirement already satisfied: torchvision in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from timm==0.5.4) (0.15.2a0)\n",
            "Requirement already satisfied: filelock in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4) (4.7.1)\n",
            "Requirement already satisfied: sympy in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4) (1.11.1)\n",
            "Requirement already satisfied: networkx in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torch>=1.4->timm==0.5.4) (3.1.2)\n",
            "Requirement already satisfied: numpy in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (1.24.3)\n",
            "Requirement already satisfied: requests in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from torchvision->timm==0.5.4) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.4->timm==0.5.4) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from requests->torchvision->timm==0.5.4) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/alindumitru/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.4->timm==0.5.4) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install timm==0.5.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "UQJJ2aJFHBzr"
      },
      "outputs": [],
      "source": [
        "#@title MAE for EEG part\n",
        "\n",
        "# utils\n",
        "import math\n",
        "import os\n",
        "\n",
        "def get_1d_sincos_pos_embed(embed_dim, length, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_l = np.arange(length, dtype=np.float32)\n",
        "\n",
        "    grid_l = grid_l.reshape([1, length])\n",
        "    pos_embed = get_1d_sincos_pos_embed_from_grid(embed_dim, grid_l)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float64)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# Interpolate position embeddings for high-resolution\n",
        "# References:\n",
        "# DeiT: https://github.com/facebookresearch/deit\n",
        "# --------------------------------------------------------\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches # cls token\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int(pos_embed_checkpoint.shape[-2] - num_extra_tokens)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %d to %d\" % (orig_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, embedding_size).permute(0, 2, 1)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size))\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 1)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed\n",
        "\n",
        "\n",
        "\n",
        "def original_schedule(epoch, config):\n",
        "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
        "    if epoch < config.warmup_epochs:\n",
        "        lr = config.lr * epoch / config.warmup_epochs\n",
        "    else:\n",
        "        lr = config.min_lr + (config.lr - config.min_lr) * 0.5 * \\\n",
        "            (1. + math.cos(math.pi * (epoch - config.warmup_epochs) / (config.num_epoch - config.warmup_epochs)))\n",
        "    return lr\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, config):\n",
        "    # Define new schedule parameters\n",
        "    cycle_length = 50\n",
        "    max_lr = config.lr\n",
        "    min_lr = config.lr * 0.1\n",
        "\n",
        "    # Blending period\n",
        "    blending_epochs = 20\n",
        "    start_blending_epoch = 1000\n",
        "    end_blending_epoch = start_blending_epoch + blending_epochs\n",
        "\n",
        "    if epoch < start_blending_epoch:\n",
        "        lr = original_schedule(epoch, config)\n",
        "    elif start_blending_epoch <= epoch < end_blending_epoch:\n",
        "        blend_weight = (epoch - start_blending_epoch) / blending_epochs\n",
        "        lr_old = original_schedule(epoch, config)\n",
        "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
        "        lr_new = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * cycle_progress))\n",
        "        lr = (1 - blend_weight) * lr_old + blend_weight * lr_new\n",
        "    else:\n",
        "        # New schedule\n",
        "        cycle_progress = (epoch % cycle_length) / cycle_length\n",
        "        lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * cycle_progress))\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if \"lr_scale\" in param_group:\n",
        "            param_group[\"lr\"] = lr * param_group[\"lr_scale\"]\n",
        "        else:\n",
        "            param_group[\"lr\"] = lr\n",
        "    return lr\n",
        "\n",
        "\n",
        "def save_model(config, epoch, model, optimizer, loss_scaler, checkpoint_paths):\n",
        "    os.makedirs(checkpoint_paths, exist_ok=True)\n",
        "    to_save = {\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'scaler': loss_scaler.state_dict(),\n",
        "        'config': config,\n",
        "    }\n",
        "    torch.save(to_save, os.path.join(checkpoint_paths, 'checkpoint.pth'))\n",
        "\n",
        "\n",
        "def load_model(config, model, checkpoint_path ):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    print(f'Model loaded with {checkpoint_path}')\n",
        "\n",
        "def patchify(imgs, patch_size):\n",
        "    \"\"\"\n",
        "    imgs: (N, 1, num_voxels)\n",
        "    x: (N, L, patch_size)\n",
        "    \"\"\"\n",
        "    p = patch_size\n",
        "    assert imgs.ndim == 3 and imgs.shape[2] % p == 0\n",
        "\n",
        "    h = imgs.shape[2] // p\n",
        "    x = imgs.reshape(shape=(imgs.shape[0], h, p))\n",
        "    return x\n",
        "\n",
        "def unpatchify(x, patch_size):\n",
        "    \"\"\"\n",
        "    x: (N, L, patch_size)\n",
        "    imgs: (N, 1, num_voxels)\n",
        "    \"\"\"\n",
        "    p = patch_size\n",
        "    h = x.shape[1]\n",
        "\n",
        "    imgs = x.reshape(shape=(x.shape[0], 1, h * p))\n",
        "    return imgs\n",
        "\n",
        "import sys\n",
        "#sys.path.append('../dreamdiffusion/code/')\n",
        "# print(sys.path)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from timm.models.vision_transformer import Block\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# class CustomBlock(nn.Module):\n",
        "#     def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, dropout_rate=0.2, attn_dropout_rate=0.2, norm_layer=nn.LayerNorm):\n",
        "#         super().__init__()\n",
        "#         # Multi-head self-attention\n",
        "#         self.attn = nn.MultiheadAttention(dim, num_heads, dropout=attn_dropout_rate)\n",
        "\n",
        "#         # Layer Norm\n",
        "#         self.norm1 = norm_layer(dim)\n",
        "#         self.norm2 = norm_layer(dim)\n",
        "\n",
        "#         # Feed-forward network\n",
        "#         self.mlp = nn.Sequential(\n",
        "#             nn.Linear(dim, int(dim * mlp_ratio)),\n",
        "#             nn.GELU(),\n",
        "#             nn.Dropout(dropout_rate),\n",
        "#             nn.Linear(int(dim * mlp_ratio), dim),\n",
        "#             nn.Dropout(dropout_rate)\n",
        "#         )\n",
        "#         self.norm1 = nn.LayerNorm(dim)\n",
        "#         self.norm2 = nn.LayerNorm(dim)\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.dropout(self.attn(self.norm1(x), x, x)[0])\n",
        "#         x = x + self.dropout(self.mlp(self.norm2(x)))\n",
        "#         return x\n",
        "\n",
        "class PatchEmbed1D(nn.Module):\n",
        "    \"\"\" 1 Dimensional version of data (fmri voxels) to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, time_len=224, patch_size=1, in_chans=14, embed_dim=256, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        num_patches = time_len // patch_size\n",
        "        self.patch_shape = patch_size\n",
        "        self.time_len = time_len\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        B, C, V = x.shape # batch, channel, voxels\n",
        "        # assert V == self.num_voxels, \\\n",
        "        #     f\"Input fmri length ({V}) doesn't match model ({self.num_voxels}).\"\n",
        "        x = self.proj(x).transpose(1, 2).contiguous() # put embed_dim at the last dimension\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class MAEforEEG(nn.Module):\n",
        "    \"\"\" Masked Autoencoder with VisionTransformer backbone\n",
        "    \"\"\"\n",
        "    def __init__(self, time_len=1024, patch_size=4, embed_dim=1024, in_chans=14,\n",
        "                 depth=24, num_heads=16, decoder_embed_dim=512,\n",
        "                 decoder_depth=4, decoder_num_heads=16,\n",
        "                 mlp_ratio=2., norm_layer=nn.LayerNorm, focus_range=None, focus_rate=None, img_recon_weight=1.0,\n",
        "                 use_nature_img_loss=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE encoder specifics\n",
        "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
        "\n",
        "        num_patches = int(time_len / patch_size)\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.115)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # --------------------------------------------------------------------------\n",
        "        # MAE decoder specifics\n",
        "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.15)\n",
        "            for i in range(decoder_depth)])\n",
        "\n",
        "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
        "        self.decoder_pred = nn.Linear(decoder_embed_dim, in_chans * patch_size, bias=True) # encoder to decoder\n",
        "        # --------------------------------------------------------------------------\n",
        "\n",
        "        # nature image decoder specifics\n",
        "        if use_nature_img_loss:\n",
        "            self.nature_img_decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
        "\n",
        "            self.nature_img_mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
        "\n",
        "            self.nature_img_decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "            self.nature_img_decoder_blocks = nn.ModuleList([\n",
        "                Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer, proj_drop=0.15, attn_drop=0.15)\n",
        "                for i in range(2)])\n",
        "\n",
        "            self.nature_img_decoder_norm = norm_layer(decoder_embed_dim)\n",
        "            self.nature_img_decoder_pred = nn.Sequential(\n",
        "                nn.Conv1d(num_patches, 512, kernel_size=1, stride=1, bias=True),\n",
        "                nn.Linear(decoder_embed_dim, 28*28, bias=True)\n",
        "            )\n",
        "            # --------------------------------------------------------------------------\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.focus_range = focus_range\n",
        "        self.focus_rate = focus_rate\n",
        "        self.img_recon_weight = img_recon_weight\n",
        "        self.use_nature_img_loss = use_nature_img_loss\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
        "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        if self.use_nature_img_loss:\n",
        "            nature_img_decoder_pos_embed = get_1d_sincos_pos_embed(self.nature_img_decoder_pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
        "            self.nature_img_decoder_pos_embed.data.copy_(torch.from_numpy(nature_img_decoder_pos_embed).float().unsqueeze(0))\n",
        "            torch.nn.init.normal_(self.nature_img_mask_token, std=.02)\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "    def patchify(self, imgs):\n",
        "        \"\"\"\n",
        "        imgs: (N, 1, num_voxels)\n",
        "        imgs: [N, chan, T]\n",
        "        x: (N, L, patch_size)\n",
        "        x: [N, chan * 4, T/4]\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size\n",
        "        assert imgs.ndim == 3 and imgs.shape[1] % p == 0\n",
        "\n",
        "        # h = imgs.shape[2] // p\n",
        "        x = imgs.reshape(shape=(imgs.shape[0], imgs.shape[1] // p, -1))\n",
        "        return x\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, L, patch_size)\n",
        "        imgs: (N, 1, num_voxels)\n",
        "        \"\"\"\n",
        "        p = self.patch_embed.patch_size\n",
        "        h = x.shape[1]\n",
        "\n",
        "        imgs = x.reshape(shape=(x.shape[0], -1, x.shape[2] // p))\n",
        "        return imgs.transpose(1,2)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio):\n",
        "        \"\"\"\n",
        "        Perform per-sample random masking by per-sample shuffling.\n",
        "        Per-sample shuffling is done by argsort random noise.\n",
        "        x: [N, L, D], sequence\n",
        "        \"\"\"\n",
        "        N, L, D = x.shape  # batch, length, dim\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        if self.focus_range is not None:\n",
        "            len_mask = L - len_keep\n",
        "            weights = [1-self.focus_rate] * L\n",
        "            weights[self.focus_range[0] // self.patch_size : self.focus_range[1] // self.patch_size\n",
        "                        ] = [self.focus_rate] * (self.focus_range[1] // self.patch_size - self.focus_range[0] // self.patch_size)\n",
        "            weights = torch.tensor(weights).repeat(N, 1).to(x.device)\n",
        "            ids_mask = torch.multinomial(weights, len_mask, replacement=False)\n",
        "\n",
        "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
        "        if self.focus_range is not None:\n",
        "            for i in range(N):\n",
        "                noise[i, ids_mask[i,:]] = 1.1  # set mask portion to 1.1\n",
        "\n",
        "        # sort noise for each sample\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        # keep the first subset\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
        "\n",
        "        # generate the binary mask: 0 is keep, 1 is remove\n",
        "        mask = torch.ones([N, L], device=x.device)\n",
        "        mask[:, :len_keep] = 0\n",
        "        # unshuffle to get the binary mask\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward_encoder(self, x, mask_ratio):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "        # print('encoder embed')\n",
        "        # print(x.shape)\n",
        "        # masking: length -> length * mask_ratio\n",
        "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
        "\n",
        "        # append cls token\n",
        "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
        "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        return x, mask, ids_restore\n",
        "\n",
        "    def forward_decoder(self, x, ids_restore = None):\n",
        "        # embed tokens\n",
        "        x = self.decoder_embed(x)\n",
        "        # print('decoder embed')\n",
        "        # print(x.shape)\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        # x_ = torch.cat([x, mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "        # x = x_\n",
        "        # add pos embed\n",
        "        x = x + self.decoder_pos_embed\n",
        "        # x = x + self.decoder_pos_embed[:, 1:, :]\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.decoder_norm(x)\n",
        "        # print(x.shape)\n",
        "        # predictor projection\n",
        "        x = self.decoder_pred(x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_nature_img_decoder(self, x, ids_restore):\n",
        "        # embed tokens\n",
        "        x = self.nature_img_decoder_embed(x)\n",
        "\n",
        "        # append mask tokens to sequence\n",
        "        mask_tokens = self.nature_img_mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
        "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
        "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
        "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
        "\n",
        "        # add pos embed\n",
        "        x = x + self.nature_img_decoder_pos_embed\n",
        "\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.nature_img_decoder_blocks:\n",
        "            x = blk(x)\n",
        "        x = self.nature_img_decoder_norm(x)\n",
        "        # remove cls token\n",
        "        x = x[:, 1:, :]\n",
        "        # predictor projection\n",
        "        # x = x.mean(dim=1, keepdim=True)\n",
        "        x = self.nature_img_decoder_pred(x)\n",
        "        x = x.view(x.shape[0], 512, 28, 28)\n",
        "\n",
        "        return x # n, 512, 28, 28\n",
        "\n",
        "    def forward_nature_img_loss(self, inputs, reconstructions):\n",
        "        loss = ((torch.tanh(inputs) - torch.tanh(reconstructions))**2).mean()\n",
        "        if torch.isnan(reconstructions).sum():\n",
        "            print('nan in reconstructions')\n",
        "        if torch.isnan(inputs).sum():\n",
        "            print('nan in inputs')\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward_loss(self, imgs, pred, mask):\n",
        "        \"\"\"\n",
        "        imgs: [N, 1, num_voxels]\n",
        "        imgs: [N, chan, T]\n",
        "        pred: [N, L, p]\n",
        "        mask: [N, L], 0 is keep, 1 is remove,\n",
        "        \"\"\"\n",
        "        imgs = imgs.transpose(1,2)\n",
        "        target = self.patchify(imgs)\n",
        "        # target = imgs.transpose(1,2)\n",
        "        loss = (pred - target) ** 2\n",
        "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
        "        # loss = loss.mean()\n",
        "        loss = (loss * mask).sum() / mask.sum()  if mask.sum() != 0 else (loss * mask).sum() # mean loss on removed patches\n",
        "        return loss\n",
        "\n",
        "    def forward(self, imgs, img_features=None, valid_idx=None, mask_ratio=0.75):\n",
        "        # latent = self.forward_encoder(imgs, mask_ratio)\n",
        "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
        "            # print(x)\n",
        "        # print(latent.shape)\n",
        "        # # print(mask)\n",
        "        # print(mask.shape)\n",
        "        # # print(ids_restore)\n",
        "        # print(ids_restore.shape)\n",
        "\n",
        "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p]\n",
        "        # pred = self.forward_decoder(latent)  # [N, L, p]\n",
        "        # pred = pred\n",
        "        # print(pred.shape)\n",
        "        # mask=None\n",
        "        loss = self.forward_loss(imgs, pred, mask)\n",
        "        # print(self.unpatchify(pred.transpose(1,2)).shape)\n",
        "\n",
        "        if self.use_nature_img_loss and img_features is not None:\n",
        "            # valid_idx = torch.nonzero(nature_image.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
        "            if len(valid_idx) != 0:\n",
        "                nature_image_recon = self.forward_nature_img_decoder(latent[valid_idx], ids_restore[valid_idx])\n",
        "                loss_nature_image_recon = self.forward_nature_img_loss(img_features, nature_image_recon)\n",
        "                if torch.isnan(loss_nature_image_recon).sum():\n",
        "                    print(loss_nature_image_recon)\n",
        "                    print(\"loss_nature_image_recon is nan\")\n",
        "\n",
        "                loss = loss + self.img_recon_weight*loss_nature_image_recon\n",
        "\n",
        "        return loss, pred, mask\n",
        "\n",
        "class eeg_encoder(nn.Module):\n",
        "    def __init__(self, time_len=1024, patch_size=4, embed_dim=2048, in_chans=14,\n",
        "                 depth=24, num_heads=16, mlp_ratio=1., norm_layer=nn.LayerNorm, global_pool=False):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed1D(time_len, patch_size, in_chans, embed_dim)\n",
        "\n",
        "        num_patches = int(time_len / patch_size)\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(embed_dim, num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer)\n",
        "            for i in range(depth)])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "        self.global_pool = global_pool\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # initialization\n",
        "        # initialize (and freeze) pos_embed by sin-cos embedding\n",
        "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.num_patches, cls_token=True)\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # initialize patch_embed like nn.Linear (instead of nn.Conv2d)\n",
        "        w = self.patch_embed.proj.weight.data\n",
        "        torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n",
        "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
        "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
        "        # initialize nn.Linear and nn.LayerNorm\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            # we use xavier_uniform following official JAX ViT:\n",
        "            torch.nn.init.xavier_uniform_(m.weight)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "        elif isinstance(m, nn.Conv1d):\n",
        "            torch.nn.init.normal_(m.weight, std=.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_encoder(self, x):\n",
        "        # embed patches\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # add pos embed w/o cls token\n",
        "        # print(x.shape)\n",
        "        # print(self.pos_embed[:, 1:, :].shape)\n",
        "        x = x + self.pos_embed[:, 1:, :]\n",
        "        # apply Transformer blocks\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        # print(x.shape)\n",
        "        if self.global_pool:\n",
        "            x = x.mean(dim=1, keepdim=True)\n",
        "        # print(x.shape)\n",
        "        x = self.norm(x)\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        if imgs.ndim == 2:\n",
        "            imgs = torch.unsqueeze(imgs, dim=0)  # N, n_seq, embed_dim\n",
        "        latent = self.forward_encoder(imgs) # N, n_seq, embed_dim\n",
        "        return latent # N, n_seq, embed_dim\n",
        "\n",
        "    def load_checkpoint(self, state_dict):\n",
        "        if self.global_pool:\n",
        "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k and 'norm' not in k)}\n",
        "        else:\n",
        "            state_dict = {k: v for k, v in state_dict.items() if ('mask_token' not in k)}\n",
        "        interpolate_pos_embed(self, state_dict)\n",
        "\n",
        "        m, u = self.load_state_dict(state_dict, strict=False)\n",
        "        print('missing keys:', u)\n",
        "        print('unexpected keys:', m)\n",
        "        return\n",
        "\n",
        "class classify_network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.Conv1d(14, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(1024, 40)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(x)\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class mapping(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.maxpool = nn.Conv1d(14, 1, 1, stride=1)#nn.AdaptiveAvgPool1d((1))\n",
        "        self.fc = nn.Linear(1024, 768)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.maxpool(x)\n",
        "        x = x.squeeze(1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "Cf55A0hwUd_r"
      },
      "outputs": [],
      "source": [
        "#@title Configs\n",
        "class Config_MBM_finetune: # back compatibility\n",
        "    pass\n",
        "\n",
        "class Config_MAE_fMRI: # back compatibility\n",
        "    pass\n",
        "\n",
        "class Config_MBM_EEG(Config_MAE_fMRI):\n",
        "    # configs for fmri_pretrain.py\n",
        "    def __init__(self):\n",
        "    # --------------------------------------------\n",
        "    # MAE for fMRI\n",
        "        # Training Parameters\n",
        "        #self.lr = 2.5e-4 - initial\n",
        "        self.lr = 1e-4\n",
        "        self.min_lr = 0.\n",
        "        self.weight_decay = 0.15\n",
        "        self.num_epoch = 3000\n",
        "        self.warmup_epochs = 40\n",
        "        self.batch_size = 16\n",
        "        self.clip_grad = 0.8\n",
        "\n",
        "        # Model Parameters\n",
        "        self.mask_ratio = 0.35\n",
        "       # self.patch_size = 8 #  1\n",
        "        self.embed_dim = 1024 #256 # has to be a multiple of num_heads\n",
        "        self.decoder_embed_dim = 512 #128\n",
        "        self.depth = 24\n",
        "        #self.depth = 12\n",
        "        self.num_heads = 8\n",
        "        self.decoder_num_heads = 8\n",
        "        self.mlp_ratio = 1.0\n",
        "\n",
        "        # Project setting\n",
        "        self.root_path = 'processed/'\n",
        "        self.output_path = 'processed/output'\n",
        "        self.seed = 2022\n",
        "        self.roi = 'VC'\n",
        "        self.aug_times = 1\n",
        "        self.num_sub_limit = None\n",
        "        self.include_hcp = True\n",
        "        self.include_kam = True\n",
        "        self.accum_iter = 1\n",
        "\n",
        "        self.use_nature_img_loss = False\n",
        "        self.img_recon_weight = 0.5\n",
        "        self.focus_range = None # [0, 1500] # None to disable it\n",
        "        self.focus_rate = 0.6\n",
        "        self.patch_size = 4\n",
        "\n",
        "        # distributed training\n",
        "        self.local_rank = 0\n",
        "\n",
        "\n",
        "class Config_EEG_finetune(Config_MBM_finetune):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Project setting\n",
        "        self.root_path = '/processed/'\n",
        "        self.output_path = '/processed/output'\n",
        "\n",
        "        self.dataset = 'EEG'\n",
        "        #self.pretrain_mbm_path = '../content/drive/My Drive/eegdataset/eeg_pretrain/checkpoint.pth'\n",
        "\n",
        "        self.include_nonavg_test = True\n",
        "\n",
        "\n",
        "        # Training Parameters\n",
        "        self.lr = 5.3e-5\n",
        "        self.weight_decay = 0.05\n",
        "        self.num_epoch = 15\n",
        "        self.batch_size = 16 if self.dataset == 'GOD' else 4\n",
        "        self.mask_ratio = 0.5\n",
        "        self.accum_iter = 1\n",
        "        self.clip_grad = 0.8\n",
        "        self.warmup_epochs = 2\n",
        "        self.min_lr = 0.\n",
        "        self.use_nature_img_loss = False\n",
        "        self.img_recon_weight = 0.5\n",
        "        self.focus_range = None # [0, 1500] # None to disable it\n",
        "        self.focus_rate = 0.6\n",
        "\n",
        "        # distributed training\n",
        "        self.local_rank = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "B7BC-DsNUbFC"
      },
      "outputs": [],
      "source": [
        "#@title Trainer util code\n",
        "import math, sys\n",
        "import torch\n",
        "from math import inf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class NativeScalerWithGradNormCount:\n",
        "    state_dict_key = \"amp_scaler\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
        "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
        "        if update_grad:\n",
        "            if clip_grad is not None:\n",
        "                assert parameters is not None\n",
        "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
        "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
        "            else:\n",
        "                self._scaler.unscale_(optimizer)\n",
        "                norm = get_grad_norm_(parameters)\n",
        "            self._scaler.step(optimizer)\n",
        "            self._scaler.update()\n",
        "        else:\n",
        "            norm = None\n",
        "        return norm\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self._scaler.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self._scaler.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def get_grad_norm_(parameters, norm_type: float = 2.0):\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    parameters = [p for p in parameters if p.grad is not None]\n",
        "    norm_type = float(norm_type)\n",
        "    if len(parameters) == 0:\n",
        "        return torch.tensor(0.)\n",
        "    device = parameters[0].grad.device\n",
        "    if norm_type == inf:\n",
        "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
        "    else:\n",
        "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
        "    return total_norm\n",
        "\n",
        "\n",
        "def train_one_epoch(model, data_loader, optimizer, device, epoch,\n",
        "                        loss_scaler, log_writer=None, config=None, start_time=None, model_without_ddp=None,\n",
        "                        img_feature_extractor=None, preprocess=None):\n",
        "    model.train(True)\n",
        "    optimizer.zero_grad()\n",
        "    total_loss = []\n",
        "    total_cor = []\n",
        "    accum_iter = config.accum_iter\n",
        "    for data_iter_step, (data_dcit) in enumerate(data_loader):\n",
        "\n",
        "        # we use a per iteration (instead of per epoch) lr scheduler\n",
        "        # print(data_iter_step)\n",
        "        # print(len(data_loader))\n",
        "\n",
        "        if data_iter_step % accum_iter == 0:\n",
        "            adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, config)\n",
        "        samples = data_dcit['eeg']\n",
        "\n",
        "        img_features = None\n",
        "        valid_idx = None\n",
        "        if img_feature_extractor is not None:\n",
        "            images = data_dcit['image']\n",
        "            valid_idx = torch.nonzero(images.sum(dim=(1,2,3)) != 0).squeeze(1)\n",
        "            img_feature_extractor.eval()\n",
        "            with torch.no_grad():\n",
        "                img_features = img_feature_extractor(preprocess(images[valid_idx]).to(device))['layer2']\n",
        "        samples = samples.to(device)\n",
        "        # img_features = img_features.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "            loss, pred, _ = model(samples, img_features, valid_idx=valid_idx, mask_ratio=config.mask_ratio)\n",
        "        # loss.backward()\n",
        "        # norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
        "        # optimizer.step()\n",
        "\n",
        "        loss_value = loss.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training at step {data_iter_step} epoch {epoch}\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # loss /= accum_iter\n",
        "        loss_scaler(loss, optimizer, parameters=model.parameters(), clip_grad=config.clip_grad)\n",
        "\n",
        "        # if (data_iter_step + 1) % accum_iter == 0:\n",
        "        # cal the cor\n",
        "        pred = pred.to('cpu').detach()\n",
        "        samples = samples.to('cpu').detach()\n",
        "        # pred = pred.transpose(1,2) #model_without_ddp.unpatchify(pred)\n",
        "        pred = model_without_ddp.unpatchify(pred)\n",
        "        # print(pred.shape)\n",
        "        # print(samples.shape)\n",
        "        # for p, s in zip(pred, samples):\n",
        "        #     print(p[0], s[0])\n",
        "        #     print(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))\n",
        "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0)))\n",
        "        #     print(torch.corrcoef(torch.cat([p[0].unsqueeze(0), s[0].unsqueeze(0)],axis=0))[0,1])\n",
        "        cor = torch.mean(torch.tensor([torch.corrcoef(torch.cat([p[7].unsqueeze(0), s[7].unsqueeze(0)],axis=0))[0,1] for p, s in zip(pred, samples)])).item()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss.append(loss_value)\n",
        "        total_cor.append(cor)\n",
        "        if device == torch.device('cuda:0'):\n",
        "            lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print('train_loss_step:', np.mean(total_loss), 'lr:', lr, 'cor', np.mean(total_cor))\n",
        "\n",
        "    if log_writer is not None:\n",
        "        lr = optimizer.param_groups[0][\"lr\"]\n",
        "        log_writer.log('train_loss_step', np.mean(total_loss), step=epoch)\n",
        "        log_writer.log('lr', lr, step=epoch)\n",
        "        log_writer.log('cor', np.mean(total_cor), step=epoch)\n",
        "        if start_time is not None:\n",
        "            log_writer.log('time (min)', (time.time() - start_time)/60.0, step=epoch)\n",
        "    if config.local_rank == 0:\n",
        "        print(f'[Epoch {epoch}] loss: {np.mean(total_loss)}')\n",
        "\n",
        "    return np.mean(total_cor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "8RTMC7_zC3cq",
        "outputId": "d8509e6b-84c0-4818-8ba7-a3e0e8bc7607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'lr': 0.00025, 'min_lr': 0.0, 'weight_decay': 0.05, 'num_epoch': 700, 'warmup_epochs': 40, 'batch_size': 20, 'clip_grad': 0.8, 'mask_ratio': 0.15, 'patch_size': 4, 'embed_dim': 1024, 'decoder_embed_dim': 256, 'depth': 24, 'num_heads': 16, 'decoder_num_heads': 16, 'mlp_ratio': 1.0, 'root_path': 'processed/', 'output_path': 'processed/results/eeg_pretrain/17-11-2023-19-00-12', 'seed': 2022, 'roi': 'VC', 'aug_times': 1, 'num_sub_limit': None, 'include_hcp': True, 'include_kam': True, 'accum_iter': 1, 'use_nature_img_loss': False, 'img_recon_weight': 0.5, 'focus_range': None, 'focus_rate': 0.6, 'local_rank': 0}\n",
            "Dataset size: 420\n",
            " Time len: 1024\n",
            "AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.00025\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.00025\n",
            "    maximize: False\n",
            "    weight_decay: 0.05\n",
            ")\n",
            "Start Training the EEG MAE ... ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alindumitru/anaconda3/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:120: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
            "/Users/alindumitru/anaconda3/lib/python3.11/site-packages/torch/amp/autocast_mode.py:204: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
            "/var/folders/62/9yvykkwd67x92x4q3877tqzh0000gn/T/ipykernel_17862/915322198.py:313: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /private/var/folders/nz/j6p8yfhx1mv_0grj5xl4650h0000gp/T/abs_1aidzjezue/croot/pytorch_1687856425340/work/aten/src/ATen/native/mps/operations/Sort.mm:39.)\n",
            "  ids_restore = torch.argsort(ids_shuffle, dim=1)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=278'>279</a>\u001b[0m config \u001b[39m=\u001b[39m Config_MBM_EEG()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=279'>280</a>\u001b[0m config \u001b[39m=\u001b[39m update_config(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, config)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=280'>281</a>\u001b[0m main(config)\n",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m     sampler\u001b[39m.\u001b[39mset_epoch(ep) \u001b[39m# to shuffle the data at every epoch\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=168'>169</a>\u001b[0m cor \u001b[39m=\u001b[39m train_one_epoch(model, dataloader_eeg, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=169'>170</a>\u001b[0m                     img_feature_extractor, preprocess)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=170'>171</a>\u001b[0m cor_list\u001b[39m.\u001b[39mappend(cor)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mif\u001b[39;00m (ep \u001b[39m==\u001b[39m \u001b[39m100\u001b[39m):\n",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     loss, pred, _ \u001b[39m=\u001b[39m model(samples, img_features, valid_idx\u001b[39m=\u001b[39mvalid_idx, mask_ratio\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mmask_ratio)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39m# norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39m# optimizer.step()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m loss_value \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=432'>433</a>\u001b[0m latent, mask, ids_restore \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_encoder(imgs, mask_ratio)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=433'>434</a>\u001b[0m     \u001b[39m# print(x)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=434'>435</a>\u001b[0m \u001b[39m# print(latent.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=435'>436</a>\u001b[0m \u001b[39m# # print(mask)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=436'>437</a>\u001b[0m \u001b[39m# print(mask.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=437'>438</a>\u001b[0m \u001b[39m# # print(ids_restore)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=438'>439</a>\u001b[0m \u001b[39m# print(ids_restore.shape)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=440'>441</a>\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_decoder(latent, ids_restore)  \u001b[39m# [N, L, p]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=441'>442</a>\u001b[0m \u001b[39m# pred = self.forward_decoder(latent)  # [N, L, p]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=442'>443</a>\u001b[0m \u001b[39m# pred = pred\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=443'>444</a>\u001b[0m \u001b[39m# print(pred.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=444'>445</a>\u001b[0m \u001b[39m# mask=None\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=445'>446</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_loss(imgs, pred, mask)\n",
            "\u001b[1;32m/Users/alindumitru/Desktop/GIT Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=363'>364</a>\u001b[0m \u001b[39m# x = x + self.decoder_pos_embed[:, 1:, :]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=364'>365</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=365'>366</a>\u001b[0m \u001b[39m# apply Transformer blocks\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=366'>367</a>\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_blocks:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=367'>368</a>\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=368'>369</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_norm(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=369'>370</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alindumitru/Desktop/GIT%20Repos/neural-art/src/ai_models/offline_colab/train_eeg.ipynb#X13sZmlsZQ%3D%3D?line=370'>371</a>\u001b[0m \u001b[39m# predictor projection\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/timm/models/vision_transformer.py:230\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    229\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)))\n\u001b[0;32m--> 230\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/normalization.py:190\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlayer_norm(\n\u001b[1;32m    191\u001b[0m         \u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalized_shape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2515\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2511\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   2512\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2513\u001b[0m         layer_norm, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, normalized_shape, weight\u001b[39m=\u001b[39mweight, bias\u001b[39m=\u001b[39mbias, eps\u001b[39m=\u001b[39meps\n\u001b[1;32m   2514\u001b[0m     )\n\u001b[0;32m-> 2515\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlayer_norm(\u001b[39minput\u001b[39m, normalized_shape, weight, bias, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Pretrain EEG state\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "import argparse\n",
        "import time\n",
        "import timm.optim.optim_factory as optim_factory\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "def add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n",
        "    decay = []\n",
        "    no_decay = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue  # frozen weights\n",
        "        if len(param.shape) == 1 or name.endswith(\".bias\") or name in skip_list:\n",
        "            no_decay.append(param)\n",
        "        else:\n",
        "            decay.append(param)\n",
        "    return [\n",
        "        {'params': no_decay, 'weight_decay': 0.},\n",
        "        {'params': decay, 'weight_decay': weight_decay}]\n",
        "\n",
        "class wandb_logger:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.step = None\n",
        "\n",
        "    def log(self, name, data, step=None):\n",
        "        if step is None:\n",
        "            print(name, data)\n",
        "        else:\n",
        "            print(name, data, step)\n",
        "            self.step = step\n",
        "\n",
        "    def watch_model(self, *args, **kwargs):\n",
        "        print('watch model')\n",
        "\n",
        "    def log_image(self, name, fig):\n",
        "        if self.step is None:\n",
        "            print(name, fig)\n",
        "        else:\n",
        "            print(name, fig, self.step)\n",
        "\n",
        "    def finish(self):\n",
        "        print('finish')\n",
        "\n",
        "def get_args_parser():\n",
        "    parser = argparse.ArgumentParser('MBM pre-training for fMRI', add_help=False)\n",
        "\n",
        "    # Training Parameters\n",
        "    parser.add_argument('--lr', type=float)\n",
        "    parser.add_argument('--weight_decay', type=float)\n",
        "    parser.add_argument('--num_epoch', type=int)\n",
        "    parser.add_argument('--batch_size', type=int)\n",
        "\n",
        "    # Model Parameters\n",
        "    parser.add_argument('--mask_ratio', type=float)\n",
        "    parser.add_argument('--patch_size', type=int)\n",
        "    parser.add_argument('--embed_dim', type=int)\n",
        "    parser.add_argument('--decoder_embed_dim', type=int)\n",
        "    parser.add_argument('--depth', type=int)\n",
        "    parser.add_argument('--num_heads', type=int)\n",
        "    parser.add_argument('--decoder_num_heads', type=int)\n",
        "    parser.add_argument('--mlp_ratio', type=float)\n",
        "\n",
        "    # Project setting\n",
        "    parser.add_argument('--root_path', type=str)\n",
        "    parser.add_argument('--seed', type=str)\n",
        "    parser.add_argument('--roi', type=str)\n",
        "    parser.add_argument('--aug_times', type=int)\n",
        "    parser.add_argument('--num_sub_limit', type=int)\n",
        "\n",
        "    parser.add_argument('--include_hcp', type=bool)\n",
        "    parser.add_argument('--include_kam', type=bool)\n",
        "\n",
        "    parser.add_argument('--use_nature_img_loss', type=bool)\n",
        "    parser.add_argument('--img_recon_weight', type=float)\n",
        "\n",
        "    # distributed training parameters\n",
        "    parser.add_argument('--local_rank', type=int)\n",
        "\n",
        "    return parser\n",
        "\n",
        "def create_readme(config, path):\n",
        "    print(config.__dict__)\n",
        "    with open(os.path.join(path, 'README.md'), 'w+') as f:\n",
        "        print(config.__dict__, file=f)\n",
        "\n",
        "def fmri_transform(x, sparse_rate=0.2):\n",
        "    # x: 1, num_voxels\n",
        "    x_aug = copy.deepcopy(x)\n",
        "    idx = np.random.choice(x.shape[0], int(x.shape[0]*sparse_rate), replace=False)\n",
        "    x_aug[idx] = 0\n",
        "    return torch.FloatTensor(x_aug)\n",
        "\n",
        "def main(config):\n",
        "    # print('num of gpu:')\n",
        "    # print(torch.cuda.device_count())\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        torch.cuda.set_device(config.local_rank)\n",
        "        torch.distributed.init_process_group(backend='nccl')\n",
        "    output_path = os.path.join(config.root_path, 'results', 'eeg_pretrain',  '%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")))\n",
        "    config.output_path = output_path\n",
        "    # logger = wandb_logger(config) if config.local_rank == 0 else None\n",
        "    logger = None\n",
        "\n",
        "    if config.local_rank == 0:\n",
        "        os.makedirs(output_path, exist_ok=True)\n",
        "        create_readme(config, output_path)\n",
        "\n",
        "    device = torch.device(f'cuda:{config.local_rank}') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    # uncomment below for macos\n",
        "    #device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device('cpu')\n",
        "    torch.manual_seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "\n",
        "    # create dataset and dataloader\n",
        "    dataset_pretrain = eeg_pretrain_dataset(path='eegData_npy/', roi=config.roi, patch_size=config.patch_size,\n",
        "                transform=fmri_transform, aug_times=config.aug_times, num_sub_limit=config.num_sub_limit,\n",
        "                include_kam=config.include_kam, include_hcp=config.include_hcp)\n",
        "\n",
        "    print(f'Dataset size: {len(dataset_pretrain)}\\n Time len: {dataset_pretrain.data_len}')\n",
        "    sampler = torch.utils.data.DistributedSampler(dataset_pretrain, rank=config.local_rank) if torch.cuda.device_count() > 1 else None\n",
        "\n",
        "    dataloader_eeg = DataLoader(dataset_pretrain, batch_size=config.batch_size, sampler=sampler,\n",
        "                shuffle=(sampler is None), pin_memory=True)\n",
        "\n",
        "    # create model\n",
        "    config.time_len=dataset_pretrain.data_len\n",
        "    model = MAEforEEG(time_len=dataset_pretrain.data_len, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
        "                    decoder_embed_dim=config.decoder_embed_dim, depth=config.depth,\n",
        "                    num_heads=config.num_heads, decoder_num_heads=config.decoder_num_heads, mlp_ratio=config.mlp_ratio,\n",
        "                    focus_range=config.focus_range, focus_rate=config.focus_rate,\n",
        "                    img_recon_weight=config.img_recon_weight, use_nature_img_loss=config.use_nature_img_loss)\n",
        "    model.to(device)\n",
        "    model_without_ddp = model\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "        model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank, find_unused_parameters=config.use_nature_img_loss)\n",
        "\n",
        "    param_groups = add_weight_decay(model, config.weight_decay)\n",
        "    optimizer = torch.optim.AdamW(param_groups, lr=config.lr, betas=(0.9, 0.95))\n",
        "    print(optimizer)\n",
        "    loss_scaler = NativeScalerWithGradNormCount()\n",
        "\n",
        "    start_epoch = 0\n",
        "    checkpoint_path = '/content/drive/MyDrive/eeg14/processed/results/eeg_pretrain/25-11-2023-10-42-24/checkpoints/checkpoint.pth'\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # handle an increase in the depth architecture:\n",
        "        model_dict = model_without_ddp.state_dict()\n",
        "        # Update the model's state dict with checkpoint weights for matching layers\n",
        "        for name, param in checkpoint['model'].items():\n",
        "            if name in model_dict and param.size() == model_dict[name].size():\n",
        "                model_dict[name].copy_(param)\n",
        "\n",
        "        model_without_ddp.load_state_dict(model_dict, strict=False)\n",
        "\n",
        "        #model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "        #optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss_scaler.load_state_dict(checkpoint['scaler'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print(f\"Loaded checkpoint from {checkpoint_path}, starting from epoch {start_epoch}\")\n",
        "\n",
        "    if logger is not None:\n",
        "        logger.watch_model(model,log='all', log_freq=1000)\n",
        "\n",
        "    cor_list = []\n",
        "    start_time = time.time()\n",
        "    print('Start Training the EEG MAE ... ...')\n",
        "    img_feature_extractor = None\n",
        "    preprocess = None\n",
        "    if config.use_nature_img_loss:\n",
        "        from torchvision.models import resnet50, ResNet50_Weights\n",
        "        from torchvision.models.feature_extraction import create_feature_extractor\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        preprocess = weights.transforms()\n",
        "        m = resnet50(weights=weights)\n",
        "        img_feature_extractor = create_feature_extractor(m, return_nodes={f'layer2': 'layer2'}).to(device).eval()\n",
        "        for param in img_feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    for ep in range(start_epoch, config.num_epoch):\n",
        "\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            sampler.set_epoch(ep) # to shuffle the data at every epoch\n",
        "        cor = train_one_epoch(model, dataloader_eeg, optimizer, device, ep, loss_scaler, logger, config, start_time, model_without_ddp,\n",
        "                            img_feature_extractor, preprocess)\n",
        "        cor_list.append(cor)\n",
        "        if (ep == 2400):\n",
        "            print('Saving the model...');\n",
        "            save_model(config, ep, model_without_ddp, optimizer, loss_scaler, os.path.join(output_path,'checkpoints'))\n",
        "            # plot figures\n",
        "            plot_recon_figures(model, device, dataset_pretrain, output_path, 5, config, logger, model_without_ddp)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "    print('Training time {}'.format(total_time_str))\n",
        "    if logger is not None:\n",
        "        logger.log('max cor', np.max(cor_list), step=config.num_epoch-1)\n",
        "        logger.finish()\n",
        "    return\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_recon_figures(model, device, dataset, output_path, num_figures = 5, config=None, logger=None, model_without_ddp=None):\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(num_figures, 3, figsize=(30,15))\n",
        "    fig.tight_layout()\n",
        "    axs[0,0].set_title('Ground-truth')\n",
        "    axs[0,1].set_title('Masked Ground-truth')\n",
        "    axs[0,2].set_title('Reconstruction')\n",
        "\n",
        "    for ax in axs:\n",
        "        sample = next(iter(dataloader))['eeg']\n",
        "        sample = sample.to(device)\n",
        "        _, pred, mask = model(sample, mask_ratio=config.mask_ratio)\n",
        "        # sample_with_mask = model_without_ddp.patchify(sample.transpose(1,2))[0].to('cpu').numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        sample_with_mask = sample.to('cpu').squeeze(0)[0].numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        # pred = model_without_ddp.unpatchify(pred.transpose(1,2)).to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        # sample = sample.to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        pred = model_without_ddp.unpatchify(pred).to('cpu').squeeze(0)[0].numpy()\n",
        "        # pred = model_without_ddp.unpatchify(model_without_ddp.patchify(sample.transpose(1,2))).to('cpu').squeeze(0)[0].numpy()\n",
        "        sample = sample.to('cpu').squeeze(0)[0].numpy()\n",
        "        mask = mask.to('cpu').numpy().reshape(-1)\n",
        "\n",
        "        cor = np.corrcoef([pred, sample])[0,1]\n",
        "\n",
        "        x_axis = np.arange(0, sample.shape[-1])\n",
        "        # groundtruth\n",
        "        ax[0].plot(x_axis, sample)\n",
        "        # groundtruth with mask\n",
        "        s = 0\n",
        "        for x, m in zip(sample_with_mask,mask):\n",
        "            if m == 0:\n",
        "                ax[1].plot(x_axis[s:s+len(x)], x, color='#1f77b4')\n",
        "            s += len(x)\n",
        "        # pred\n",
        "        ax[2].plot(x_axis, pred)\n",
        "        ax[2].set_ylabel('cor: %.4f'%cor, weight = 'bold')\n",
        "        ax[2].yaxis.set_label_position(\"right\")\n",
        "\n",
        "    fig_name = 'reconst-%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
        "    fig.savefig(os.path.join(output_path, f'{fig_name}.png'))\n",
        "    if logger is not None:\n",
        "        logger.log_image('reconst', fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_recon_figures2(model, device, dataset, output_path, num_figures = 5, config=None, logger=None, model_without_ddp=None):\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(num_figures, 2, figsize=(20,15))\n",
        "    fig.tight_layout()\n",
        "    axs[0,0].set_title('Ground-truth')\n",
        "    # axs[0,1].set_title('Masked Ground-truth')\n",
        "    axs[0,1].set_title('Reconstruction')\n",
        "\n",
        "    for ax in axs:\n",
        "        sample = next(iter(dataloader))['eeg']\n",
        "        sample = sample.to(device)\n",
        "        _, pred, mask = model(sample, mask_ratio=config.mask_ratio)\n",
        "        # sample_with_mask = model_without_ddp.patchify(sample.transpose(1,2))[0].to('cpu').numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        sample_with_mask = sample.to('cpu').squeeze(0)[0].numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        # pred = model_without_ddp.unpatchify(pred.transpose(1,2)).to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        # sample = sample.to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        pred = model_without_ddp.unpatchify(pred).to('cpu').squeeze(0)[0].numpy()\n",
        "        # pred = model_without_ddp.unpatchify(model_without_ddp.patchify(sample.transpose(1,2))).to('cpu').squeeze(0)[0].numpy()\n",
        "        sample = sample.to('cpu').squeeze(0)[0].numpy()\n",
        "        cor = np.corrcoef([pred, sample])[0,1]\n",
        "\n",
        "        x_axis = np.arange(0, sample.shape[-1])\n",
        "        # groundtruth\n",
        "        ax[0].plot(x_axis, sample)\n",
        "\n",
        "        ax[1].plot(x_axis, pred)\n",
        "        ax[1].set_ylabel('cor: %.4f'%cor, weight = 'bold')\n",
        "        ax[1].yaxis.set_label_position(\"right\")\n",
        "\n",
        "    fig_name = 'reconst-%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
        "    fig.savefig(os.path.join(output_path, f'{fig_name}.png'))\n",
        "    if logger is not None:\n",
        "        logger.log_image('reconst', fig)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def update_config(args, config):\n",
        "    for attr in config.__dict__:\n",
        "        if hasattr(args, attr):\n",
        "            if getattr(args, attr) != None:\n",
        "                setattr(config, attr, getattr(args, attr))\n",
        "    return config\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = Config_MBM_EEG()\n",
        "    config = update_config(\"\", config)\n",
        "    main(config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "config = Config_MBM_EEG()\n",
        "\n",
        "device = torch.device(f'cuda:{config.local_rank}') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# initialize model\n",
        "model = MAEforEEG(time_len=1024, patch_size=config.patch_size, embed_dim=config.embed_dim,\n",
        "      decoder_embed_dim=config.decoder_embed_dim, depth=config.depth,\n",
        "      num_heads=config.num_heads, decoder_num_heads=config.decoder_num_heads, mlp_ratio=config.mlp_ratio,\n",
        "      focus_range=config.focus_range, focus_rate=config.focus_rate,\n",
        "      img_recon_weight=config.img_recon_weight, use_nature_img_loss=config.use_nature_img_loss)\n",
        "\n",
        "model.to(device)\n",
        "model_without_ddp = model\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n",
        "    model = DistributedDataParallel(model, device_ids=[config.local_rank], output_device=config.local_rank, find_unused_parameters=config.use_nature_img_loss)\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/eeg14/processed/results/eeg_pretrain/23-11-2023-20-14-11/checkpoints/checkpoint.pth'\n",
        "if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model_without_ddp.load_state_dict(checkpoint['model'])\n",
        "\n",
        "def fmri_transform(x, sparse_rate=0.2):\n",
        "    # x: 1, num_voxels\n",
        "    x_aug = copy.deepcopy(x)\n",
        "    idx = np.random.choice(x.shape[0], int(x.shape[0]*sparse_rate), replace=False)\n",
        "    x_aug[idx] = 0\n",
        "    return torch.FloatTensor(x_aug)\n",
        "\n",
        "test_dataset = eeg_pretrain_dataset(path='test_npy/', roi=config.roi, patch_size=config.patch_size,\n",
        "                transform=fmri_transform, aug_times=config.aug_times, num_sub_limit=config.num_sub_limit,\n",
        "                include_kam=config.include_kam, include_hcp=config.include_hcp)\n",
        "\n",
        "# Test how good the algorithm is performing on data it never seen before\n",
        "@torch.no_grad()\n",
        "def plot_recon_figures(model, device, dataset, num_figures = 20, config=None, model_without_ddp=None):\n",
        "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    model.eval()\n",
        "    fig, axs = plt.subplots(num_figures, 3, figsize=(30,15))\n",
        "    fig.tight_layout()\n",
        "    axs[0,0].set_title('Ground-truth')\n",
        "    axs[0,1].set_title('Masked Ground-truth')\n",
        "    axs[0,2].set_title('Reconstruction')\n",
        "    corSum = 0\n",
        "\n",
        "    for ax in axs:\n",
        "        sample = next(iter(dataloader))['eeg']\n",
        "        sample = sample.to(device)\n",
        "        _, pred, mask = model(sample, mask_ratio=config.mask_ratio)\n",
        "        print(mask.shape)\n",
        "        # sample_with_mask = model_without_ddp.patchify(sample.transpose(1,2))[0].to('cpu').numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        chann = 7\n",
        "        sample_with_mask = sample.to('cpu').squeeze(0)[chann].numpy().reshape(-1, model_without_ddp.patch_size)\n",
        "        # pred = model_without_ddp.unpatchify(pred.transpose(1,2)).to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        # sample = sample.to('cpu').squeeze(0)[0].unsqueeze(0).numpy()\n",
        "        pred = model_without_ddp.unpatchify(pred).to('cpu').squeeze(0)[chann].numpy()\n",
        "        # pred = model_without_ddp.unpatchify(model_without_ddp.patchify(sample.transpose(1,2))).to('cpu').squeeze(0)[0].numpy()\n",
        "        sample = sample.to('cpu').squeeze(0)[chann].numpy()\n",
        "        mask = mask.to('cpu').numpy().reshape(-1)\n",
        "\n",
        "        cor = np.corrcoef([pred, sample])[0,1]\n",
        "        corSum = corSum + cor\n",
        "\n",
        "        x_axis = np.arange(0, sample.shape[-1])\n",
        "        # groundtruth\n",
        "        ax[0].plot(x_axis, sample)\n",
        "        # groundtruth with mask\n",
        "        s = 0\n",
        "        for x, m in zip(sample_with_mask,mask):\n",
        "            if m == 0:\n",
        "                ax[1].plot(x_axis[s:s+len(x)], x, color='#1f77b4')\n",
        "            s += len(x)\n",
        "        # pred\n",
        "        ax[2].plot(x_axis, pred)\n",
        "        ax[2].set_ylabel('cor: %.4f'%cor, weight = 'bold')\n",
        "        ax[2].yaxis.set_label_position(\"right\")\n",
        "\n",
        "    fig_name = 'reconst-%s'%(datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\"))\n",
        "    #plt.show()\n",
        "    print(f'Cor average: {corSum / num_figures}')\n",
        "\n",
        "plot_recon_figures(model, device, test_dataset, 8, config, model_without_ddp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
